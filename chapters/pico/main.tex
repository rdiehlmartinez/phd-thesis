\chapter{The \picosupabig Framework: Lightweight Infrastructure for Studying Learning Dynamics}
\label{sec:pico-intro}

The rapid progress of large language models (LLMs) has produced systems that excel at a wide range of natural language tasks, from reasoning and summarization to coding and multilingual translation \citep{hendrycks2021mmlu, cobbe2021gsm8k, srivastava2023bigbench}. Yet this progress has also introduced new challenges: as models grow larger and more complex, they become harder to analyze, diagnose, and refine. Understanding how these models acquire knowledge—particularly how their internal representations evolve during training—remains a critical yet under-explored research frontier.

Existing model development frameworks such as DeepSpeed \citep{rasley2020deepspeed}, Megatron-LM \citep{narayanan2021megatron}, and MosaicML’s MPT \citep{mosaic2023mpt} have pushed the boundaries of efficient, large-scale training. However, their emphasis on production performance and throughput often comes at the cost of transparency. Intermediate representations, gradients, and activations are rarely captured in a structured way, and instrumentation for fine-grained interpretability is typically an afterthought. As a result, researchers seeking to study learning dynamics or test training interventions face steep technical overhead.

Conversely, frameworks built for interpretability—such as TransformerLens \citep{nanda2022transformerlens}, ROME \citep{meng2022locating}, and ACDC \citep{conmy2023towards}—enable detailed circuit-level analysis but are mostly limited to post-hoc exploration of fully trained models. They rarely support modifications to the training process itself, and when they do, such interventions tend to be tightly coupled to specific checkpoints, architectures, or datasets. Even frameworks explicitly designed for developmental analysis, like DevInterp \citep{devinterpcode}, rely on external checkpointing pipelines and are often decoupled from model training.

To support the growing demand for training-time interpretability—that is, the ability to track and intervene in the learning process as it unfolds—we need a framework that makes the training loop itself an object of study. We need infrastructure that not only logs activations and gradients but also invites experimentation with training interventions, architectural variants, and representational diagnostics. Crucially, this framework should be lightweight and accessible, making it possible to iterate quickly without requiring industrial-scale compute or engineering resources.

\textbf{This chapter introduces \pico}, a modular, extensible framework designed to fill this gap. Built for training and analyzing small-to-medium scale models (1M--1B parameters), Pico is engineered to support research into the internal mechanisms and developmental trajectories of language models. It consists of two components:

\begin{enumerate}
    \item \texttt{pico-train}, a customizable training library that makes it easy to train models from scratch while recording rich internal signals (weights, gradients, activations) at regular checkpoints.
    \item \texttt{pico-analyze}, a companion toolkit for querying, visualizing, and analyzing these signals—designed to integrate smoothly with standard scientific workflows and support user-defined metrics.
\end{enumerate}

Together, these tools make it possible to perform fine-grained, in-situ analysis of how models learn over time, test hypotheses about representation development, and prototype interventions that might improve training outcomes—particularly for small models, where interpretability and efficiency are especially critical.

Pico was built to address a concrete gap identified in the previous chapter: existing suites like Pythia offer rich insights into learning dynamics after training, but lack the flexibility and transparency needed to test interventions during training. We need tools that bring the same level of rigor to the training process as to post-hoc analysis.

The rest of this chapter is structured as follows. We begin by situating Pico within the broader ecosystem of language model training and interpretability frameworks, reviewing both large-scale production platforms and experimental research toolkits. We then describe the key architectural and design decisions behind \texttt{pico-train} and \texttt{pico-analyze}, highlighting how they support training-time experimentation. Finally, we compare Pico to existing frameworks in terms of flexibility, granularity, and ease of use, showing how it enables new kinds of research in learning dynamics and model interpretability.


{
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}

\begin{table}[htbp]
    \centering
    \footnotesize

    \begin{tabular}{@{}p{2.6cm} p{1.8cm} p{2.6cm} p{2.4cm} p{2.0cm} p{2.3cm}@{}}
    \toprule
    \textbf{Tool} &
    \textbf{Custom Training} &
    \textbf{Checkpointing \& Logging} &
    \textbf{Analysis \& Extensibility} &
    \textbf{Small Scale Suitability} &
    \textbf{Ease of Use} \\
    \midrule
    \textbf{\pico} & 
    \cmark \newline Modular PyTorch &
    \cmark \newline Activations, grads, weights &
    \cmark \newline Extensible metrics \& components &
    \cmark \newline Built for 1M-1B models &
    \cmark \newline Extensive documentation, simple setup\\

    \midrule

    TransformerLens & 
    \xmark & \xmark & \cmark & \cmark & \warnmark \\

    ACDC & 
    \xmark & \warnmark & \cmark & \cmark & \xmark \\

    ROME & 
    \xmark & \warnmark & \cmark & \cmark & \xmark \\

    \midrule

    DevInterp & 
    \warnmark & \cmark & \cmark & \warnmark & \warnmark \\

    \midrule

    SmolLM2 & 
    \xmark & \warnmark & \warnmark & \cmark & \cmark \\

    Pythia Suite & 
    \warnmark & \warnmark & \xmark & \cmark & \xmark \\

    OLMo & 
    \warnmark & \warnmark & \xmark & \warnmark & \xmark \\

    \bottomrule
    \end{tabular}
    \vspace{0.5em}
    
    \raggedright
    \caption{Comparison of Pico and related frameworks for interpretability and learning dynamics}
    \label{tab:pico_comparison_final}
    \textbf{Legend:} \cmark = Fully supported; \xmark = Not supported; \warnmark = Partial or limited support
\end{table}
}

\section{Situating \picolarge in the Landscape of Language Model Frameworks}
\label{sec:pico-related}

Language model research increasingly depends on infrastructure—not just for scaling up training, but for opening up models to analysis. As interest grows in how models acquire structure, allocate capacity, and evolve over time, researchers need frameworks that support more than just performance benchmarking. They need transparency into the training process itself.

Today's ecosystem of LLM tooling is rich but uneven. On one end are large-scale training platforms that emphasize throughput, hardware optimization, and production readiness—such as DeepSpeed \citep{rasley2020deepspeed}, Megatron-LM \citep{narayanan2021megatron}, and MosaicML's MPT \citep{mosaic2023mpt}. On the other are interpretability toolkits that provide detailed post-hoc analysis of model internals, often at the level of neurons, circuits, or activations—examples include TransformerLens \citep{nanda2022transformerlens}, ROME \citep{meng2022locating}, and ACDC \citep{conmy2023towards}. However, few systems support both perspectives simultaneously: training frameworks rarely expose the internal states needed for interpretability, and interpretability tools rarely integrate with the training loop.

Pico is designed to bridge this gap. It supports full training from scratch, while offering built-in mechanisms to log and analyze activations, gradients, and other internal signals throughout training. This makes it possible to study learning dynamics not only retrospectively, but as they unfold.

To understand the niche Pico fills, it helps to situate it relative to two major categories of tools: training frameworks and analysis frameworks.

\paragraph{Training Frameworks}
Large-scale systems like DeepSpeed \citep{rasley2020deepspeed}, Megatron-LM \citep{narayanan2021megatron}, and MosaicML \citep{mosaic2023mpt} are highly optimized for training massive models. They handle distributed computation and mixed-precision arithmetic efficiently, but their training pipelines are complex and often opaque. Modifying or instrumenting them to inspect model internals—let alone run repeated training interventions—can require significant effort. These frameworks are indispensable for production-scale pretraining, but ill-suited for small-scale, experiment-driven research.

In contrast, lightweight training frameworks such as NanoGPT \citep{karpathy2023nanogpt}, SmolLM2 \citep{allal2025smollm2}, TinyLlama \citep{zhang2024tinyllama}, and TinyStories \citep{eldan2023tinystories} emphasize simplicity and fast iteration. Their codebases are accessible and easy to modify, making them ideal for prototyping new ideas. However, they tend to treat training as a black box. Logging activations or inspecting weight evolution generally requires manual modification, and internal state tracking is minimal or absent. These frameworks make it easy to run experiments, but hard to measure what's happening inside.

Pico aims to combine the best of both worlds. It is optimized for small and mid-sized models (1M--1B parameters) and is built in modular PyTorch for ease of use. But unlike most lightweight suites, it is designed from the ground up to expose model internals. It logs key training signals by default—weights, activations, gradients—and does so in structured, extensible formats. This makes Pico not just a tool for training models, but a platform for studying how they learn.

\paragraph{Analysis Frameworks}
Where training frameworks help build models, analysis frameworks help interpret them. Recent progress in mechanistic interpretability has produced tools like TransformerLens \citep{nanda2022transformerlens}, ROME \citep{meng2022locating}, and ACDC \citep{conmy2023towards}, which allow detailed inspection of trained transformer circuits. These tools have been invaluable for understanding how attention heads specialize or how knowledge is stored and retrieved. But they are mostly post-hoc: they assume a trained model and operate over static checkpoints.

Complementing these are tools like DevInterp \citep{devinterpcode}, which begin to trace how models develop during training by analyzing intermediate checkpoints. This shift toward developmental interpretability has revealed that models undergo discrete representational shifts, acquire capabilities in phases \citep{hoogland2023towards, hoogland2025losslandscape}, and sometimes diverge in ways that are hard to recover from. Still, most of these frameworks remain downstream—they depend on external training pipelines and cannot modify or influence the training process itself.

Here again, Pico closes a key loop. By integrating training and analysis into the same workflow, it enables fine-grained, in-situ investigation of representational dynamics. Rather than relying on occasional checkpoints saved by another framework, Pico makes detailed logging a native part of the training loop. This allows researchers to monitor changes in capacity usage, representation similarity, or sparsity metrics in real time, and to test hypotheses by intervening directly in training.

\paragraph{Toward Integrated Research Infrastructure}
The distinctions between training and analysis frameworks have historically reflected different goals—efficiency vs. interpretability, scale vs. visibility. But to understand how language models learn, especially at small and intermediate scales, these goals need to be brought together. Pico is an attempt to unify them into a single research workflow, one that enables controlled experimentation, rapid iteration, and structured analysis without sacrificing clarity or flexibility.

Table~\ref{tab:pico_comparison_final} provides a comparative summary of how Pico fits into the current landscape, alongside other commonly used frameworks. While each tool has strengths of its own, Pico is distinct in offering custom training, rich logging, and built-in analysis—all in a lightweight package designed for learning dynamics research.

\section{\picolarge}

In this section we provide a concise overview of the two \textbf{Pico} libraries: \texttt{pico-train} and \texttt{pico-analyze}. 

\subsection{\texttt{pico-train}: A Minimalist Approach to Model Training}

\texttt{pico-train} is a lightweight, transparent framework for training small- to medium-scale language models. Unlike many existing training libraries that prioritize efficiency at the cost of clarity, \texttt{pico-train} is designed to be simple, modular, and easy to modify—making it a flexible foundation for experimentation in language model research.

Out of the box, \texttt{pico-train} implements \texttt{pico-decoder}, a LLaMA-style transformer \citep{touvron2023llama} that incorporates key features of modern auto-regressive language models, including Grouped Query Attention (GQA) \citep{ainslie2023gqa}, Rotary Position Embeddings (RoPE) \citep{su2024rope}, FlashAttention \citep{dao2022flashattention}, SwiGLU activations \citep{shazeer2020glu}, and RMSNorm \citep{zhang2019rmsnorm}. All components—except FlashAttention—are re-implemented from scratch in plain PyTorch \citep{paszke2017pytorch}, with an emphasis on readability and documentation. Future iterations of \textbf{Pico} will introduce additional architectures, such as \texttt{pico-diffusion} and \texttt{pico-statespace} models, all adhering to the same guiding principle: every \texttt{pico-*} model must be simple, well-documented, and serve as a clear base implementation for the given model architecture.

To ensure efficient multi-GPU and distributed training, \texttt{pico-train} is built on Lightning Fabric \citep{lightning-fabric}—a framework that, like \textbf{Pico}, prioritizes simplicity and flexibility. Lightning Fabric enables users to scale up training across multiple GPUs or nodes without introducing excessive abstractions and ensures that the core training logic remains easy to understand and modify.

A distinguishing feature of \texttt{pico-train} is its systematic checkpointing and version control system. It automatically saves:
\begin{itemize}
    \item \textbf{Model states in both PyTorch- and Hugging Face-compatible formats} \citep{huggingface}. This dual-format checkpointing enables straightforward loading with vanilla PyTorch or integration into the Hugging Face ecosystem, facilitating downstream tasks such as fine-tuning, inference, or model sharing. Researchers can thus easily plug \texttt{pico-train} outputs into existing pipelines or community projects.

    \item \textbf{Intermediate activations and gradients.} At user-defined intervals, the library gathers layerwise activations and gradients from the forward and backward passes on the current training batch. Optionally, it can also capture these metrics from a fixed evaluation batch for consistent comparisons over training. Collecting these tensors at each checkpoint provides a granular record of how representations and gradient flows evolve over time.

    \item \textbf{Evaluation results.} During training, \texttt{pico-train} records user-defined evaluation metrics (e.g., validation perplexity or accuracy) alongside model checkpoints.
\end{itemize}
\vspace{-0.2em}
All checkpoints are automatically uploaded and version-controlled on Hugging Face, ensuring that researchers can revisit any point in training to analyze how the model evolved over time. These structured checkpoints integrate seamlessly with \texttt{pico-analyze}, enabling learning dynamics research with minimal setup.

\subsection{Pretokenized Dataset: \texttt{pretokenized-dolma}}

To simplify experimentation, \texttt{pico-train} uses a pre-tokenized, pre-chunked, and pre-shuffled version of Dolma \citep{soldaini2024dolma}, a large, open-source English dataset. 

To prepare the \texttt{pretokenized-dolma} dataset used in our experiments, we begin by downloading the Dolma corpus and selecting a random 30\% subset. The text is then tokenized using the \verb|allenai/OLMo-7B-0724-hf| tokenizer and split into fixed-length sequences of 2049 tokens (2048 + 1 for next-token prediction). We ensure consistency across shards by chunking token streams without overlap, dropping any remainder shorter than the full sequence length.

After tokenization and chunking, we shuffle the dataset and sample a fixed number of sequences per shard, generating 100 shards in total. The resulting dataset is saved as Parquet files and uploaded to our Hugging Face organization under \verb|pico-lm/pretokenized-dolma|.

To facilitate scalable loading and training, we further fine-shard the dataset into 10,000 pieces using a secondary script. These final shards are compact (~78MB each), randomly shuffled, pre-tokenized, and ready for streaming via the Hugging Face datasets API. This preprocessing ensures that all models see data in a consistent order, which is critical for learning dynamics analysis. We release all of the scripts we use for preprocessing data in our GitHub repository.

We release this preprocessed version of Dolma - \texttt{pretokenized-dolma} - on HuggingFace. Our dataset eliminates preprocessing overhead, guarantees consistency across experiments and can be loaded in as a streaming dataset, reducing the need for large amounts of storage.

Our design philosophy for the dataset is the same as for \texttt{pico-train}: minimalism, modularity, and transparency, that enable users to easily modify all aspects of the training pipeline. 

\subsection{\texttt{pico-analyze}: A General-Purpose Framework for Studying Learning Dynamics}

\texttt{pico-analyze} is a companion tool to \texttt{pico-train} designed to make analyzing learning dynamics seamless and reproducible. It directly integrates with the checkpoints saved by \texttt{pico-train}—including model weights, optimizer states, activations, and gradients—allowing researchers to systematically compute the learning dynamics of trained models.

At its core, \texttt{pico-analyze} follows a simple abstraction: it applies metrics to components. Metrics provide quantitative insights into various aspects of model behavior, while components define the specific model elements being analyzed. This design allows for flexible and fine-grained analysis of training dynamics.

\paragraph{Metrics.} Out of the box, \texttt{pico-analyze} supports a range of built-in metrics, including:
\begin{itemize}
    \item \textbf{Sparsity Measures}: \textit{Gini coefficient} \citep{hurley2009gini} and \textit{Hoyer metric} \citep{hoyer2004sparsity} gauge how concentrated the values of a tensor are around zero.

    \item \textbf{Rank-Based Metrics}: \textit{Proportional Effective Rank} \citep{diehlmartinez2024tending} captures a matrix’s “effective dimensionality,” while \textit{Condition Number} evaluates its numerical stability.

    \item \textbf{Representation Similarity}: \textit{CKA} \citep{kornblith2019cka} and \textit{PWCCA} \citep{morcos2018pwcca} compare activation patterns across layers or checkpoints, revealing how internal representations evolve.
    
    \item \textbf{Norms}: \textit{Frobenius}, \textit{Nuclear}, and \textit{Infinity} norms measure the scale of a tensor, spotlighting issues such as vanishing or exploding parameters.
\end{itemize}

\paragraph{Components.} Metrics can be computed on different types of components, which fall into two categories: 
\begin{itemize} 
\item \textbf{Simple components}: Individual weight matrices, gradients, or activations from a single layer. 
\item \textbf{Compound components}: Higher-level structures that combine multiple model elements. One example is the OV circuit, which tracks how information flows in transformer models by combining the value projection and output projection matrices in self-attention layers \cite{elhage2021mathematical}. 
\end{itemize}

This two-step abstraction is designed for extensibility—new metrics and component types can be easily defined, allowing researchers to tailor analyses to specific hypotheses about language model learning. 


\section{Usage Demonstration: Training and Analysis with \picolarge}

This section describes a practical workflow for training language models using \texttt{pico-train} and analyzing their learning dynamics using \texttt{pico-analyze}.

\subsection{Training Models with \texttt{pico-train}}

Setting up the \texttt{pico-train} codebase is as easy as:

\begin{center}
    \includegraphics[width=0.7\columnwidth]{chapters/pico/figures/demo/demo_setup.png}
\end{center}

We provide a simple \verb|setup.sh| script that uses Poetry \citep{poetry} to install dependencies and configure the environment.

Training a model requires only a configuration file. \texttt{pico-train} uses flexible dataclasses with sensible defaults that can be easily customized for each run. We report all of the defaults in \cref{tab:default_configs}. For demonstration purposes, we include a sample configuration file in \texttt{pico-train} that can be found under \verb|configs/demo.yaml|. Here we show an abridged version of this configuration:

\begin{figure}[h!] 
    \centering
    \includegraphics[width=0.7\columnwidth]{chapters/pico/figures/demo/demo_config.png}
    \caption{The abridged demo configuration setup that we include with \texttt{pico-train}.}
    \label{fig:demo_config}
\end{figure}

In general, the configuration file consists of six main components, each controlling key aspects of model training. 

Using this demo configuration file, we can now easily launch a training run:

\begin{center}
    \includegraphics[width=0.7\columnwidth]{chapters/pico/figures/demo/demo_run.png}
\end{center}

Training will start immediately, automatically saving learning dynamics checkpoints both locally and on Hugging Face.


\begin{table*}[h!]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
    \setlength{\tabcolsep}{8pt} % Adjust column spacing
    \footnotesize
    \begin{tabular}{|>{\centering\arraybackslash}p{3cm}|p{5cm}|p{5.5cm}|}
        \hline
        \textbf{Category} & \textbf{Parameter} & \textbf{Default Value} \\
        \hline
        \multirow{10}{*}{\textbf{Model}}  
            & Model Type & \texttt{pico\_decoder} \\
            & Hidden Dimension ($d_{\text{model}}$) & 768 \\
            & Number of Layers ($n_{\text{layers}}$) & 12 \\
            & Vocabulary Size & 50,304 \\
            & Sequence Length & 2,048 \\
            & Attention Heads & 12 \\
            & Key/Value Heads & 4 \\
            & Activation Hidden Dim & 3,072 \\
            & Normalization Epsilon & $1 \times 10^{-6}$ \\
            & Positional Emb. Theta & 10,000.0 \\
        \hline
        \multirow{7}{*}{\textbf{Training}}  
            & Optimizer & AdamW \\
            & Learning Rate & $3 \times 10^{-4}$ \\
            & LR Scheduler & Linear w/ Warmup \\
            & Warmup Steps & 2,500 \\
            & Gradient Accum. Steps & 128 \\
            & Max Training Steps & 200,000 \\
            & Precision & BF16 Mixed \\
            %& Accelerator & CUDA \\
            %& Nodes & 1 \\
            %& Devices per Node & 1 \\
        \hline
        \multirow{3}{*}{\textbf{Data}}  
            & Dataset Name & \texttt{pretokenized-dolma} \\
            & Batch Size & 1,024 \\
            & Tokenizer & \texttt{allenai/OLMo-7B-0724-hf} \\
        \hline
        \multirow{6}{*}{\textbf{Checkpointing}}  
            & Auto Resume & True \\
            & Save Every N Steps & 1,000 \\
            %& Save to Hugging Face & False \\
            & Learning Dynamics Layers & \texttt{"attention.v\_proj",} \newline \texttt{"attention.o\_proj",} \newline \texttt{"swiglu.w\_2"} \\
            & Learning Dynamics Data & \texttt{pretokenized-paloma-tinsy} \\
        \hline
        \multirow{3}{*}{\textbf{Evaluation}}  
            & Metrics & \texttt{["paloma"]} \\
            & Eval Dataset Name & \texttt{pretokenized-paloma-tinsy} \\
            & Eval Batch Size & 16 \\
        \hline
        \multirow{3}{*}{\textbf{Monitoring}}  
            & Logging Level & INFO \\
            & Log Every N Steps & 100 \\
            %& Save to Weights \& Biases & False \\
        \hline
    \end{tabular}
    \caption{Default configuration settings used in \texttt{pico-train}, organized by configuration category.}
    \label{tab:default_configs}
\end{table*}

\subsection{Analyzing Models with \texttt{pico-analyze}}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/pico/figures/demo_full_run.png}
    \caption{Sample analysis output of a dummy model trained using \texttt{pico-train} and analyzed with \texttt{pico-analyze}. The top row illustrates some sample metrics computed by \texttt{pico-analyze} on the dummy model that was trained for 100 steps using \texttt{pico-train}, the bottom row shows training logs.}
    \label{fig:demo_full_run}
\end{figure}


Once training is complete, we can inspect various aspects of the model's learning dynamics using \texttt{pico-analyze}. Setup follows the same process as \texttt{pico-train}. \texttt{pico-analyze} operates directly on the structured checkpoints saved by \texttt{pico-train}, using model states, gradients, activations to provide insights into how the model evolved over training. To streamline experimentation, \texttt{pico-analyze} employs a YAML-based configuration system, enabling users to specify which layers, metrics, and training steps to run analyzes over. Below is an example of an abridged YAML configuration:

\begin{figure}[h!] 
    \centering
    \includegraphics[width=0.7\columnwidth]{chapters/pico/figures/demo/demo_config_analyze.png}
    \caption{The abridged demo configuration setup that we include with \texttt{pico-analyze}.}
    \label{fig:demo_analysis_config}
\end{figure}

In this example, we configure \texttt{pico-analyze} to compute the CKA metric for the OV circuit in layers 0 and 11 of a trained model.

In general, users can easily customize their own YAML configuration files by specifying different model components and analysis metrics. The full list of available metrics is provided in \cref{tab:pico_analyze_metrics}. We include a complete sample configuration at \texttt{configs/demo.yaml} within the \texttt{pico-analyze} library.

With a configuration file in place, launching an analysis is as simple as running:

\begin{center}
    \includegraphics[width=0.7\columnwidth]{chapters/pico/figures/demo/demo_run_analyze.png}
\end{center}

This command will execute the analysis specified in the configuration file directly on the model checkpoints uploaded to Hugging Face by the corresponding \texttt{pico-train} run, identified using \verb|repo_id| and \verb|branch|. Once run, the analysis results can be stored locally or uploaded to Weights \& Biases (wandb) \citep{wandb} for visualization, allowing for efficient tracking of learning dynamics across multiple runs.  

By integrating seamlessly with \texttt{pico-train}, \texttt{pico-analyze} provides a structured, reproducible framework for studying learning dynamics, enabling researchers to explore how representations form and evolve over training. In \cref{fig:demo_full_run} we visualize the output of this analysis and more generally the typical output of running the two libraries. 


\section{Pico Model Suite}

We train a suite of \texttt{pico-decoder} models at various scales using \texttt{pico-train}, all of which are open-sourced on our Hugging Face organization. These models range from 11M to 570M parameters, with plans to extend up to 1B, and serve both as evaluations of our training pipeline and as testbeds for research on scaling laws and interpretability. We provide a comparison of these models in \cref{tab:pico-decoder-configs}.

\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|p{0.30\textwidth}||p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|p{0.13\textwidth}|}
    \hline
    \multicolumn{5}{|c|}{\textbf{Pico-Decoder Model Comparison}} \\
    \hline
    \textbf{Attribute} & \texttt{tiny} & \texttt{small} & \texttt{medium} & \texttt{large} \\
    \hline
    Parameter Count & 11M & 65M & 181M & 570M \\
    Hidden Dimension ($d_{\text{model}}$) & 96 & 384 & 768 & 1536 \\
    Feed-forward Dim & 384 & 1536 & 3072 & 6144 \\
    Training Time (50K steps) & 10d 9h & 15d 6h & 17d 15h & 23d 16h \\
    \hline
    \end{tabular}
    \vspace{0.5em}
    \caption{Comparison of \texttt{pico-decoder} model variants trained with default \texttt{pico-train} configurations. Except for hidden and feed-forward dimension, all models share the training settings detailed in \cref{tab:default_configs}. Models are scheduled for 200,000 total training steps on 16 NVIDIA A100-SXM4-80GB GPUs; the listed training times correspond to the initial 50,000 steps.}
    \label{tab:pico-decoder-configs}
    \end{table}

Each model is scheduled to train for 200,000 steps (covering 400B tokens), though we have already released early checkpoints at 125,000 steps (250B tokens). We evaluate these initial checkpoints on the Paloma benchmark \citep{magnusson2024paloma}, comparing performance against established decoder models. Notably, our partially trained models are quickly closing in on the results of fully trained counterparts, despite running on an academic-level compute budget (4 nodes of 4 A100s each).

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccc}
\hline
\textbf{Family} & \textbf{Size} & \textbf{\#Tokens} & \textbf{Perplexity} \\
\hline \hline

\textbf{\pico} & 11M & 100B & 167.72 \\
             & 65M & 100B & 46.41 \\
             & 181M & 100B & 34.04 \\
             & 570M & 100B & 26.52 \\
\hline

\textbf{Pythia} & 14M & 300B & 86.64 \\
                & 70M & 300B & 43.76 \\
                & 160M & 300B & 29.96 \\
                & 410M & 300B & 20.55 \\
\hline

\textbf{OPT} & 125M & 300B & 27.22 \\
             & 350M & 300B & 20.91 \\
\hline

\textbf{OLMO} & 1B & 2T & 13.40 \\
\hline
\end{tabular}
\caption{Perplexity of various models on the Paloma dataset. All models use similar BPE-based tokenizers, with nearly identical vocabulary sizes.}
\label{tab:model_perplexities}
\end{table}

\vspace{-0.5em}
\section{Conclusion}

We introduce \pico, a modular framework for training and analyzing small to medium-scale language models. \texttt{pico-train} provides a minimal yet flexible environment for training language models that emphasizes transparency and checkpointing for learning dynamics research. \texttt{pico-analyze} then uses these checkpoints to facilitate a broad set of learning dynamics analyses including model convergence patterns, sparsity, and rank dynamics.

In addition, we presented the \textbf{Pico} Model Suite, a collection of \texttt{pico-decoder} models ranging from 11M to 570M parameters, with future plans to scale to 1B+. These models are trained under controlled conditions, supporting research on scaling laws and representation learning.

By bridging training and analysis in a lightweight, extensible framework, \textbf{Pico} lowers the barrier for studying language model learning dynamics. Future work will expand the model suite and integrate advanced interpretability tools, further enhancing its utility for rigorous and reproducible research.





