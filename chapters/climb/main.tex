\chapter{Curriculum Learning for Infant-inspired Model Building: A Framework for Human-like Language Acquisition}
\label{chapter:CLIMB}

\newtcbox{\lightorangehighlight}{on line, colback=orange!10, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}
\newtcbox{\darkorangehighlight}{on line, colback=orange!25, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}

\newtcbox{\lightgreenhighlight}{on line, colback=green!10, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}
\newtcbox{\darkgreenhighlight}{on line, colback=green!25, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}
\newtcbox{\verydarkgreenhighlight}{on line, colback=green!40, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}

\newtcbox{\lightpurplehighlight}{on line, colback=purple!10, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}
\newtcbox{\darkpurplehighlight}{on line, colback=purple!25, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}
\newtcbox{\verydarkpurplehighlight}{on line, colback=purple!40, boxrule=0.2mm, left=0.5mm, right=0.2mm, top=0.2mm, bottom=0.2mm}

% Color commands for relative differences
\newcommand{\posval}[1]{\textcolor{green!70!black}{#1}}
\newcommand{\negval}[1]{\textcolor{red!70!black}{#1}}


While Chapter 2 established how human learning principles can inform language modelling, this chapter puts these ideas into practice through a concrete framework for curriculum learning. I present \climb (Curriculum Learning for Infant-inspired Model Building), a systematic approach to implementing developmentally plausible training protocols for language models.\footnote{Code available at: \url{https://github.com/rdiehlmartinez/climb}.} This work is motivated by three key observations.

First, the current paradigm of language model training that relies on large datasets and computational resources stands in stark contrast to human language acquisition. Children acquire sophisticated language capabilities from only a few million words per year \citep{gilkerson2017mapping}, while state-of-the-art language models require trillions of tokens and extensive computational resources \citep{zhang2021need, zhao2023llmsurvey}. This discrepancy raises fundamental questions about the efficiency of current training approaches.

Second, conventional language model training differs from human learning in its structure: models operate on a predetermined static vocabulary and optimise a fixed objective on randomly shuffled data. In contrast, human language acquisition follows a carefully orchestrated progression: from babbling to simple utterances, and eventually to complex syntax and abstract meaning. This developmental trajectory suggests that structured learning protocols might enable more efficient model training.

Third, while curriculum learning has shown promise in various machine learning domains \citep{bengio2009curriculum}, its application to language modelling remains fragmented. Previous work has explored individual aspects such as data sequencing, or objective simplification but lacks a unified framework for implementing and evaluating these strategies, particularly in resource-constrained settings.

The idea that structured, developmental training might benefit learning has a long history in connectionist research. \citet{elman1993learning} demonstrated that simple recurrent networks trained on artificially constructed languages could only succeed when they began with limited working memory that gradually matured. This finding suggests that developmental constraints might be necessary for mastering complex domains like language. However, \citet{rohde1999language} subsequently challenged this finding, showing that when languages were made more realistic by including syntactic constraints, starting with simplified inputs actually hindered rather than helped acquisition. This early debate foreshadowed the mixed results observed in modern curriculum learning research and raises important questions about whether structured curricula genuinely benefit neural networks. Recent comprehensive studies have found that curricula offer only marginal benefits on standard benchmarks, with randomly ordered samples often performing comparably \citep{wu2021when, surkov2022datacurricula}. In the specific context of language model pre-training, \citet{campos2021curriculum} found no compelling evidence that linguistically motivated curricula improve training outcomes. These findings underscore a central tension in the field: while the cognitive motivation for curriculum learning remains compelling, translating these principles into practical machine learning benefits is non-trivial.

To explore this tension systematically, I develop \climb within the context of the BabyLM Challenge \citep{warstadt2023babylm1}, which provides an ideal testbed for exploring cognitively plausible training protocols under strict data constraints (10 million words). This framework systematically implements three curriculum dimensions:

\begin{itemize}
    \item \textbf{Vocabulary Curriculum:} Gradually expanding the model's lexicon, mirroring how children build their vocabulary from concrete nouns and verbs to more abstract terms.
    \item \textbf{Data Curriculum:} Structuring training data to progress from simpler to more complex linguistic structures, following the developmental trajectory observed in child language acquisition.
    \item \textbf{Objective Curriculum:} Evolving learning objectives from broad linguistic categories to specific token prediction, similar to how children first grasp word classes before mastering precise lexical distinctions.
\end{itemize}

\paragraph{Chapter Contributions} This chapter makes several key contributions:

\begin{enumerate}
    \item I establish a novel framework (\climb) for categorising and implementing curriculum learning strategies that simulate human language acquisition. Importantly, this framework provides a reusable codebase that can be used to test other cognitively-motivated training strategies in the future.
    
    \item Through extensive experimentation across three curriculum dimensions and multiple configurations, I provide a comprehensive evaluation of curriculum learning approaches in resource-constrained settings and document the limitations of these approaches.
    
    % \item I report negative results transparently: the developmentally-motivated curricula, though theoretically compelling, fail to reliably improve upon baseline performance. This finding underscores that intuitive cognitive parallels do not straightforwardly translate to practical benefits.

\end{enumerate}

\paragraph{Addressing RQ1} This chapter addresses the first research question of this thesis. By translating developmental principles into three structured curricula and systematically evaluating them, I provide a rigorous test of whether cognitively inspired training protocols improve the efficiency of small-scale pre-training. The largely negative results reported here offer a cautionary perspective on \textbf{RQ1}: while the motivation for drawing on human learning remains compelling, simple operationalisations of developmental principles may not transfer straightforwardly to neural language models. 

\vspace{1em}

The remainder of this chapter is organised as follows: \cref{sec:climb-methodology} details the setup of the experiments, including the model architecture and training setup. \cref{sec:three-dimensional-framework} introduces the three-dimensional framework for curriculum learning. \cref{sec:climb-results} presents the results and analysis, comparing different curriculum strategies and their combinations. \cref{sec:climb-conclusion} summarises the implications of my findings and directions for future work.

\section{Methodology}
\label{sec:climb-methodology}

%The work in this chapter is conducted within the context of the BabyLM Challenge \citep{warstadt2023babylm1}, which provides a constrained setting (10 million words) for exploring cognitively plausible training protocols. 
Before implementing the curriculum learning strategies, I first establish a strong baseline model and data preprocessing pipeline. 

\subsection{Model Architecture and Training Setup}
All of the models in this chapter are based on an 8-layer Transformer language model (\cref{subsec:baseline}) comparable to the BabyBERTa model \citep{huebner2021babyberta}. This architecture choice was motivated by the success of smaller models in low-resource settings, as demonstrated in the original BabyBERTa work. For all experiments, I leverage several key tools and frameworks to ensure robust and reproducible training. I use the Hugging Face Transformers library \citep{transformers} to implement the model, while Weights \& Biases \citep{wandb} enables comprehensive performance tracking and experiment monitoring. I use Hydra \citep{hydra} for experiment configuration management, allowing me to systematically explore different curriculum learning strategies. All training is conducted on a high performance computing cluster to ensure efficient model development and experimentation.

\subsection{Training Data}
\label{subsec:data}

\subsubsection{Data Source and Size}
I work exclusively with the training data provided in the \textsc{strict-small} track of the BabyLM challenge. This dataset is carefully constrained to 10 million words, compiled from 10 diverse corpora to ensure a representative sample of language use. Through the preprocessing pipeline, I reduce the initial 1,058,740 newline-delineated samples to 335,858 instances, corresponding to approximately 9.4 million words.\footnote{The word count is estimated by whitespace splitting, following the same metric used by the task organisers. When applying a tokeniser, the preprocessed dataset contains 11.7 million words (including punctuation) or 13.6 million subwords, reflecting the additional tokens introduced by subword tokenisation.} This reduction in instances is primarily due to the concatenation strategy for shorter sequences, which I discuss in detail below.

\subsubsection{Data Preprocessing}
The diversity of the data sources (spanning books, subtitles, transcripts, and articles) necessitated careful curation to ensure consistency across corpora. My preprocessing pipeline implements several key transformations. First, I standardise the text through lowercasing and punctuation normalisation. I then apply regular expressions to standardise typographical conventions, ensuring consistent representation of numbers, dates, and special characters. The pipeline also removes extraneous content that could interfere with language modelling, including page numbers, bibliography entries, plain text tables, and one-word on-screen actions commonly found in subtitle data.

For transcribed speech corpora (with the exception of the British National Corpus), I implement a special concatenation strategy. Contiguous sections of five lines are combined into single data instances, addressing the challenge of relatively short sequence lengths in speech data. This approach helps maximise the effective use of the model's context window. Finally, at the point of model input, I join data segments to make full use of the available sequence length, which is set to 128 subtokens. This joining strategy is particularly important for maintaining the coherence of longer texts while staying within the model's context window constraints.

To promote reproducibility and further research in this area, I provide the cleaned version of the 10M word dataset on Hugging Face, along with the complete preprocessing scripts.\footnote{\url{https://huggingface.co/cambridge-climb}} This includes all the transformations described above, as well as the POS tagging pipeline that I discuss in the next section.

\subsubsection{Part-of-Speech Tagging}

While the preprocessing pipeline ensures consistent text formatting, I also need to capture linguistic structure to support the curriculum learning experiments. In particular, the vocabulary and objective curricula (\cref{subsec:vocab-cl} and \cref{subsec:objective-cl}) rely on syntactic information (i.e., POS tags) to guide the learning process. However, the \textsc{strict-small} track's prohibition on external resources presented a unique challenge for implementing POS tagging, as I could not use supervised taggers. From a cognitive-plausibility standpoint this restriction is logical; human infants must also infer syntactic roles of words indirectly as part of their learning process.

To address this, I developed an unsupervised POS-tagging approach that leverages the \texttt{anchor-features} algorithm \citep{stratos2016unsupervisedpos}, which identifies ``anchor words" strongly associated with specific grammatical categories and uses these to learn a Hidden Markov Model (HMM). I ran this algorithm on the training dataset, and generated 30 clusters of features that each capture some latent syntactic information. I then manually mapped each cluster to a universal POS tag \citep{petrov2012universalpos}, with several clusters often mapping to the same grammatical category. Notably, the initial HMM clustering approach failed to identify distinct groups for adverbs (ADV) and unknown tokens (X). 

\begin{wrapfigure}{r}{0.41\textwidth}
    \vspace{-1em}
    \centering
    \small
    \begin{tabular}{lrrr}
    \toprule
    POS & Precision & Recall & F1 \\
    \midrule
    NOUN & 0.786 & 0.790 & 0.788 \\
    DET & 0.820 & 0.772 & 0.795 \\
    CONJ & 0.969 & 0.821 & 0.895 \\
    NUM & 0.592 & 0.799 & 0.681 \\
    PRON & 0.592 & 0.962 & 0.733 \\   
    VERB & 0.816 & 0.823 & 0.819 \\
    PRT & 0.501 & 0.701 & 0.584 \\
    ADJ & 0.673 & 0.554 & 0.608 \\
    ADP & 0.842 & 0.888 & 0.864 \\
    PUNC & 0.944 & 0.960 & 0.952 \\
    \bottomrule
    \end{tabular}
    \caption{\label{tbl:unsupervised-pos-performance} Performance metrics of the unsupervised POS tagger compared to NLTK's supervised system.}
    \label{fig:unsupervised-pos-performance}
\end{wrapfigure}

As a means of evaluating how well this unsupervised POS tagger performs, I compare it to the performance of NLTK's supervised POS tagger \citep{bird2009natural}. \cref{fig:unsupervised-pos-performance} summarises the precision, recall, and F1 scores for each POS tag, when using the tags generated by the NLTK tagger as a reference. Overall, the unsupervised tagger showed strong performance on punctuation (F1: 0.952) and conjunctions (F1: 0.895), likely due to their consistent usage patterns. However, it struggled more with particles (F1: 0.584) and adjectives (F1: 0.608), which may have more variable usage patterns or semantic dependencies. These variations highlight the challenges of unsupervised grammatical category learning in low-resource settings.

% \subsubsection{Data Availability and Observations}
%  Interestingly, the experiments revealed that models trained on the raw, unprocessed data often outperformed those trained on the carefully preprocessed version. This counterintuitive finding, which I discuss in detail in \cref{sec:climb-discussion}, suggests that the linguistic ``noise" in raw data may actually provide valuable learning signals for language models, particularly in low-resource settings.

\subsection{Vanilla Models}
\label{subsec:baseline}

\begin{table*}
\centering
\small
\setlength{\tabcolsep}{4pt}  % Reduce column spacing
\begin{tabular}{l | rrrrr | rrrr}
\toprule
Model  & L & H & Hidden & Int. & Vocab & Steps & BLiMP$\uparrow$ & BLiMP.S$\uparrow$ & Perplexity$\downarrow$ \\
\midrule
Small  & 8 & 8 & 256 & 2,048   & 8,192   & 250K      & 75.43      & 61.14       & 9.46    \\
Medium & 10 & 10 & 500 & 2,000 & 8,192  & 156K      & 76.45      & 63.28        & 9.05  \\
Large  & 12 & 12 & 768 & 3,072 & 8,192   & 94K      & 75.80      & 60.83      & 9.34 \\[2mm]
\hline \\
Small  & 8 & 8 & 256 & 2,048   & 16,384  & 250K      & 76.16      & 60.85       & 13.80    \\
Medium & 10 & 10 & 500 & 2,000  & 16,384 & 94K      & 76.09      & 60.03        & 13.80     \\
Large  & 12 & 12 & 768 & 3,072 & 16,384  & 62K      & 75.08      & 63.45      & 14.22     \\
\bottomrule
\end{tabular}
\caption{\label{tbl:baseline-size-comparison} The vanilla BabyBERTa-style models evaluated on original BLiMP and the supplemental BLiMP-like tasks prepared for BabyLM (BLiMP.S=BLiMP Supplement). Models are grouped by their vocabulary sizes. L denotes the number of Transformer layers and H the number of attention heads per layer. The Hidden dimension (Hidden) represents the size of token representations at each layer, while the Intermediate dimension (Int.) indicates the expanded dimension size in the feed-forward network (typically 4x the hidden dimension).}
\end{table*}

I investigate three different sizes of a vanilla Pre-Layer Norm RoBERTa model \citep{liu2019roberta} based on the BabyBERTa model \citep{huebner2021babyberta}: `small', `medium', and `large'. \cref{tbl:baseline-size-comparison} lists the model configurations and presents the results for the different model sizes evaluated by perplexity, on BLiMP \citep{warstadt2020blimp} and on the supplementary BLiMP-like tasks issued by the BabyLM organisers (`Blimp.Supp'). The medium model with a small vocabulary size performs the best overall; however, the small model achieves similar results. Therfore, to save on compute and keep to the restrained intentions of the \textsc{strict-small} track, I used the small model in the subsequent curriculum learning experiments.

\begin{wrapfigure}{r}{0.40\textwidth}
    \vspace{-1em}
    \centering
    \small
    \begin{tabular}{lc}
    \toprule
         Parameter& Value\\
    \midrule
         Layer Norm EPS& 1e-5 \\
         Tie Word Embeddings & False \\
         Learning Rate & 0.001 \\
         Optimiser & AdamW \\
         Scheduler Type & Linear\\
         Max Steps & 400,000 \\
         Warm-up Steps & 100,000\\
         Per Device Batch Size & 32 \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter settings which are constant across the vanilla models described in \cref{subsec:baseline}.}
    \label{tbl:baseline_hyperparams}
    \vspace{-1em}
\end{wrapfigure}

All models use BPE tokenisation \citep{gage1994bpe} with a vocabulary of 8,192 because it yields better overall performance compared to a larger vocabulary of 16,384. The tokenisers I use in the experiments are trained on the cleaned data that I processed using the steps outlined in \cref{subsec:data}. In pilot experiments, I did not observe the benefits reported by \citet{huebner2021babyberta} from removing the unmasking procedure that is a standard component of the MLM objective \citep{devlin2019bert}, and therefore did not investigate this option further. In \cref{tbl:baseline_hyperparams}, I report all of the hyper-parameters I use throughout my experiments.

All of the curriculum learning methods in the following sections are applied on top of the small vanilla BabyBERTa-style baseline. To isolate the effect of the curriculum-learning training process, I fix the architecture of the model and the model hyper-parameters. I train all models using an AdamW optimiser with linear scheduling \citep{loshchilov2019decoupled}.

\section{A Three-Dimensional Framework for Curriculum Learning}
\label{sec:three-dimensional-framework}
Curriculum learning \citep{bengio2009curriculum} is a machine-learning paradigm which optimises a model's performance by gradually increasing the difficulty of training over time according to a set schedule (a `curriculum'). This approach is based on the idea that learning should proceed from easy to hard, inspired by the way that humans learn \citep{elman1993learning}. Within the context of curriculum learning, one of the central questions is how to define the difficulty of the learning process over the course of training. As \citet{soviany2022curriculum} note in their survey, the necessity of finding a way to rank samples from easy to hard can fundamentally limit the applicability of curriculum approaches. \citet{rampp2024difficulty} further demonstrate that different scoring functions for sample difficulty exhibit varying robustness and often fail to correlate with curriculum learning performance, raising questions about whether any single difficulty metric can reliably guide training. 

%This challenge is particularly acute in NLP, where linguistic complexity is multidimensional and context-dependent.

%In a recent survey, \citet{soviany2022curriculum} decompose this challenge into two main sub-problems: determining a sorting mechanism to assess the difficulty of instances and developing a pacing function for increasing difficulty over time. 

\begin{table*}[H]
    \centering
    \small
    \begin{tabular}{lll}
    \toprule
    \textbf{Curriculum Type} & \textbf{Parameter} &\textbf{Variants} \\
    \midrule
     \multirow{2}{*}{Vocabulary} & Selection & frequency, word class, mixed \\
     & Pacing & linear, logarithmic \\
     \midrule
     \multirow{3}{*}{Data} & Difficulty & source, unigram perplexity, self-perplexity \\
     & Pacing & linear, logarithmic \\
     & Initial Perplexity & unigram, random \\
      \midrule
     \multirow{2}{*}{Objective} & Tasks & noun-verb prediction, POS prediction, MLM\\
     & Learning Setup & sequential, multitask \\
    \bottomrule
    \end{tabular}
    \caption{\label{tbl:configurations} Curriculum learning experiments overview}
\end{table*}

\subsection{Defining Curricula across Three Dimensions}
Previous work in curriculum learning typically focuses on difficulty from a data-centric perspective, however, I note that difficulty can arise from (at least) three major elements of training a neural model: the input representation, the data sampling, and the training process. I explore curriculum learning strategies across three distinct dimensions: the vocabulary, the order of training data, and the objective function.

\subsection{Vocabulary Curriculum}
\label{subsec:vocab-cl}

I propose a novel \textbf{vocabulary curriculum} that gradually expands the model's lexicon during training, inspired by how children build their vocabulary. While large language models typically begin training with a full, fixed vocabulary, children acquire language through a more progressive process, starting with a small vocabulary that expands rapidly at a rate of eight to ten words per day \citep{weizman2001lexical}. Moreover, children prioritise learning certain word classes before others, typically mastering verbs and nouns before progressing to more abstract parts of speech \citep{bergelson2015early}.

To simulate this developmental trajectory, I implement a curriculum that begins with a limited vocabulary (10\% of tokens) and gradually expands it over the course of training. During the initial stages, tokens not included in the active vocabulary are mapped to an unknown token (\textsc{UNK}) representation. The curriculum regime spans from 25,000 to 350,000 training steps, after which all vocabulary tokens become available for the final 50,000 steps of training.

I explore three strategies for selecting which tokens to include at each stage of the curriculum:

\begin{enumerate}
    \item \lightgreenhighlight{\textbf{Frequency-based selection} (Freq)} Tokens are chosen based on their frequency in the corpus, approximated using the BPE tokeniser's ID assignments (lower IDs correspond to more frequent tokens).
    
    \item \darkgreenhighlight{\textbf{Word class-based selection} (POS)} Tokens are selected according to their grammatical category, following a progression from lexical to functional classes as observed in child language acquisition \citep{bergelson2015early}: NOUN, VERB, ADJ, PRON, DET, ADP, NUM, CONJ, PRT, PNCT. All words within a given part-of-speech category are introduced simultaneously.
    
    \item \verydarkgreenhighlight{\textbf{Hybrid selection} (Hybrid)} I combine frequency and word class constraints by sorting words by their frequency within each part-of-speech category. This approach allows for more granular control over vocabulary expansion while maintaining the developmental progression of word classes.
\end{enumerate}

The rate at which the vocabulary expands is controlled by a pacing function. I experiment with both linear and logarithmic pacing functions, with the latter potentially better reflecting the rapid early vocabulary growth observed in children. \cref{fig:pacing_fn} illustrates how the percentage of unmasked vocabulary increases over the course of training under these different pacing regimes. This approach represents, to my knowledge, the first systematic attempt at implementing a vocabulary curriculum in language model training, offering a more cognitively plausible alternative to the standard practice of training with a fixed, full vocabulary from the outset.

% \newpage

\subsection{Data Curriculum}
\label{subsec:data-cl}

\begin{wrapfigure}{r}{0.45\textwidth}    
    \vspace{-6em}
    \centering
    \small
    \renewcommand{\arraystretch}{0.9}
    \begin{tabular}{cl}
    \toprule
    \textbf{Difficulty} & \textbf{Corpora} \\
    \midrule
    1 & AO-CHILDES \\
    2 & BNC Spoken, Switchboard \\
    3 & Open Subtitles, QED \\
    4 & CBT, Children's Stories \\
    5 & Simple Wikipedia \\
    6 & Wikipedia, Gutenberg \\
    \bottomrule
    \end{tabular}
    \caption{\label{tbl:source_order}  Assigned difficulty levels from 1 (easiest) to 6 (hardest).}
    \vspace{-2em}
\end{wrapfigure}

I implement a \textbf{data curriculum} that structures the presentation of training instances to mirror how children learn language. Unlike conventional language model training, which typically presents randomly ordered data after minimal cleaning, my approach carefully sequences training instances based on their difficulty. This is motivated by the observation that the order of training instances can significantly impact model performance \citep{schluter2018data} and by the ``Goldilocks effect" in human learning, where optimal learning occurs when stimuli are neither too easy nor too hard \citep{kidd2012goldilocks}. I explore two complementary approaches to determining instance difficulty:

\begin{enumerate}

\item \lightpurplehighlight{\textbf{Source-based difficulty} (Source)} Following \citet{huebner2021babyberta}, I order datasets based on their source, considering spoken samples as `easier' and written texts as `harder'. This ordering reflects the natural progression of language acquisition, where children typically learn from spoken language before mastering written forms. I implement a six-level difficulty hierarchy (\cref{tbl:source_order}), ranging from child-directed speech (CHILDES) to complex written texts (Wikipedia, Gutenberg).

\item \darkpurplehighlight{\textbf{Static perplexity-based difficulty} (Static PPX)} While source-based difficulty provides a useful heuristic, it fails to capture the variation in complexity within each corpus. To address this limitation, I implement a more fine-grained approach using perplexity as a model-intrinsic metric of instance difficulty. Perplexity measures how well a language model predicts a sequence of words, with lower perplexity indicating that the model finds the sequence more predictable and thus potentially easier to learn from. 

 I explore two distinct approaches to perplexity-based difficulty assessment. The first approach uses a static assessment, where I employ a unigram language model to evaluate perplexity once at the start of training. This method provides a simple, computationally efficient baseline that captures basic frequency patterns in the data. The unigram model's perplexity scores remain fixed throughout training, offering a consistent difficulty ranking of instances that reflects the inherent complexity of the text based on word frequencies.

\item \verydarkpurplehighlight{\textbf{Dynamic perplexity-based difficulty} (Dynamic PPX)} The second approach implements a dynamic assessment strategy, where I periodically re-evaluate perplexity using the current model state. I perform these reassessments every 25,000 training steps, allowing the difficulty assessment to evolve with the model's growing capabilities. This dynamic approach better reflects the ``Goldilocks effect" in learning, where optimal progress occurs when instances are neither too easy nor too hard \citep{kidd2012goldilocks}. As the model learns and develops, instances that were initially challenging may become more manageable, while others may reveal hidden complexities that were not apparent at first.

The dynamic approach presents several unique challenges that require careful consideration. The primary challenge arises at the start of training, when the model lacks sufficient exposure to provide meaningful perplexity scores. I address this initialisation challenge through two complementary strategies. First, I can use a separately trained unigram model for initial perplexity evaluation, which provides a reasonable starting point for difficulty assessment (Dynamic PPX-U). Alternatively, I can begin with random sampling for the first 25,000 steps before switching to model-based perplexity evaluation (Dynamic PPX-R).

The periodic reassessment of perplexity every 25,000 steps creates an adaptive curriculum that evolves with the model's capabilities. This approach identifies instances that have become too easy (exhibiting low perplexity) and potentially deprioritise them in the training schedule. Simultaneously, the model can maintain focus on instances that remain challenging (showing high perplexity) and discover instances that have moved into the ``Goldilocks zone" of optimal difficulty for the current model state.

\end{enumerate}

\subsection{Objective Curriculum}
\label{subsec:objective-cl}

I develop an \textbf{objective curriculum} that evolves the learning task from broad linguistic categories to specific token prediction, mirroring how children progress from understanding word classes to mastering precise lexical distinctions. This approach is motivated by the observation that human language learning is guided by interactions with other agents (e.g., adult caregivers, siblings) who help shape the learning process. In machine learning, these interactions are modeled through the objective function that guides the model's optimisation.

The popular approach to training language models relies on an objective function, such as MLM (defined in Chapter 2) \citep{devlin2019bert}. However, psycholinguistic research suggests that this may not be cognitively plausible as an approximation of child language acquisition \citep{caucheteux2023evidence}. The MLM objective presents a challenging discriminative classification task, requiring the model to predict a masked token's identity from the entire vocabulary (an $N$-way classification problem). This stands in contrast to how children learn, where they first develop sensitivity to distributional patterns and gradually learn to recognise lexical categories before mastering specific word forms \citep{alishahi2010computational, gleitman1990structural}.

To better align with this developmental trajectory, I implement a curriculum that gradually increases the specificity of the learning objective. I begin with broader linguistic categories and progressively narrow down to specific token prediction. This approach is inspired by recent work that simplifies classification tasks by reducing the number of possible classes from $N$ to $K$ (where $K << N$). Previous research has explored mapping rare words to hypernyms \citep{bai2022better} or replacing words with part-of-speech tags \citep{wang2023language} or syntactic dependency relations \citep{cui2022lert}. These approaches significantly reduce task difficulty by working with a smaller set of categories.

In the implementation, I use the unsupervised POS tagger to estimate word classes and experiment with two levels of classification granularity:
\begin{enumerate}
    \item A three-way classification distinguishing between VERB, NOUN, and OTHER categories
    \item A more detailed ten-way classification using universal POS tags
\end{enumerate}

I explore two strategies for implementing this curriculum:

\begin{enumerate}
    \item \lightorangehighlight{\textbf{Sequential learning} (Seq)} I first train the model to predict word classes, then transition to the full MLM objective. This approach mirrors the developmental progression observed in children, where they first learn to distinguish between major word classes before mastering specific lexical items.
    
    \item \darkorangehighlight{\textbf{Multitask learning} (MT)} I train the model to simultaneously predict both word classes and specific tokens, with separate task heads and optimisers for each objective. This approach allows the model to benefit from both coarse-grained and fine-grained learning signals throughout training.
\end{enumerate}

The psycholinguistic literature remains divided on how exactly word learning progresses from memorising specific lexical items to developing generalised representations of word classes \citep{clark2015first}. This framework provides a flexible approach to studying this progression by enabling systematic investigation of how different objective functions affect the acquisition of linguistic knowledge. By varying the timing and combination of learning objectives, I can explore different hypotheses about the relationship between category learning and specific word acquisition in language development.

This objective curriculum represents a novel approach to making language model training more cognitively plausible while maintaining the benefits of the MLM objective. By starting with broader linguistic categories and gradually increasing specificity, I aim to create a learning trajectory that better reflects human language acquisition while potentially improving the model's ability to generalise from limited training data.

\subsection{Pacing Functions} 
\label{subsec:pacing-functions}

Once a notion of difficulty is set, a pacing function is needed to govern how quickly the model will progress from training on easier examples to training on harder ones \citep{wu2021when}. I experiment with two different pacing functions: linear and logarithmic. Linear pacing functions involve a steady and consistent advancement through the curriculum. This approach ensures a gradual increase in difficulty over time. Logarithmic pacing functions, on the other hand, emphasise early exposure to ``easier" concepts, with diminishing increments as the model's capabilities are assumed to increase. Both pacing functions have been proposed in the broader curriculum learning literature \citep{bai2022better, li2021curriculum, wu2021when}. %In \cref{fig:pacing_fn}, I illustrate the two pacing functions for the vocabulary curriculum.

% \begin{wrapfigure}{l}{0.50\textwidth}
%     \vspace{-1em}
%     \centering
%     \includegraphics[width=0.45\textwidth]{chapters/climb/figures/pacing_fns.png}
%     \caption{Illustration of the linear and logarithmic pacing functions used in my vocabulary curriculum experiments. The red dotted lines denote the curriculum regime, during which the percentage of unmasked words available to the model grows.}
%     \label{fig:pacing_fn}
%     \vspace{-1em}
% \end{wrapfigure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{chapters/climb/figures/babylm_blimp_diffs_boxplots.png}
%     \vspace{1em}  % Add some vertical space between figures
%     \includegraphics[width=0.9\textwidth]{chapters/climb/figures/babylm_superglue_diffs_boxplots.png}
%     \caption{Comparison of model performance across BLiMP (top) and SuperGLUE (bottom) tasks. The plots show the differences in performance between the curriculum learning models and baseline models. Types of curriculum learning are indicated in the legend and highlighted with different colours with explanations in the text.}
%     \label{fig:combined-boxplots}
% \end{figure}

\section{Results}
\label{sec:climb-results}

Multiple evaluation metrics are employed in BabyLM. In this work, I focus on BLiMP \citep{warstadt2020blimp} and the supplementary BLiMP-style tests provided by the shared task organisers. I also report results on the natural language understanding benchmark, SuperGLUE \citep{wang2019superglue}, and the ambiguous subset of MSGS (the Mixed Signals Generalisation Set) \citep{warstadt2020msgs}. In brief, BLiMP evaluates specific linguistic abilities, MSGS evaluates linguistic preference over surface generalisation and SuperGLUE evaluates downstream task performance. All of the curriculum learning models are small BabyBERTa-style ones; model parameters are shown in \cref{tbl:baseline-size-comparison}. All experiments use the same set of hyper-parameters (\cref{tbl:baseline_hyperparams}) and are trained on the same, cleaned training dataset (\cref{subsec:data}).

% For all scores, I report the average score across all categories, rather than test instances, as provided by the BabyLM evaluation pipeline.\footnote{For instance, there are 12 categories in BLiMP but 50+ individual tests. I average over the scores given for each category, rather than the scores given for each test.}

To facilitate comparison across curriculum strategies, I report results as mean relative differences from the vanilla baseline, rather than absolute scores. For each evaluation benchmark (e.g., BLiMP, SuperGLUE), I first compute the difference between each curriculum model's score and the baseline score on every subtask within that benchmark. I then calculate the mean ($\mu$) and standard deviation ($\sigma$) of these differences across all subtasks.\footnote{For instance, there are 12 task categories in BLiMP.} Formally, for a curriculum model $c$ and baseline model $b$ evaluated on subtasks $\{t_1, \ldots, t_n\}$:
\begin{equation}
    \Delta_i = \text{score}_c(t_i) - \text{score}_b(t_i), \quad \mu = \frac{1}{n}\sum_{i=1}^{n} \Delta_i, \quad \sigma = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(\Delta_i - \mu)^2}
\end{equation}
Results are reported as $\mu \pm \sigma$, where \posval{green} values indicate improvements over the baseline and \negval{red} values indicate decreases. The standard deviation captures the variability of a curriculum's effect across different linguistic phenomena within each benchmark, providing insight into whether gains (or losses) are consistent or driven by specific subtasks. For reference, the absolute scores of the vanilla baseline model are included at the bottom of each table.

Below, I summarise key findings according to the three curriculum learning approaches: \textbf{vocabulary}, \textbf{data}, and \textbf{objective} curricula. The results for each type are reported in separate tables: \cref{tbl:result-vocab-cl} shows vocabulary curricula results, \cref{tbl:result-data-cl} shows data curricula results and \cref{tbl:result-obj-cl} shows objective curricula results. Each table also includes the performance of the small BabyBERTa-style vanilla model trained as a baseline (\cref{subsec:baseline}).

As a general note, the results of this comprehensive exploration are largely negative: \textbf{none of the curriculum learning strategies consistently outperform the vanilla baseline} when accounting for the high variance across subtasks. The mean differences are typically small (often $<1$ percentage point) relative to their standard deviations (often $>2$ percentage points), indicating that any observed effects are not statistically robust. While a few configurations show nominal improvements on specific benchmarks, these gains do not generalise across evaluation tasks and are likely within the noise of the evaluation. Despite these null results, this systematic investigation serves an important purpose: it establishes what does \emph{not} work under these conditions and provides a comprehensive reference for future research on curriculum learning in resource-constrained settings.

Furthermore, these largely negative results align with a growing body of evidence questioning the efficacy of curriculum learning in neural language modelling. \citet{wu2021when} conducted extensive experiments across thousands of orderings and found that any observed benefits were primarily attributable to dynamic training set size rather than the ordering itself. Similarly, \citet{surkov2022datacurricula} demonstrated that curricula based on various complexity measures rarely outperform random sampling for large language models like BERT and T5. My findings extend this cautionary evidence to the resource-constrained setting of the BabyLM challenge.%, suggesting that the difficulties of curriculum learning may be exacerbated, rather than ameliorated, when data is scarce.

%Additionally, \cref{fig:combined-boxplots} visualises a joint comparisons of all the curricula for the BLiMP and SuperGLUE tasks.

%I find notable gains for the vanilla baseline models over the BabyLM baselines, and, while I do not identify further large improvements in the curriculum learning models, I do notice some modest gains which suggest possibilities for future research and experimentation over variables. While the differences in performance between most of the experimental conditions are small, the large number of ablations provides a comprehensive set of recommendations for how and when different curriculum learning strategies may offer improved performance on linguistic tasks. %Below I summarise my observations over the full results tables.

% \paragraph{Reporting Relative Differences.} To facilitate comparison across curriculum strategies, I report results as mean relative differences from the vanilla baseline, rather than absolute scores. For each evaluation benchmark (e.g., BLiMP, SuperGLUE), I first compute the difference between each curriculum model's score and the baseline score on every subtask within that benchmark. I then calculate the mean ($\mu$) and standard deviation ($\sigma$) of these differences across all subtasks. Formally, for a curriculum model $c$ and baseline model $b$ evaluated on subtasks $\{t_1, \ldots, t_n\}$:
% \begin{equation}
%     \Delta_i = \text{score}_c(t_i) - \text{score}_b(t_i), \quad \mu = \frac{1}{n}\sum_{i=1}^{n} \Delta_i, \quad \sigma = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(\Delta_i - \mu)^2}
% \end{equation}
% % Results are reported as $\mu \pm \sigma$, where \posval{green} values indicate improvements over the baseline and \negval{red} values indicate decreases. The standard deviation captures the variability of a curriculum's effect across different linguistic phenomena within each benchmark, providing insight into whether gains (or losses) are consistent or driven by specific subtasks. For reference, the absolute scores of the vanilla baseline model are included at the bottom of each table.

\subsection{Vocabulary Curriculum}

\begin{table*}[h!]
    \centering
    \small
    \begin{tabular}{ll|rrrr}
    \toprule
    Pacing & Type & $\Delta$BLiMP & $\Delta$BLiMP.S & $\Delta$S.GLUE & $\Delta$MSGS \\
    \midrule
    Linear & \lightgreenhighlight{Freq} & $\negval{-0.4}$\tiny{$\negval{\pm 2.0}$} & $\mathbf{\posval{+1.1}}$\tiny{$\posval{\pm 1.7}$} & $\negval{-1.8}$\tiny{$\negval{\pm 3.4}$} & $\posval{+0.3}$\tiny{$\posval{\pm 3.0}$} \\
    Linear & \darkgreenhighlight{POS} & $\negval{-3.4}$\tiny{$\negval{\pm 3.2}$} & $\negval{-1.9}$\tiny{$\negval{\pm 3.7}$} & $\negval{-1.0}$\tiny{$\negval{\pm 3.2}$} & $\negval{-1.4}$\tiny{$\negval{\pm 4.6}$} \\
    Linear & \verydarkgreenhighlight{Hybrid} & $\negval{-2.1}$\tiny{$\negval{\pm 2.7}$} & $\negval{-3.3}$\tiny{$\negval{\pm 7.1}$} & $\negval{-1.3}$\tiny{$\negval{\pm 2.9}$} & $\negval{-1.7}$\tiny{$\negval{\pm 4.4}$} \\
    Log & \lightgreenhighlight{Freq} & $\negval{-0.5}$\tiny{$\negval{\pm 1.6}$} & $\posval{+0.8}$\tiny{$\posval{\pm 2.1}$} & $\negval{-0.5}$\tiny{$\negval{\pm 2.2}$} & $\negval{-1.5}$\tiny{$\negval{\pm 3.9}$} \\
    Log & \darkgreenhighlight{POS} & $\negval{-1.4}$\tiny{$\negval{\pm 2.5}$} & $\negval{-0.7}$\tiny{$\negval{\pm 3.0}$} & $\mathbf{\posval{+0.2}}$\tiny{$\posval{\pm 1.7}$} & $\mathbf{\posval{+2.2}}$\tiny{$\posval{\pm 5.1}$} \\
    Log & \verydarkgreenhighlight{Hybrid} & $\negval{-0.7}$\tiny{$\negval{\pm 2.4}$} & $\negval{-1.7}$\tiny{$\negval{\pm 5.2}$} & $\negval{-0.2}$\tiny{$\negval{\pm 0.8}$} & $\negval{-1.9}$\tiny{$\negval{\pm 3.4}$} \\
    \midrule
    \multicolumn{2}{l|}{Vanilla Baseline} & 75.48 & 65.34 & 70.47 & 68.30 \\
    \bottomrule
    \end{tabular}
    \caption{\label{tbl:result-vocab-cl} Results for vocabulary curriculum models (\cref{subsec:vocab-cl}), reported as mean relative difference ($\pm$ std) from the vanilla baseline across subtasks. \posval{Green} indicates improvement; \negval{red} indicates decrease. The bottom row shows absolute scores for the vanilla baseline. Abbreviations: BLiMP.S = BLiMP Supplement, S.GLUE = (Super)GLUE, MSGS = MSGS Ambiguous.}
\end{table*}


\paragraph{Vocabulary curricula do not reliably improve over baseline.}
The vocabulary curriculum experiments reveal no consistent benefit over the vanilla baseline (\cref{tbl:result-vocab-cl}). While the frequency-based curriculum with linear pacing shows a nominal improvement on BLiMP Supplement ($+1.1$), and the POS-based curriculum with log pacing shows nominal gains on GLUE ($+0.2$) and MSGS ($+2.2$), all of these differences are small relative to the standard deviations across subtasks. Most configurations result in decreased performance, particularly on BLiMP where the vanilla baseline achieves the highest score overall. These results suggest that restricting the vocabulary during early training may hinder rather than help the model's ability to learn robust representations in this low-resource setting.

\paragraph{Pacing function choice does not yield consistent patterns.}
I explored whether logarithmic pacing (which front-loads easier material) outperforms linear pacing, but the results are inconclusive. Log pacing shows marginal improvements on some metrics while underperforming on others, with no clear pattern emerging. This inconsistency, combined with the overall negative results, suggests that the choice of pacing function is secondary to the more fundamental question of whether vocabulary restriction is beneficial at all.

\subsection{Data Curriculum}

\begin{table*}[h!]
    \centering
    \small
    \begin{tabular}{ll|rrrr}
    \toprule
    Pacing & Type & $\Delta$BLiMP & $\Delta$BLiMP.S & $\Delta$S.GLUE & $\Delta$MSGS \\
    \midrule
    Linear & \lightpurplehighlight{Source} & $\negval{-2.2}$\tiny{$\negval{\pm 3.1}$} & $\negval{-3.3}$\tiny{$\negval{\pm 2.8}$} & $\negval{-0.8}$\tiny{$\negval{\pm 2.1}$} & $\negval{-2.1}$\tiny{$\negval{\pm 3.8}$} \\
    Linear & \darkpurplehighlight{Static PPX} & $\negval{-3.0}$\tiny{$\negval{\pm 3.4}$} & $\negval{-3.7}$\tiny{$\negval{\pm 5.8}$} & $\negval{-1.4}$\tiny{$\negval{\pm 2.0}$} & $\negval{-1.4}$\tiny{$\negval{\pm 2.9}$} \\
    Linear & \verydarkpurplehighlight{Dynamic PPX-U} & $\negval{-2.9}$\tiny{$\negval{\pm 3.7}$} & $\negval{-2.8}$\tiny{$\negval{\pm 5.7}$} & $\negval{-0.6}$\tiny{$\negval{\pm 1.2}$} & $\negval{-1.7}$\tiny{$\negval{\pm 4.2}$} \\
    Linear & \verydarkpurplehighlight{Dynamic PPX-R} & $\negval{-3.6}$\tiny{$\negval{\pm 3.8}$} & $\negval{-2.2}$\tiny{$\negval{\pm 3.2}$} & $\negval{-0.1}$\tiny{$\negval{\pm 1.1}$} & $\negval{-0.8}$\tiny{$\negval{\pm 3.3}$} \\
    Log & \lightpurplehighlight{Source} & $\mathbf{\posval{+0.4}}$\tiny{$\posval{\pm 2.0}$} & $\negval{-1.1}$\tiny{$\negval{\pm 3.7}$} & $\negval{-0.3}$\tiny{$\negval{\pm 1.0}$} & $\mathbf{\posval{+2.7}}$\tiny{$\posval{\pm 8.4}$} \\
    Log & \darkpurplehighlight{Static PPX} & $\negval{-0.5}$\tiny{$\negval{\pm 1.3}$} & $\negval{-1.6}$\tiny{$\negval{\pm 1.8}$} & $\negval{-0.6}$\tiny{$\negval{\pm 1.4}$} & $\negval{-1.6}$\tiny{$\negval{\pm 2.8}$} \\
    Log & \verydarkpurplehighlight{Dynamic PPX-U} & $\negval{-0.7}$\tiny{$\negval{\pm 2.2}$} & $\negval{-1.1}$\tiny{$\negval{\pm 2.7}$} & $\negval{-0.4}$\tiny{$\negval{\pm 1.9}$} & $\negval{-1.4}$\tiny{$\negval{\pm 3.0}$} \\
    Log & \verydarkpurplehighlight{Dynamic PPX-R} & $\posval{+0.3}$\tiny{$\posval{\pm 1.0}$} & $\negval{-2.3}$\tiny{$\negval{\pm 3.1}$} & $\negval{-1.5}$\tiny{$\negval{\pm 5.0}$} & $\negval{-1.7}$\tiny{$\negval{\pm 2.3}$} \\
    \midrule
    \multicolumn{2}{l|}{Vanilla Baseline} & 75.48 & 65.34 & 70.47 & 68.30 \\
    \bottomrule
    \end{tabular}
    \caption{\label{tbl:result-data-cl} Results for data curriculum models (\cref{subsec:data-cl}), reported as mean relative difference ($\pm$ std) from the vanilla baseline across subtasks. \posval{Green} indicates improvement; \negval{red} indicates decrease. The bottom row shows absolute scores for the vanilla baseline. Abbreviations: BLiMP.S = BLiMP Supplement, S.GLUE = (Super)GLUE, MSGS = MSGS Ambiguous.}
\end{table*}

\paragraph{Data curricula show mixed results with high variance.}

The data curriculum experiments (\cref{tbl:result-data-cl}) reveal that ordering training data by difficulty does not reliably improve model performance. Linear pacing consistently underperforms the baseline across all difficulty metrics, with mean decreases ranging from $-2$ to $-4$ points on BLiMP. Log pacing with source-based ordering shows the only nominal improvements ($+0.4$ on BLiMP, $+2.7$ on MSGS), but the high standard deviations ($\pm 2.0$ and $\pm 8.4$ respectively) indicate these effects are not robust across subtasks. The perplexity-based difficulty metrics, despite being more principled measures of linguistic complexity, perform no better than the simpler source-based heuristic. These findings suggest that the ``easy-to-hard'' curriculum hypothesis may not hold in practice for language model pre-training at this scale, or that the difficulty metrics explored here do not capture the relevant notion of complexity for learning.

% \paragraph{Perplexity-driven approaches for data curriculum learning underperform.}
% Although perplexity-based ordering intuitively reflects learning difficulty, it proves less effective in my experiments. Across 24 different perplexity-based configurations, only one outperformed the vanilla model (log pacing, random initialisation + model perplexity) \cref{fig:baseline_obj_cl_superglue}. This suggests that the perplexity estimates may not capture relevant complexity for the model at small scales, and further work is needed to refine how perplexity is computed or used.

\subsection{Objective Curriculum}
\begin{table*}[h!]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l ccc | rrrr}
    \toprule
    & \multicolumn{3}{c|}{Task duration (\% of training)} & & & & \\
    Type & 3 POS & 10 POS & MLM & $\Delta$BLiMP & $\Delta$BLiMP.S & $\Delta$S.GLUE & $\Delta$MSGS \\
    \midrule
    \lightorangehighlight{Seq} & -- & 0--12.5 & 12.5--100 & $\negval{-1.6}$\tiny{$\negval{\pm 2.2}$} & $\negval{-2.4}$\tiny{$\negval{\pm 4.2}$} & $\negval{-0.6}$\tiny{$\negval{\pm 2.6}$} & $\negval{-1.6}$\tiny{$\negval{\pm 4.4}$} \\
    \darkorangehighlight{MT} & -- & 0--100 & 12.5--100 & $\negval{-0.9}$\tiny{$\negval{\pm 1.7}$} & $\negval{-3.2}$\tiny{$\negval{\pm 5.4}$} & $\negval{-1.4}$\tiny{$\negval{\pm 2.1}$} & $\negval{-1.7}$\tiny{$\negval{\pm 4.3}$} \\
    \darkorangehighlight{MT} & -- & 0--100 & 0--100 & $\mathbf{\posval{+0.3}}$\tiny{$\posval{\pm 1.3}$} & $\posval{+0.4}$\tiny{$\posval{\pm 3.2}$} & $\mathbf{\posval{+0.3}}$\tiny{$\posval{\pm 1.4}$} & $\negval{-1.7}$\tiny{$\negval{\pm 4.0}$} \\
    \lightorangehighlight{Seq} & 0--6.25 & 6.25--12.5 & 12.5--100 & $\negval{-1.5}$\tiny{$\negval{\pm 3.2}$} & $\negval{-2.3}$\tiny{$\negval{\pm 3.5}$} & $\posval{+0.2}$\tiny{$\posval{\pm 1.6}$} & $\negval{-1.4}$\tiny{$\negval{\pm 4.6}$} \\
    \darkorangehighlight{MT} & 0--6.25 & 6.25--100 & 12.5--100 & $\negval{-1.8}$\tiny{$\negval{\pm 1.5}$} & $\negval{-1.5}$\tiny{$\negval{\pm 4.2}$} & $\negval{-0.4}$\tiny{$\negval{\pm 1.4}$} & $\negval{-1.3}$\tiny{$\negval{\pm 4.4}$} \\
    \darkorangehighlight{MT} & 0--6.25 & 6.25--100 & 0--100 & $\negval{-0.7}$\tiny{$\negval{\pm 1.0}$} & $\mathbf{\posval{+2.2}}$\tiny{$\posval{\pm 2.7}$} & $\negval{-0.6}$\tiny{$\negval{\pm 1.7}$} & $\negval{-0.6}$\tiny{$\negval{\pm 2.2}$} \\
    \darkorangehighlight{MT} & 0--100 & -- & 0--100 & $\negval{-1.0}$\tiny{$\negval{\pm 1.2}$} & $\negval{-1.4}$\tiny{$\negval{\pm 2.1}$} & $\negval{-0.7}$\tiny{$\negval{\pm 2.4}$} & $\negval{-0.6}$\tiny{$\negval{\pm 2.2}$} \\
    \midrule
    \multicolumn{4}{l|}{Vanilla Baseline} & 75.48 & 65.34 & 70.47 & 68.30 \\
    \bottomrule
    \end{tabular}
    \caption{\label{tbl:result-obj-cl} Results for objective curriculum models (\cref{subsec:objective-cl}), reported as mean relative difference ($\pm$ std) from the vanilla baseline across subtasks. \posval{Green} indicates improvement; \negval{red} indicates decrease. The bottom row shows absolute scores for the vanilla baseline. Task duration defines when an objective function was active during training, as a percentage of the total number of training steps. Abbreviations: BLiMP.S = BLiMP Supplement, S.GLUE = (Super)GLUE, MSGS = MSGS Ambiguous.}
\end{table*}


\paragraph{Objective curricula yield the most promising, though still inconclusive, results.}
Among the three curriculum dimensions, the objective curriculum experiments show the most promising patterns, though they remain statistically inconclusive. The multitask model combining 10-class POS prediction with MLM from the start shows small positive effects on BLiMP ($+0.3 \pm 1.3$) and S.GLUE ($+0.3 \pm 1.4$), and a multitask configuration achieves a notable gain on BLiMP Supplement ($+2.2 \pm 2.7$). However, even these best-case results have standard deviations that overlap substantially with zero, and no single configuration improves consistently across all benchmarks. Sequential curricula, which more directly mirror the developmental progression from coarse to fine-grained learning, generally underperform their multitask counterparts. This suggests that the specific scheduling of objectives matters less than simply providing multiple learning signals throughout training.

\vspace{1em}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{chapters/climb/figures/baseline_vs_obj_cl_blimp_supp_new.png}
        \caption{BLiMP supplementary tasks}
        \label{fig:baseline_obj_cl_blimp_supp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{chapters/climb/figures/baseline_vs_obj_cl_superglue.png}
        \caption{(Super)GLUE tasks}
        \label{fig:baseline_obj_cl_superglue}
    \end{subfigure}
    \caption{Comparison between the vanilla model and the best objective curriculum learning setting on different benchmark tasks.}
    \label{fig:baseline_obj_cl_comparison}
\end{figure}

\paragraph{Per-task analysis reveals inconsistent effects.} \cref{fig:baseline_obj_cl_blimp_supp} and \cref{fig:baseline_obj_cl_superglue} break down the comparison between the vanilla model and the best objective curriculum configuration on individual tasks within BLiMP Supplement and SuperGLUE. While the curriculum model shows improvements on some tasks, the pattern is inconsistent: gains on certain syntactic phenomena are offset by losses on others. This task-level variability results in the high standard deviations observed in the aggregate results.

\paragraph{Auxiliary POS objectives may aid specific syntactic tasks.}
One tentative observation is that the best performing objective curriculum model on BLiMP Supplement shows strong relative performance on certain difficult syntactic tasks (e.g. QA Congruency). This pattern (although not observed across all subtasks) aligns with the intuition that explicit syntactic supervision could guide the model toward better grammatical representations. However, this benefit does not transfer to other benchmarks, suggesting that any gains are narrow rather than reflecting improved general linguistic competence.

\subsection{Combined Curricula and Additional Observations}

% After presenting results for each curriculum learning strategy in isolation, in this section we report some findings from combining multiple types of curricula together.

\begin{table*}[h!]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l l l | rrrr}
    \toprule
    Vocab & Data & Objective & $\Delta$BLiMP & $\Delta$BLiMP.S & $\Delta$S.GLUE & $\Delta$MSGS  \\
    \midrule
    -- & \lightpurplehighlight{Source} & \darkorangehighlight{MT} & $\negval{-1.4}$\tiny{$\negval{\pm 2.2}$} & $\negval{-1.3}$\tiny{$\negval{\pm 3.1}$} & $\negval{-0.5}$\tiny{$\negval{\pm 2.5}$} & $\negval{-1.4}$\tiny{$\negval{\pm 3.8}$} \\
    -- & \verydarkpurplehighlight{Dyn. PPX-R} & \darkorangehighlight{MT} & $\posval{+0.4}$\tiny{$\posval{\pm 1.8}$} & $\negval{-0.7}$\tiny{$\negval{\pm 1.1}$} & $\negval{-0.8}$\tiny{$\negval{\pm 2.5}$} & $\negval{-0.5}$\tiny{$\negval{\pm 1.9}$} \\
    \lightgreenhighlight{Freq} & \lightpurplehighlight{Source} & -- & $\posval{+0.4}$\tiny{$\posval{\pm 2.0}$} & $\negval{-0.7}$\tiny{$\negval{\pm 3.0}$} & $\negval{-0.2}$\tiny{$\negval{\pm 2.5}$} & $\negval{-0.4}$\tiny{$\negval{\pm 2.9}$} \\
    \lightgreenhighlight{Freq} & \verydarkpurplehighlight{Dyn. PPX-R} & -- & $\posval{+0.4}$\tiny{$\posval{\pm 2.0}$} & $\mathbf{\posval{+0.5}}$\tiny{$\posval{\pm 2.8}$} & $0.0$\tiny{$\pm 2.5$} & $\negval{-1.7}$\tiny{$\negval{\pm 4.2}$} \\
    \lightgreenhighlight{Freq} & \lightpurplehighlight{Source} & \darkorangehighlight{MT} & $\negval{-0.6}$\tiny{$\negval{\pm 2.0}$} & $\negval{-2.5}$\tiny{$\negval{\pm 4.9}$} & $\negval{-0.4}$\tiny{$\negval{\pm 2.0}$} & $\negval{-1.6}$\tiny{$\negval{\pm 4.0}$} \\
    \lightgreenhighlight{Freq} & \verydarkpurplehighlight{Dyn. PPX-R} & \darkorangehighlight{MT} & $\mathbf{\posval{+0.4}}$\tiny{$\posval{\pm 1.2}$} & $\negval{-1.7}$\tiny{$\negval{\pm 4.2}$} & $\negval{-0.5}$\tiny{$\negval{\pm 2.0}$} & $\mathbf{\posval{+3.0}}$\tiny{$\posval{\pm 7.3}$} \\
    \midrule
    \multicolumn{3}{l|}{Vanilla Baseline} & 75.48 & 65.34 & 70.47 & 68.30 \\
    \bottomrule
    \end{tabular}
    \caption{\label{tbl:result-combination-cl} Results for the combination curriculum models, reported as mean relative difference ($\pm$ std) from the vanilla baseline across subtasks. \posval{Green} indicates improvement; \negval{red} indicates decrease. The bottom row shows absolute scores for the vanilla baseline. The multitask (MT) objective curriculum refers to the 2-task 10-POS and MLM model shown in \cref{tbl:result-obj-cl}. Abbreviations: BLiMP.S = BLiMP Supplement, S.GLUE = (Super)GLUE, MSGS = MSGS Ambiguous.}
\end{table*}

\paragraph{Combining curricula does not yield additive benefits.}
Given that individual curricula showed limited success, I investigated whether combining multiple curriculum dimensions might produce synergistic effects. The results (\cref{tbl:result-combination-cl}) do not support this hypothesis. While a few combinations show nominal positive effects (e.g., $+0.4$ on BLiMP for several configurations), the standard deviations remain high and the effects do not generalise across benchmarks. The combination of all three curricula with dynamic perplexity-based data ordering shows an unusually high MSGS score ($+3.0$), but this is accompanied by an equally high standard deviation ($\pm 7.3$), suggesting this result is driven by outlier subtasks rather than a genuine improvement. These findings indicate that curriculum learning effects do not compound in straightforward ways.

\paragraph{Summary of findings across curriculum types.}
Across all three curriculum dimensions and their combinations, the pattern is consistent: \textbf{no configuration reliably outperforms the vanilla baseline}. The few positive mean differences observed are small relative to the variance across subtasks, and no curriculum strategy shows consistent benefits across all four evaluation benchmarks. Log pacing tends to perform slightly better than linear pacing when differences exist, but this pattern is likely not strong enough to constitute a statistically significant recommendation.



%\paragraph{In small data settings, filtering noisy data is in fact counter-productive.} Perhaps surprisingly, I find that the vanilla models trained on the raw data outperform those trained on the preprocessed data on BLiMP and MSGS. I surmise that models can learn even from linguistically non-standard datapoints.

% \section{Discussion}
% \label{sec:climb-discussion}

% This work investigates curriculum learning approaches for language model training, motivated by insights from human language acquisition and the need for cost-effective small model training. I developed an improved baseline model based on BabyBERTa \citep{huebner2021babyberta}, demonstrating that an 8-layer vanilla model can outperform the provided BabyLM baselines on BLiMP grammaticality tests and approach the best RoBERTa shared-task baseline on SuperGLUE. These results support the BabyBERTa findings that smaller datasets benefit from smaller models and reduced vocabulary sizes.

% % I set out to investigate a number of curriculum learning approaches to language model training, motivated by findings from the human language acquisition process and by the wish to successfully train smaller models for smaller budgets. I first implemented a stronger baseline model of my own, based on BabyBERTa \citep{huebner2021babyberta} and found that a small 8-layer vanilla model could outperform the provided BabyLM baselines on the BLiMP grammaticality tests and get close to the best RoBERTa shared-task baseline on SuperGLUE. This underlines the findings reported in the BabyBERTa paper: that with smaller datasets, it makes sense to use smaller models and a smaller vocabulary size.

% The results of the curriculum learning experiments suggest that model performance can be improved in certain linguistic tasks by careful application of a pacing function, with certain types of curricula working more effectively than others. Specifically, I find that a logarithmic pacing function works better for the data curriculum than a linear one, but the findings for the vocabulary curriculum are less clear. Other pacing functions might be tried in the future, particularly those that more closely reflect learning trajectories in language acquisition. %theory around non-monotonic or `U-shaped' development trajectories.

% The results also suggest that ordering subcorpora within training sets can be beneficial, and that perplexity-based data selection approaches show promise, though optimal perplexity calculation methods remain unclear. While multitask learning proves effective in language modeling, MLM and AR objective functions remain the dominant single tasks. Multitask models perform particularly well in objective curriculum settings, though they also show strong performance in sequential settings. Future work could explore varying task switch timing and incorporating additional tasks.

% On a more general note, the BabyLM challenge evaluates a language model only on its final downstream performance on a set of tasks --- i.e. at a finite point in time. The challenge does not directly measure whether a given model is learning in a `human-like' fashion. My contribution to the BabyLM challenge is to provide a set of curriculum learning strategies which are motivated by the language learning dynamics of infants and children. Future research could study how to quantitatively evaluate whether the learning trajectory of a model parallels that of a human language learner and how similarities to human language learning results in downstream NLU performance. 



\section{Conclusion}
\label{sec:climb-conclusion}
This chapter presented a comprehensive investigation of curriculum learning strategies for language model pre-training, motivated by principles from child language acquisition. I implemented and evaluated three curriculum dimensions (\textbf{vocabulary}, \textbf{data}, and \textbf{objective} curricula) across a variety of configurations, with the goal of testing whether developmentally inspired training protocols could improve learning efficiency in resource-constrained settings.

The central finding of this chapter is negative: \textbf{none of the curriculum learning strategies reliably outperform a well-tuned vanilla baseline}. Across all configurations and benchmarks, the observed differences are small relative to their variance, and no approach shows consistent benefits across evaluation tasks. This null result stands in contrast to the intuitive appeal of curriculum learning and the promising findings reported in some prior work on other domains.

Several factors may explain these negative results, and my findings agree with broader patterns observed in the literature. First, \citet{wu2021when} show that curriculum learning provides benefits primarily under specific conditions such as limited training time budgets or the presence of noisy data. The BabyLM setting, while data-constrained, may not satisfy these conditions since training is run to convergence and the data has been carefully curated and cleaned. Second, as \citet{surkov2022datacurricula} note, the choice of hyper-parameters is often more important than the choice of curriculum. Third, the analogy between human and machine learning may be too simplistic. As \citet{campos2021curriculum} argues, modern language models may be sufficiently robust to learn from randomly ordered data, reducing any potential benefit from structured curricula.

Despite the null findings, this systematic exploration serves an important scientific purpose. Not every hypothesis will be confirmed, and documenting what does \emph{not} work is as valuable as documenting what does. It prevents redundant effort and helps calibrate expectations for future research. The \climb framework itself remains a useful contribution: the modular implementation of vocabulary, data, and objective curricula provides infrastructure for testing alternative difficulty metrics, pacing functions, or curriculum combinations that future work may identify. Moreover, the strong performance of the vanilla baseline underscores the finding that in low-resource settings, careful hyper-parameter tuning and model selection may matter more than sophisticated training protocols.

This chapter provides a nuanced, but largely negative answer to \textbf{RQ1}. While the motivation for curriculum learning from human development remains compelling in principle, translating these insights into practical benefits for neural language models is non-trivial. The gap between human and machine learning may be too large for simple analogies (such as presenting easy examples before hard ones) to transfer directly. This suggests that addressing RQ1 may require either more sophisticated implementations of developmental principles or fundamentally different approaches to incorporating cognitive insights into model training.

In this chapter, I examined the \emph{macro-level structure} of training, by investigating whether developmentally inspired pacing and sequencing can improve efficiency. The next chapter builds on this foundation by shifting focus to the \emph{micro-level dynamics} of representation. There, I introduce \smoothing, a method motivated by how children use grammatical context to infer word meaning. While \climb investigated how to structure the learning experience, \smoothing explores how to shape the internal representations that support generalisation from limited exposure. Together, these two chapters provide complementary perspectives on \textbf{RQ1} and begin to set the stage for \textbf{RQ2}, where I turn to a more systematic analysis of learning dynamics in small language models.