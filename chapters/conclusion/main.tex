\chapter{Concluding Remarks}
In this thesis, I have argued that building better small language models is not merely a matter of scaling down large ones, but a distinct scientific challenge. Meeting this challenge requires both principled training paradigms and a deeper understanding of the learning process itself. While recent advances in NLP have been fueled by massive models with ever-growing parameter counts, this work has shown that small models offer unique advantages: they are more transparent, more adaptable, and better suited for systematic scientific inquiry.

In addressing Research Question 1, the first half of this thesis explored how insights from human language acquisition can inform more efficient and effective training strategies. By designing curriculum learning approaches grounded in developmental linguistics and introducing syntactic smoothing, it demonstrated that cognitive priors can improve generalisation and mitigate structural inefficiencies, particularly in data-scarce regimes. These findings show that cognitively inspired approaches are not just philosophically appealing, but feasible to implement and, in the right contexts, empirically beneficial for training small language models.

In addressing Research Question 2, the second half of this thesis shifted focus to understanding the learning dynamics of small models: how they acquire structure, how they diverge from larger counterparts, and why they often struggle to converge stably. Using tools such as representational similarity analysis and proportional effective rank, this work established empirical foundations for diagnosing these challenges.

A central outcome of this work is the development of \pico, a modular, open-source framework designed explicitly for the scientific study of language models. Unlike conventional pipelines, where model training and post-hoc analysis are disconnected stages, \pico integrates them into a single, hypothesis-driven workflow. This integration allows researchers to observe, quantify, and intervene in the learning process as it unfolds, turning training runs into controlled experiments. By design, \pico is lightweight and hardware-friendly, lowering the barrier for rigorous and reproducible language model research.

Together, these contributions reflect a shift in emphasis away from brute-force scaling, and toward thoughtful, interpretable, and community-driven model design. Rather than viewing small models as constrained versions of large ones, we should treat them as scientific instruments: transparent enough to study, flexible enough to modify, and accessible enough for a wide research community to improve.