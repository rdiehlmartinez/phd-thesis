\chapter{Concluding Remarks}

This thesis has argued that building better small language models is not just a question of scaling down large ones, but a distinct scientific challenge—one that demands both principled training paradigms and deeper insights into the learning process itself. While recent advances in NLP have largely been driven by massive models with ever-growing parameter counts, this work has shown that small models offer a different, equally valuable opportunity: to study how language learning unfolds, to experiment with interpretable interventions, and to develop more accessible, sustainable, and privacy-preserving technologies.

In addressing Research Question 1, this thesis explored how insights from human language acquisition can inform more efficient and effective model training. By designing curriculum learning strategies grounded in developmental linguistics and by introducing techniques like syntactic smoothing, it demonstrated that cognitive priors can improve generalization and mitigate structural inefficiencies—particularly in data-scarce regimes. These findings support the view that cognitively inspired approaches are not just philosophically appealing, but practically beneficial for small-scale model development.

In response to Research Question 2, the second half of the thesis focused on understanding the learning dynamics of small models—how they acquire structure, how they diverge from larger counterparts, and why they often struggle to converge stably. Using tools such as effective rank, representational similarity, and capacity tracking, this work developed an empirical foundation for diagnosing the training challenges of small models. It also introduced Pico, a lightweight framework designed to make learning dynamics visible and analyzable throughout training, enabling a new class of intervention-driven experiments.

Together, these contributions reflect a shift in emphasis: away from brute-force scale, and toward thoughtful, interpretable, and theory-guided model design. They suggest that rather than viewing small models as constrained versions of larger ones, we should treat them as unique scientific objects—capable of revealing insights about learning that would be difficult to observe in more opaque, overparameterized systems.

Looking forward, the methods and tools introduced here lay the groundwork for several promising directions. These include extending curriculum strategies to multilingual or multimodal settings, exploring generalization under distribution shift, and developing richer metrics for tracking emergent structure during training. Perhaps most importantly, this work underscores the value of building models not just to perform, but to understand—models that are transparent enough to study and simple enough to improve.

In a field increasingly dominated by scale, this thesis offers a different perspective: that progress in language modeling may come not only from going bigger, but from going deeper—into the principles of learning itself.