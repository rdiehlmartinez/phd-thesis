\chapter{Concluding Remarks}
In this thesis, I have argued that building better small language models is not merely a matter of scaling down large ones, but a distinct scientific challenge. Meeting this challenge requires both principled training paradigms and a deeper understanding of the learning process itself. While recent advances in NLP have been fueled by massive models with ever-growing parameter counts, this work has shown that small models offer unique advantages: they are more transparent, more adaptable, and better suited for systematic scientific inquiry.

In addressing Research Question 1, the first half of this thesis explored how insights from human language acquisition can inform more efficient and effective training strategies. By designing curriculum learning approaches grounded in developmental linguistics and introducing syntactic smoothing, it demonstrated that cognitive priors can improve generalisation and mitigate structural inefficiencies, particularly in data-scarce regimes. These findings show that cognitively inspired approaches are not just philosophically appealing, but feasible to implement and, in the right contexts, empirically beneficial for training small language models.

It is worth reiterating that the goal of this research question has not been to model human language acquisition itself. Humans and artificial neural networks operate under profoundly different principles and many aspects of human cognition remain poorly understood. Instead, my aim has been pragmatic: to demonstrate how human learning can serve as a source of guiding hypotheses to facilitate the development of small language models. The fact that humans can achieve data-efficient, generalisable language acquisition suggests that certain high-level strategies, such as curriculum learning, or leveraging syntactic structure, are worth testing as design principles for artificial models. Even if the underlying mechanisms differ, aligning our experiments with observed human behaviours provides a principled starting point for improving efficiency and robustness in small language models.

In addressing Research Question 2, the second half of this thesis shifted focus to understanding the learning dynamics of small models: how they acquire structure, how they diverge from larger counterparts, and why they often struggle to converge stably. Using tools such as representational similarity analysis and proportional effective rank, this work established empirical foundations for diagnosing these challenges.

A central outcome of this work is the development of \pico, a modular, open-source framework designed explicitly for the scientific study of language models. Unlike conventional pipelines, where model training and post-hoc analysis are disconnected stages, \pico integrates them into a single, hypothesis-driven workflow. This integration allows researchers to observe, quantify, and intervene in the learning process as it unfolds, turning training runs into controlled experiments. By design, \pico is lightweight and hardware-friendly, lowering the barrier for rigorous and reproducible language model research.

Together, these contributions reflect a shift in emphasis away from brute-force scaling, and toward thoughtful, interpretable, and community-driven model design. Rather than viewing small models as constrained versions of large ones, we should treat them as scientific instruments: transparent enough to study, flexible enough to modify, and accessible enough for a wide research community to improve.

\section*{Future Directions}

While this thesis has taken steps towards efficient and principled training paradigms for small language models, it represents only an initial foundation for a much broader research agenda.

From the perspective of cognitive inspiration, one promising direction is to broaden the architectural landscape used for language modelling. Biological systems process language through mechanisms that remain only partially understood, but they offer inspiration for alternative approaches that move beyond today's Transformer paradigm. Already, we are beginning to see the emergence of new architectures, such as state space models \citep{gu2021efficiently,gu2023mamba} and the resurgence of recurrent-style models like RWKV \citep{peng2023rwkv}. These architectures suggest that efficiency and generalisation may be achievable through structures quite different from Transformers. Exploring architectures guided by insights from how natural systems acquire and process language may yield further directions for the next generation of small language models.

Similarly, future work should explore training paradigms that move closer to how biological systems adapt and learn. Humans acquire language not only by passively predicting the next word, but also by actively interacting with their environment, receiving feedback, and continuously adjusting their strategies. In this light, paradigms such as reinforcement learning and meta-learning appear especially promising. When incorporated into the pre-training process itself these methods may encourage models to develop more efficiently and learn more generalisable behaviours.

The tools introduced in this thesis, particularly \pico, are designed to enable exactly these kinds of future experiments. By lowering the barrier for controlled, hypothesis-driven training and analysis, \pico provides a foundation for future students and researchers to explore new architectures and training strategies. Already, early adopters have begun to extend this line of work: projects I mentored during my PhD have been accepted to upcoming conferences, and the open-source release of \pico has drawn substantial community interest, with around 500 stars on GitHub and 125,000 views on YouTube at the time of writing. These numbers underscore the demand for lightweight, transparent frameworks that make the scientific study of language models more accessible.

Looking ahead, one key challenge is to expand the set of metrics for studying learning dynamics. The methods introduced in this thesis shed light on representational capacity usage (PER) and model convergence dynamics, but much remains to be explored about the internal circuitry of models, the formation of compositional representations, and the mechanisms that underlie generalisation. Developing richer diagnostic tools will not only deepen our scientific understanding, but also make it possible to design more robust and interpretable small models.

Ultimately, progress in this area will depend on community effort. The development of small language models should be seen not as a scaled-down version of frontier LLM research, but as a novel and collaborative scientific effort. I believe that the methods and tools developed in this thesis will make this effort more accessible and help lay the groundwork for principled, and efficient language modelling.