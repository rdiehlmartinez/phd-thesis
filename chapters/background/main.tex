\chapter{Language Modeling Background and Small Language Models}

In this chapter, we will explore the background of language modeling, including the early approaches, word embeddings, and language models, leading up to current state of the art large language models. We will then pivot and outline some of the methods that researchers have used to make progress on efficiently training small language models. 

\section{Language Modeling Task}

In this section, we will explore some of the key concepts and techniques used in language modeling. 

Language modeling refers to the task of assigning probabilities to sequences of words or tokens. The goal is to model the likelihood of a sequence in a language. In the case of a sequence of words, $w = w_1, w_2, \ldots, w_n$, the probability of the sequence is given by the product of the probabilities of the words occurring in the sequence:

\begin{equation}    
    P(w) = P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^n P(w_i | w_1, \ldots, w_{i-1})
\end{equation}

Training a language model is then equivalent to estimating the parameters of this probability distribution. In practice, this is done by estimating the parameters of a neural network that maps a sequence of words to a probability distribution over the next word. 

\subsection{Early Approaches: N-Gram Models}
The earliest language models were based on n-gram statistics, where the probability of a word depends only on the preceding $n-1$ words. These models, described in foundational texts such as \cite{jurafsky2025speech}, are simple and effective but suffer from data sparsity and limited context. To address these issues, various smoothing techniques were developed, including Katz backoff \citep{katz2003estimation} and Kneser-Ney smoothing \citep{kneser1995improved}, with interpolated Kneser-Ney \citep{chen1999empirical} becoming the de facto standard for n-gram language modeling.

\subsection{Neural Language Models}

A major breakthrough came with the introduction of neural probabilistic language models by \cite{bengio2003neural}, which proposed using feed-forward neural networks to estimate the probability of word sequences. This approach introduced distributed word representations (embeddings), alleviating the data sparsity problem inherent in n-gram models. Subsequent work explored more powerful architectures, such as recurrent neural networks (RNNs) \citep{mikolov2010recurrent}, which can, in principle, capture dependencies across arbitrarily long contexts. However, training RNNs is challenging due to the vanishing and exploding gradient problems \citep{bengio1994learning}, leading to the development of gated architectures like Long Short-Term Memory (LSTM) networks \citep{hochreiter1997lstm} and Gated Recurrent Units (GRUs) \citep{cho2014gru}.
Large-scale empirical studies, such as \cite{jozefowicz2016exploring}, benchmarked various neural architectures and demonstrated that both model architecture and training data size significantly impact language modeling performance, as measured by perplexity.
\subsection{Word Embeddings}
In parallel, the adoption of distributed word representations, or word embeddings, marked a paradigm shift in NLP. Methods such as Skip-gram and Continuous Bag-of-Words (CBOW) \citep{mikolov2013efficient, mikolov2013distributed} and GloVe \citep{pennington2014glove} learn dense vector representations that capture syntactic and semantic relationships between words. These embeddings enabled transfer learning in NLP, as pre-trained vectors could be used for a variety of downstream tasks, including named entity recognition~\citep{lample2016neural}, sentence classification~\citep{kim2014convolutional}, and machine translation~\citep{qi2018translation}. However, traditional word embeddings are static, assigning the same vector to a word regardless of context.

Some novel methods have attempted to preproces or represent the input text data in unique ways, such as \cite{kim2016character} and phoneme-level representations \cite{goriely2024babble}.

\subsection{Contextualization and Attention}
To address the limitations of static embeddings, attention mechanisms were introduced, initially to solve the sequence alignment problem in machine translation \citep{bahdanau2015neural,luong2015effective}. Attention allows models to dynamically focus on relevant parts of the input sequence, improving translation quality and enabling the development of encoder-decoder architectures \citep{sutskever2014sequence}. The concept of contextual word embeddings was further advanced by models such as ELMo \citep{peters2018deep}, which extract context-sensitive representations from deep, bidirectional LSTMs.

\subsection{The Transformer and Modern Language Models}
The introduction of the Transformer architecture by Vaswani et al.~\citep{vaswani2017attention} marked a turning point in language modeling by replacing recurrence with self-attention mechanisms. This enabled efficient parallelization and the modeling of long-range dependencies, setting new performance benchmarks in machine translation and other NLP tasks. The Transformer built on earlier advances such as the soft attention mechanism of Bahdanau et al.~\citep{bahdanau2015neural}, which allowed neural machine translation models to dynamically align source and target sequences, improving translation quality and eliminating the need for a fixed-size encoding.

BERT~\citep{devlin2019bert} introduced the masked language modeling (MLM) objective, where some percentage of input tokens are replaced with a special [MASK] token, and the model is trained to predict the original identity of these masked tokens given their context. Formally, for a sequence of tokens $w = (w_1, \ldots, w_n)$ and a set of masked positions $M \subset \{1, \ldots, n\}$, the MLM objective maximizes
\begin{equation}
    \mathcal{L}_{\text{MLM}} = \sum_{i \in M} \log P(w_i \mid w_{\setminus M}),
\end{equation}
where $w_{\setminus M}$ denotes the sequence with masked tokens replaced by [MASK]. This enables BERT to learn deep bidirectional representations, as the model can attend to both left and right context. 

Following BERT, a number of variants improved on its design. RoBERTa~\citep{liu2019roberta} removed the next sentence prediction task and scaled up training data and compute. ALBERT~\citep{lan2019albert} introduced parameter sharing and sentence order prediction to reduce model size and improve efficiency. SpanBERT~\citep{joshi2020spanbert} focused on span-level objectives for better performance on related tasks. Other architectural innovations include DistilBERT~\citep{sanh2019distilbert}, which compresses BERT via knowledge distillation, and DeBERTa~\citep{he2021deberta}, which enhances attention mechanisms through disentangled representations.

In contrast to masked language modeling, autoregressive pretraining—as used in the GPT series~\citep{radford2018gpt1, radford2019gpt2, brown2020gpt3}—trains models to predict each token based only on its preceding context. This objective maximizes the likelihood:

\begin{equation}
\mathcal{L}_{\text{AR}} = \sum_{i=1}^n \log P(w_i \mid w_1, \ldots, w_{i-1}),
\end{equation}

enforcing a unidirectional, left-to-right dependency. Despite this constraint, autoregressive models have proven particularly effective for large-scale pretraining, and form the foundation of many state-of-the-art language models used today. Their simplicity and scalability have contributed to their widespread adoption for open-ended generation tasks.

Building on these two main paradigms, several models have explored alternative objectives and architectures. One especially influential direction is the encoder-decoder, or text-to-text, framework. T5~\citep{raffel2020t5} exemplifies this approach by casting all NLP tasks—classification, translation, question answering—as text generation problems within a unified pretraining scheme. Similarly, BART~\citep{lewis2020bart} combines a denoising autoencoder objective with a sequence-to-sequence architecture, enabling both robust understanding and fluent generation.

Other notable innovations include Transformer-XL~\citep{dai2019transformer}, which improves long-context modeling through segment-level recurrence and relative positional encodings; CTRL~\citep{keskar2019ctrl}, which allows controllable generation via conditioning on control codes; ELECTRA~\citep{clark2020electra}, which reframes pretraining as a discriminative task of replaced-token detection for improved sample efficiency; and XLNet~\citep{yang2019xlnet}, which generalizes autoregressive modeling using permutation-based objectives to capture bidirectional dependencies without masking.

Together, these advances reflect a broad exploration of architectures and training strategies, enabling the current generation of language models to achieve impressive results across a wide range of NLP tasks

\subsection{Training and Optimization}

\paragraph{Optimization Algorithms.} The foundational method for training neural networks is stochastic gradient descent (SGD)\citep{robbins1951stochastic}, which iteratively updates model parameters using noisy gradient estimates from mini-batches. To address limitations in learning rate tuning and convergence speed, a range of adaptive optimizers have been developed. These include Adagrad\citep{duchi2011adaptive}, which adapts learning rates based on historical gradients; RMSProp~\citep{tieleman2012lecture}, which normalizes updates using a moving average of squared gradients; Adam~\citep{kingma2015adam}, which combines momentum with adaptive learning rates; and AdamW~\citep{loshchilov2019decoupled}, which decouples weight decay from the gradient update, improving regularization behavior.

\paragraph{Regularization and Stabilization.} To prevent overfitting and promote better generalization, regularization techniques such as dropout~\citep{srivastava2014dropout} randomly deactivate units during training, introducing noise that encourages robustness. Layer normalization~\citep{ba2016layernorm} standardizes activations across feature dimensions, stabilizing training in deep architectures. Residual connections~\citep{he2016deep}, originally introduced in ResNets, are critical in Transformer-based models, enabling deeper networks by mitigating vanishing gradients and facilitating gradient flow.

\paragraph{Architectural Enhancements.} Beyond optimization and regularization, several architectural components have played a crucial role in model performance. Positional encoding methods—such as the original sinusoidal encodings in the Transformer~\citep{vaswani2017attention}, relative positional encodings~\citep{shaw2018self}, Rotary Positional Embeddings (RoPE)\citep{su2024rope}, and learned position embeddings\citep{press2021train}—allow models to incorporate token order into otherwise permutation-invariant self-attention mechanisms. Additionally, advances in activation functions have contributed to better expressiveness and gradient flow. These include GELU~\citep{hendrycks2016gaussian}, Swish and other learned activations~\citep{ramachandran2017searching}, and Gated Linear Units (GLUs)~\citep{shazeer2020glu}, which enhance nonlinear modeling capacity while preserving training stability.

\subsection{Scaling and Architecture Search}
While recent research has shown that scaling model and dataset size leads to predictable improvements in performance~\citep{kaplan2020scaling, henighan2020scaling}, designing neural network architectures in a robust and rigorous way remains a significant challenge. Much of architecture design is still guided by empirical intuition and trial-and-error, rather than first principles. For example, \citet{levine2020depth} offer a structured perspective on the trade-offs between scaling depth and width in Transformers, proposing guidelines for balancing these factors under a fixed compute budget. The scaling laws established by \citet{kaplan2020scaling} demonstrate that both wider and deeper models can improve performance if scaled appropriately, but do not prescribe how to select specific architectural configurations for a given task or resource constraint.

To address the difficulty of manual architecture design, neural architecture search (NAS) methods have been developed to automate this process. Early work by \citet{zoph2017neural} introduced the use of reinforcement learning to train a controller network that proposes architectures, using the accuracy of candidate models as a reward signal. More recent approaches have applied NAS specifically to Transformer models: AutoFormer~\citep{chen2021autoformer} efficiently searches over large design spaces (such as number of heads, depth, and MLP size) using weight-sharing, while NAS-BERT~\citep{xu2021nasbert} tailors BERT-like architectures for specific NLP tasks, reducing model size while maintaining performance through task-conditioned optimization.

Despite these advances, finding good architectures remains challenging and is difficult to do rigorously. Automated methods like NAS are increasingly important for discovering efficient and effective models, but robust principles for architecture design are still an open research area.

\section{Large Language Models}

The rapid progress in large language models (LLMs) has been marked by the development of a series of influential systems, each pushing the boundaries of scale, capability, and accessibility. GPT-3~\citep{brown2020gpt3} was a landmark model, demonstrating that sufficiently large transformer-based models could perform a wide range of tasks in a few-shot or even zero-shot setting, simply by conditioning on natural language prompts. Its successor, GPT-4~\citep{openai2023gpt4}, further increased model scale and performance, with the technical report highlighting the challenges of tuning and evaluating such massive models, often relying on smaller proxy models to predict performance. Open-source efforts have also made significant strides: DeepSeek LLM~\citep{deepseek2024llm} carefully tunes hyperparameters in line with scaling laws, while Mistral 7B~\citep{jiang2023mistral} introduces architectural innovations such as Grouped Query Attention and Sliding Window Attention for improved efficiency. PaLM~\citep{chowdhery2023palm} leverages the Pathways system to orchestrate large-scale distributed training, enabling the scaling of standard decoder architectures to hundreds of billions of parameters. Meta's OPT~\citep{zhang2022opt} and the BLOOM model~\citep{le2023bloom} from the BigScience project both emphasize transparency and open access, releasing model weights, training logs, and code to the research community.

Also there is something about meta llama \citep{touvron2023llama} and LLaMA 2 \citep{touvron2023llama2} that I want to write.

% And also \citep{rae2021scaling}

% DeepMind's Gopher~\citep{rae2021scaling} there is something here to say and Meta's LLaMA series~\citep{touvron2023llama} further demonstrate the benefits of scaling, with LLaMA models optimized for research use and LLaMA 2 offering improved pretraining, fine-tuning, and a commercial license. There are other things to do 

Other notable models include Anthropic's Claude series~\citep{anthropic2024claude3,anthropic2024claude35,anthropic2025claude37}, which emphasize “Constitutional AI” by training models to follow explicit principles for helpfulness, honesty, and harmlessness; Google DeepMind's Gemini models~\citep{deepmind2023gemini}, which are multimodal and support long-context reasoning; Baidu's ERNIE 4.0~\citep{baidu2023ernie4}, a multimodal model focused on Chinese-language tasks; and Alibaba's Qwen models~\citep{alibaba2023qwen}, which provide strong performance in both Chinese and English, with Qwen-VL extending capabilities to vision-language tasks. Collectively, these models exemplify the state of the art in LLM research, showcasing advances in scale, architecture, training methodology, and openness.



\section{Small Language Models}

This is something else I want to write.






