\chapter{Language Modeling Background and Small Language Models}

In this chapter, we will explore the background of language modeling, including the early approaches, word embeddings, and language models, leading up to current state of the art large language models. We will then pivot and outline some of the methods that researchers have used to make progress on efficiently training small language models. 

\section{Language Modeling Task}

In this section, we will explore some of the key concepts and techniques used in language modeling. 

Language modeling refers to the task of assigning probabilities to sequences of words or tokens. The goal is to model the likelihood of a sequence in a language. In the case of a sequence of words, $w = w_1, w_2, \ldots, w_n$, the probability of the sequence is given by the product of the probabilities of the words occurring in the sequence:

\begin{equation}    
    P(w) = P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^n P(w_i | w_1, \ldots, w_{i-1})
\end{equation}

Training a language model is then equivalent to estimating the parameters of this probability distribution. In practice, this is done by estimating the parameters of a neural network that maps a sequence of words to a probability distribution over the next word. 

\subsection{Early Approaches: N-Gram Models}
The earliest language models were based on n-gram statistics, where the probability of a word depends only on the preceding $n-1$ words. These models, described in foundational texts such as \cite{jurafsky2025speech}, are simple and effective but suffer from data sparsity and limited context. To address these issues, various smoothing techniques were developed, including Katz backoff \citep{katz2003estimation} and Kneser-Ney smoothing \citep{kneser1995improved}, with interpolated Kneser-Ney \citep{chen1999empirical} becoming the de facto standard for n-gram language modeling.

\subsection{Neural Language Models}

A major breakthrough came with the introduction of neural probabilistic language models by \cite{bengio2003neural}, which proposed using feed-forward neural networks to estimate the probability of word sequences. This approach introduced distributed word representations (embeddings), alleviating the data sparsity problem inherent in n-gram models. Subsequent work explored more powerful architectures, such as recurrent neural networks (RNNs) \citep{mikolov2010recurrent}, which can, in principle, capture dependencies across arbitrarily long contexts. However, training RNNs is challenging due to the vanishing and exploding gradient problems \citep{bengio1994learning}, leading to the development of gated architectures like Long Short-Term Memory (LSTM) networks \citep{hochreiter1997lstm} and Gated Recurrent Units (GRUs) \citep{cho2014gru}.
Large-scale empirical studies, such as \cite{jozefowicz2016exploring}, benchmarked various neural architectures and demonstrated that both model architecture and training data size significantly impact language modeling performance, as measured by perplexity.
\subsection{Word Embeddings}
In parallel, the adoption of distributed word representations, or word embeddings, marked a paradigm shift in NLP. Methods such as Skip-gram and Continuous Bag-of-Words (CBOW) \citep{mikolov2013efficient, mikolov2013distributed} and GloVe \citep{pennington2014glove} learn dense vector representations that capture syntactic and semantic relationships between words. These embeddings enabled transfer learning in NLP, as pre-trained vectors could be used for a variety of downstream tasks, including named entity recognition~\citep{lample2016neural}, sentence classification~\citep{kim2014convolutional}, and machine translation~\citep{qi2018translation}. However, traditional word embeddings are static, assigning the same vector to a word regardless of context.

Some novel methods have attempted to preproces or represent the input text data in unique ways, such as \cite{kim2016character} and phoneme-level representations \cite{goriely2024babble}.

\subsection{Contextualization and Attention}
To address the limitations of static embeddings, attention mechanisms were introduced, initially to solve the sequence alignment problem in machine translation \citep{bahdanau2015neural,luong2015effective}. Attention allows models to dynamically focus on relevant parts of the input sequence, improving translation quality and enabling the development of encoder-decoder architectures \citep{sutskever2014sequence}. The concept of contextual word embeddings was further advanced by models such as ELMo \citep{peters2018deep}, which extract context-sensitive representations from deep, bidirectional LSTMs.

\subsection{The Transformer and Modern Language Models}
The introduction of the Transformer architecture by Vaswani et al.~\citep{vaswani2017attention} marked a turning point in language modeling by replacing recurrence with self-attention mechanisms. This enabled efficient parallelization and the modeling of long-range dependencies, setting new performance benchmarks in machine translation and other NLP tasks. The Transformer built on earlier advances such as the soft attention mechanism of Bahdanau et al.~\citep{bahdanau2015neural}, which allowed neural machine translation models to dynamically align source and target sequences, improving translation quality and eliminating the need for a fixed-size encoding.
Building on the Transformer, a series of pre-trained language models have driven rapid progress in NLP. BERT~\citep{devlin2019bert} introduced deep bidirectional representations via masked language modeling and next sentence prediction, achieving state-of-the-art results on a range of benchmarks. Subsequent variants refined BERT’s objectives and architecture: RoBERTa~\citep{liu2019roberta} removed next sentence prediction and scaled up training, ALBERT~\citep{lan2019albert} reduced model size through parameter sharing and introduced sentence order prediction, and SpanBERT~\citep{joshi2020spanbert} focused on span-level pretraining for improved performance on span-based tasks. Other architectural innovations include DistilBERT~\citep{sanh2019distilbert}, which applies knowledge distillation to compress BERT while retaining most of its performance, and DeBERTa~\citep{he2021deberta}, which introduces disentangled attention and enhanced decoding strategies.
In parallel, the GPT series~\citep{radford2018gpt1, radford2019gpt2, brown2020gpt3} demonstrated the power of large-scale, unidirectional transformers trained with autoregressive objectives. GPT-1 established the pretrain-then-finetune paradigm, GPT-2 scaled up model and data size to enable strong zero- and few-shot generalization, and GPT-3 showed that further scaling leads to emergent capabilities and prompt-based learning.
A range of further innovations have extended the Transformer’s capabilities. Transformer-XL~\citep{dai2019transformer} introduced segment-level recurrence and relative positional encodings for improved long-context modeling. CTRL~\citep{keskar2019ctrl} enabled controllable generation via conditioning on control codes. ELECTRA~\citep{clark2020electra} reframed pretraining as a discriminative replaced-token detection task, improving sample efficiency. T5~\citep{raffel2020t5} unified NLP tasks under a text-to-text framework, while BART~\citep{lewis2020bart} combined denoising autoencoding with a seq2seq architecture for both understanding and generation. XLNet~\citep{yang2019xlnet} generalized autoregressive pretraining with permutation-based objectives, outperforming BERT on several benchmarks.
These advances collectively underpin the current generation of language models, which leverage architectural innovations, large-scale pretraining, and diverse objectives to achieve state-of-the-art performance across a wide range of NLP tasks.

\subsection{Training and Optimization}
Training large neural language models requires effective optimization techniques. Stochastic gradient descent (SGD) \citep{robbins1951stochastic} and its adaptive variants, such as Adagrad \citep{duchi2011adaptive}, RMSProp \citep{tieleman2012lecture}, Adam \citep{kingma2015adam}, and AdamW \citep{loshchilov2019decoupled}, are widely used. Regularization methods like dropout \citep{srivastava2014dropout}, layer normalization \citep{ba2016layernorm}, and residual connections \citep{he2016deep} improve training stability and generalization. Advances in positional encoding \citep{vaswani2017attention, shaw2018self, su2021roformer, press2021train} and activation functions \citep{hendrycks2016gelu, ramachandran2017searching, shazeer2020swiglu} have further enhanced model performance.

\subsection{Scaling and Architecture Search}
Recent research has shown that scaling model size and dataset size leads to predictable improvements in performance \citep{kaplan2020scaling, zhang2023train}. Neural architecture search (NAS) methods \citep{zoph2016neural, chen2021autoformer, xu2021nasbert} automate the design of model architectures, enabling the discovery of efficient and effective language models tailored to specific tasks and resource constraints.

