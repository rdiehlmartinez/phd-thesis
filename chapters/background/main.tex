\chapter{Language Modeling Background and Small Language Models}

In this chapter, we will explore the background of language modeling, including the early approaches, word embeddings, and language models, leading up to current state of the art large language models. We will then pivot and outline some of the methods that researchers have used to make progress on efficiently training small language models. 

\section{Language Modeling Task}

Language modeling refers to the task of assigning probabilities to sequences of words or tokens. The goal is to model the likelihood of a sequence in a language. In the case of a sequence of words, $w = w_1, w_2, \ldots, w_n$, the probability of the sequence is given by the product of the probabilities of the words occurring in the sequence:

\begin{equation}    
    P(w) = P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^n P(w_i | w_1, \ldots, w_{i-1})
\end{equation}

Training a language model is then equivalent to estimating the parameters of this probability distribution. In practice, this is done by estimating the parameters of a neural network that maps a sequence of words to a probability distribution over the next word. 

\subsection{Early Approaches: N-Gram Models}
The earliest language models were based on n-gram statistics, where the probability of a word depends only on the preceding $n-1$ words. These models, described in foundational texts such as \cite{jurafsky2025speech}, are simple and effective but suffer from data sparsity and limited context. To address these issues, various smoothing techniques were developed, including Katz backoff \citep{katz2003estimation} and Kneser-Ney smoothing \citep{kneser1995improved}, with interpolated Kneser-Ney \citep{chen1999empirical} becoming the de facto standard for n-gram language modeling.

\subsection{Neural Language Models}

A major breakthrough came with the introduction of neural probabilistic language models by \cite{bengio2003neural}, which proposed using feed-forward neural networks to estimate the probability of word sequences. This approach introduced distributed word representations (embeddings), alleviating the data sparsity problem inherent in n-gram models. Subsequent work explored more powerful architectures, such as recurrent neural networks (RNNs) \citep{mikolov2010recurrent}, which can, in principle, capture dependencies across arbitrarily long contexts. However, training RNNs is challenging due to the vanishing and exploding gradient problems \citep{bengio1994learning}, leading to the development of gated architectures like Long Short-Term Memory (LSTM) networks \citep{hochreiter1997lstm} and Gated Recurrent Units (GRUs) \citep{cho2014gru}.
Large-scale empirical studies, such as \cite{jozefowicz2016exploring}, benchmarked various neural architectures and demonstrated that both model architecture and training data size significantly impact language modeling performance, as measured by perplexity.
\subsection{Word Embeddings}
In parallel, the adoption of distributed word representations, or word embeddings, marked a paradigm shift in NLP. Methods such as Skip-gram and Continuous Bag-of-Words (CBOW) \citep{mikolov2013efficient, mikolov2013distributed} and GloVe \citep{pennington2014glove} learn dense vector representations that capture syntactic and semantic relationships between words. These embeddings enabled transfer learning in NLP, as pre-trained vectors could be used for a variety of downstream tasks, including named entity recognition~\citep{lample2016neural}, sentence classification~\citep{kim2014convolutional}, and machine translation~\citep{qi2018translation}. However, traditional word embeddings are static, assigning the same vector to a word regardless of context.

Some novel methods have attempted to preproces or represent the input text data in unique ways, such as \cite{kim2016character} and phoneme-level representations \cite{goriely2024babble}.

\subsection{Contextualization and Attention}
To address the limitations of static embeddings, attention mechanisms were introduced, initially to solve the sequence alignment problem in machine translation \citep{bahdanau2015neural,luong2015effective}. Attention allows models to dynamically focus on relevant parts of the input sequence, improving translation quality and enabling the development of encoder-decoder architectures \citep{sutskever2014sequence}. The concept of contextual word embeddings was further advanced by models such as ELMo \citep{peters2018deep}, which extract context-sensitive representations from deep, bidirectional LSTMs.

\subsection{The Transformer and Modern Language Models}
The introduction of the Transformer architecture by Vaswani et al.~\citep{vaswani2017attention} marked a turning point in language modeling by replacing recurrence with self-attention mechanisms. This enabled efficient parallelization and the modeling of long-range dependencies, setting new performance benchmarks in machine translation and other NLP tasks. The Transformer built on earlier advances such as the soft attention mechanism of Bahdanau et al.~\citep{bahdanau2015neural}, which allowed neural machine translation models to dynamically align source and target sequences, improving translation quality and eliminating the need for a fixed-size encoding.

BERT~\citep{devlin2019bert} introduced the masked language modeling (MLM) objective, where some percentage of input tokens are replaced with a special [MASK] token, and the model is trained to predict the original identity of these masked tokens given their context. Formally, for a sequence of tokens $w = (w_1, \ldots, w_n)$ and a set of masked positions $M \subset \{1, \ldots, n\}$, the MLM objective maximizes
\begin{equation}
    \mathcal{L}_{\text{MLM}} = \sum_{i \in M} \log P(w_i \mid w_{\setminus M}),
\end{equation}
where $w_{\setminus M}$ denotes the sequence with masked tokens replaced by [MASK]. This enables BERT to learn deep bidirectional representations, as the model can attend to both left and right context. 

Following BERT, a number of variants improved on its design. RoBERTa~\citep{liu2019roberta} removed the next sentence prediction task and scaled up training data and compute. ALBERT~\citep{lan2019albert} introduced parameter sharing and sentence order prediction to reduce model size and improve efficiency. SpanBERT~\citep{joshi2020spanbert} focused on span-level objectives for better performance on related tasks. Other architectural innovations include DistilBERT~\citep{sanh2019distilbert}, which compresses BERT via knowledge distillation, and DeBERTa~\citep{he2021deberta}, which enhances attention mechanisms through disentangled representations.

In contrast to masked language modeling, autoregressive pretraining—as used in the GPT series~\citep{radford2018gpt1, radford2019gpt2, brown2020gpt3}—trains models to predict each token based only on its preceding context. This objective maximizes the likelihood:

\begin{equation}
\mathcal{L}_{\text{AR}} = \sum_{i=1}^n \log P(w_i \mid w_1, \ldots, w_{i-1}),
\end{equation}

enforcing a unidirectional, left-to-right dependency. Despite this constraint, autoregressive models have proven particularly effective for large-scale pretraining, and form the foundation of many state-of-the-art language models used today. Their simplicity and scalability have contributed to their widespread adoption for open-ended generation tasks.

Building on these two main paradigms, several models have explored alternative objectives and architectures. One especially influential direction is the encoder-decoder, or text-to-text, framework. T5~\citep{raffel2020t5} exemplifies this approach by casting all NLP tasks—classification, translation, question answering—as text generation problems within a unified pretraining scheme. Similarly, BART~\citep{lewis2020bart} combines a denoising autoencoder objective with a sequence-to-sequence architecture, enabling both robust understanding and fluent generation.

Other notable innovations include Transformer-XL~\citep{dai2019transformer}, which improves long-context modeling through segment-level recurrence and relative positional encodings; CTRL~\citep{keskar2019ctrl}, which allows controllable generation via conditioning on control codes; ELECTRA~\citep{clark2020electra}, which reframes pretraining as a discriminative task of replaced-token detection for improved sample efficiency; and XLNet~\citep{yang2019xlnet}, which generalizes autoregressive modeling using permutation-based objectives to capture bidirectional dependencies without masking.

Together, these advances reflect a broad exploration of architectures and training strategies, enabling the current generation of language models to achieve impressive results across a wide range of NLP tasks

\subsection{Training and Optimization}

\paragraph{Optimization Algorithms.} The foundational method for training neural networks is stochastic gradient descent (SGD)\citep{robbins1951stochastic}, which iteratively updates model parameters using noisy gradient estimates from mini-batches. To address limitations in learning rate tuning and convergence speed, a range of adaptive optimizers have been developed. These include Adagrad\citep{duchi2011adaptive}, which adapts learning rates based on historical gradients; RMSProp~\citep{tieleman2012lecture}, which normalizes updates using a moving average of squared gradients; Adam~\citep{kingma2015adam}, which combines momentum with adaptive learning rates; and AdamW~\citep{loshchilov2019decoupled}, which decouples weight decay from the gradient update, improving regularization behavior.

\paragraph{Regularization and Stabilization.} To prevent overfitting and promote better generalization, regularization techniques such as dropout~\citep{srivastava2014dropout} randomly deactivate units during training, introducing noise that encourages robustness. Layer normalization~\citep{ba2016layernorm} standardizes activations across feature dimensions, stabilizing training in deep architectures. Residual connections~\citep{he2016deep}, originally introduced in ResNets, are critical in Transformer-based models, enabling deeper networks by mitigating vanishing gradients and facilitating gradient flow.

\paragraph{Architectural Enhancements.} Beyond optimization and regularization, several architectural components have played a crucial role in model performance. Positional encoding methods—such as the original sinusoidal encodings in the Transformer~\citep{vaswani2017attention}, relative positional encodings~\citep{shaw2018self}, Rotary Positional Embeddings (RoPE)\citep{su2024rope}, and learned position embeddings\citep{press2021train}—allow models to incorporate token order into otherwise permutation-invariant self-attention mechanisms. Additionally, advances in activation functions have contributed to better expressiveness and gradient flow. These include GELU~\citep{hendrycks2016gaussian}, Swish and other learned activations~\citep{ramachandran2017searching}, and Gated Linear Units (GLUs)~\citep{shazeer2020glu}, which enhance nonlinear modeling capacity while preserving training stability.

\subsection{Scaling and Architecture Search}
While recent research has shown that scaling model and dataset size leads to predictable improvements in performance~\citep{kaplan2020scaling, henighan2020scaling}, designing neural network architectures in a robust and rigorous way remains a significant challenge. Much of architecture design is still guided by empirical intuition and trial-and-error, rather than first principles. For example, \citet{levine2020depth} offer a structured perspective on the trade-offs between scaling depth and width in Transformers, proposing guidelines for balancing these factors under a fixed compute budget. The scaling laws established by \citet{kaplan2020scaling} demonstrate that both wider and deeper models can improve performance if scaled appropriately, but do not prescribe how to select specific architectural configurations for a given task or resource constraint.

To address the difficulty of manual architecture design, neural architecture search (NAS) methods have been developed to automate this process. Early work by \citet{zoph2017neural} introduced the use of reinforcement learning to train a controller network that proposes architectures, using the accuracy of candidate models as a reward signal. More recent approaches have applied NAS specifically to Transformer models: AutoFormer~\citep{chen2021autoformer} efficiently searches over large design spaces (such as number of heads, depth, and MLP size) using weight-sharing, while NAS-BERT~\citep{xu2021nasbert} tailors BERT-like architectures for specific NLP tasks, reducing model size while maintaining performance through task-conditioned optimization.

Despite these advances, finding good architectures remains challenging and is difficult to do rigorously. Automated methods like NAS are increasingly important for discovering efficient and effective models, but robust principles for architecture design are still an open research area.

\section{Large Language Models}

\subsection{Transformer-based Language Models}

The rapid progress in large language models (LLMs) has been marked by the development of a series of influential systems, each pushing the boundaries of scale, capability, and accessibility. GPT-3~\citep{brown2020gpt3} was a landmark model, demonstrating that sufficiently large transformer-based models could perform a wide range of tasks in a few-shot or even zero-shot setting, simply by conditioning on natural language prompts. Its successor, GPT-4~\citep{openai2023gpt4}, further increased model scale and performance, with the technical report highlighting the challenges of tuning and evaluating such massive models, often relying on smaller proxy models to predict performance. Open-source efforts have also made significant strides: DeepSeek LLM~\citep{deepseek2024llm} carefully tunes hyperparameters in line with scaling laws, while Mistral 7B~\citep{jiang2023mistral} introduces architectural innovations such as Grouped Query Attention and Sliding Window Attention for improved efficiency. PaLM~\citep{chowdhery2023palm} leverages the Pathways system to orchestrate large-scale distributed training, enabling the scaling of standard decoder architectures to hundreds of billions of parameters. Meta's OPT~\citep{zhang2022opt} and the BLOOM model~\citep{le2023bloom} from the BigScience project both emphasize transparency and open access, releasing model weights, training logs, and code to the research community. DeepMind's Gopher~\citep{rae2021scaling}  and Meta's LLaMA series~\citep{touvron2023llama} further demonstrate the benefits of scaling, with LLaMA models optimized for research use and LLaMA 2 offering improved pretraining, fine-tuning, and a commercial license.

Other notable models include Anthropic's Claude series~\citep{anthropic2024claude3,anthropic2024claude35,anthropic2025claude37}, which emphasize "Constitutional AI" by training models to follow explicit principles for helpfulness, honesty, and harmlessness; Google DeepMind's Gemini models~\citep{deepmind2023gemini}, which are multimodal and support long-context reasoning; Baidu's ERNIE 4.0~\citep{baidu2023ernie4}, a multimodal model focused on Chinese-language tasks; and Alibaba's Qwen models~\citep{alibaba2023qwen}, which provide strong performance in both Chinese and English, with Qwen-VL extending capabilities to vision-language tasks. Collectively, these models exemplify the state of the art in LLM research, showcasing advances in scale, architecture, training methodology, and openness.


\subsection{Alternative Architectures for Large Language Models}
While Transformers dominate large language modeling, recent research has explored alternative sequence modeling approaches to address their limitations, such as quadratic complexity and long-range dependency challenges. One such direction is state space models (SSMs). The HiPPO framework~\citep{gu2020hippo} introduced a method for representing continuous-time sequences using discrete orthogonal polynomial projections, enabling efficient memory representations. Building on this, the Structured State Space Sequence (S4) model~\citep{gu2021efficiently} uses a diagonal plus low-rank structure for the state transition matrix, allowing long-range dependencies to be modeled efficiently via fast convolution in the Fourier domain.
Mamba~\citep{gu2023mamba} advances SSMs by introducing dynamic, input-dependent state transitions, enabling Transformer-level performance with linear time and memory complexity. Another notable architecture is RWKV~\citep{peng2023rwkv}, a hybrid between Transformers and RNNs. RWKV replaces self-attention with a time-mixed mechanism, achieving linear complexity in sequence length and allowing parallel training, while remaining competitive on language modeling tasks.
These innovations demonstrate that efficient and expressive alternatives to Transformers are emerging, with the potential to further advance large-scale language modeling.

\subsection{Post-Training Methods for Large Language Models}
Recent advances in large language models (LLMs) have been driven not only by architectural innovations and pretraining scale, but also by sophisticated post-training methods that enhance factuality, alignment, and task performance. One prominent direction is retrieval-augmented generation (RAG), which combines parametric language models with non-parametric memory. For example, Retrieval-Augmented Generation (RAG)~\citep{lewis2020retrieval} integrates a retriever (such as Dense Passage Retrieval) to fetch relevant documents, and a generator (such as BART) to produce answers conditioned on both the query and retrieved passages. This approach enables LLMs to access up-to-date or domain-specific information beyond their training data, achieving state-of-the-art results on knowledge-intensive tasks. More recent models, such as Command R+~\citep{cohere2024commandrplus}, are fine-tuned specifically for retrieval-augmented workflows, further improving grounding and factual accuracy.
Another major area of post-training is alignment with human preferences and instructions. Reinforcement learning from human feedback (RLHF)~\citep{christiano2017deep, ouyang2022training} has become a standard technique, where models are fine-tuned using reward signals derived from human preference data. Proximal Policy Optimization (PPO)~\citep{schulman2017proximal} is a widely used policy gradient method in RLHF pipelines, balancing performance and stability during fine-tuning. Direct Preference Optimization (DPO)~\citep{rafailov2023direct} offers an alternative by directly optimizing model parameters from preference data, bypassing the need for a separate reward model and reinforcement learning loop. This method has been adopted in recent open-source models such as Zephyr~\citep{huggingface2023zephyr}, which are chat-tuned for high-quality conversational performance.
Instruction finetuning is another effective post-training strategy. Models such as InstructGPT~\citep{ouyang2022training} and FLAN~\citep{wei2021flan} are further trained on datasets of tasks phrased as instructions, often with human feedback. This process significantly improves the ability of LLMs to follow user instructions, generate helpful and truthful outputs, and generalize to unseen tasks. Recent work also highlights the importance of including chain-of-thought reasoning in finetuning data to boost model performance on complex tasks. Large-scale RL-based approaches, such as DeepSeek-R1~\citep{guo2025deepseekr1}, further incentivize advanced reasoning capabilities in LLMs.
Collectively, these post-training methods—retrieval augmentation, RLHF, DPO, and instruction finetuning—are critical for aligning LLMs with human values, improving factuality, and enabling robust performance across a wide range of real-world applications.


\subsection{Datasets for Large Language Model Pretraining}
The performance and generalization ability of large language models (LLMs) are strongly influenced by the scale, diversity, and quality of their pretraining datasets. Over the past several years, a number of large-scale corpora have been developed to support open and reproducible LLM research. Table~\ref{tab:llm-datasets} summarizes some of the most widely used datasets.
\begin{table}[h]
\centering
\begin{tabular}{p{3.5cm} p{1.5cm} p{8.5cm}}
\toprule
\textbf{Dataset} & \textbf{Size} & \textbf{Type of Data} \\
\midrule
OpenWebText~\citep{gokaslan2019openweb} & $\sim$8B & Web, Wikipedia \\
BooksCorpus~\citep{zhu2015aligning} & $\sim$0.8B & Fiction books \\
The Pile~\citep{gao2020pile} & 250B & Web, academic, code, books, forums \\
C4~\citep{raffel2020t5} & 365B & Web (Common Crawl) \\
RedPajama~\citep{together2023redpajama} & 1.2T & Web, books, Wikipedia, code, academic \\
RefinedWeb~\citep{penedo2023refinedweb} & 600B & Web (Common Crawl) \\
FineWeb~\citep{penedo2024fineweb} & 15T & Web, education \\
DOLMa~\citep{soldaini2024dolma} & 3T & Web, Wikipedia, books, code, academic, forums \\
\bottomrule
\end{tabular}
\caption{Major datasets used for large language model pretraining, with approximate number of tokens and data types.}
\label{tab:llm-datasets}
\end{table}
These datasets are constructed from a variety of sources, including web pages (Common Crawl), books, academic papers, code repositories, and community forums. Notably, OpenWebText~\citep{gokaslan2019openweb} and BooksCorpus~\citep{zhu2015aligning} were foundational for early models like GPT-2 and BERT, while The Pile~\citep{gao2020pile} and C4~\citep{raffel2020t5} introduced greater diversity and scale. More recent efforts such as RedPajama~\citep{together2023redpajama}, RefinedWeb~\citep{penedo2023refinedweb}, FineWeb~\citep{penedo2024fineweb}, and DOLMa~\citep{soldaini2024dolma} focus on openness, quality filtering, and massive scale, supporting the next generation of open LLMs.

\subsection{Evaluation Benchmarks for Large Language Models}
The rapid progress in large language models (LLMs) has been driven not only by advances in architecture and training scale, but also by the development of rigorous evaluation benchmarks. These benchmarks assess a model's ability to understand, reason, and generate language across a wide range of tasks. Below, we summarize some of the most influential benchmarks used to evaluate LLMs.
\paragraph{GLUE and SuperGLUE.} The General Language Understanding Evaluation (GLUE) benchmark \citep{wang2018glue} popularized the paradigm of transfer learning and fine-tuning in NLP. It consists of nine tasks, including natural language inference (MNLI, RTE) \citep{williams2018mnli,dagan2006rte}, paraphrase detection (MRPC) \citep{dolan2005mrpc}, sentiment analysis (SST-2) \citep{socher2013sst}, linguistic acceptability (CoLA) \citep{warstadt2019cola} and semantic similarity (STS-B) \citep{cer2017stsb}. BERT's breakthrough performance on GLUE established it as a standard benchmark, but as models approached the performance ceiling, overfitting became a concern. SuperGLUE \citep{wang2019superglue} was introduced as a more challenging successor, featuring tasks that require multi-sentence reasoning, commonsense inference, and coreference resolution \citep{zhang2018record,pilehvar2019wic,khashabi2018multirc,roemmele2011copa}. Progress on these benchmarks has closely tracked advances in LLM capabilities.

\paragraph{MMLU.} The Massive Multitask Language Understanding (MMLU) benchmark \citep{hendrycks2021mmlu} evaluates models on 57 diverse tasks spanning STEM, humanities, law, and more, using multiple-choice questions. MMLU emphasizes zero-shot and few-shot generalization, and is now a standard for evaluating the broad knowledge and reasoning abilities of frontier LLMs such as GPT-3, GPT-4, Claude, and LLaMA.

\paragraph{ARC.} The AI2 Reasoning Challenge (ARC) \citep{clark2018arc} tests scientific and commonsense reasoning using grade-school multiple-choice questions. The challenge set is specifically designed to be unsolvable by information retrieval alone, requiring genuine reasoning and world knowledge.

\paragraph{BIG-Bench and BIG-Bench Hard.} The Beyond the Imitation Game Benchmark (BIG-Bench) \citep{suzgun2023bigbenchhard} is a large-scale suite of over 200 tasks covering math, code, ethics, linguistics, and more, designed to probe

\paragraph{HellaSwag.} HellaSwag: Can a Machine Really Finish Your Sentence?~\citep{zellers2019hellaswag} is a commonsense inference benchmark that evaluates a model's ability to complete sentences in a grounded and contextually appropriate manner. Each example presents a context followed by four possible sentence endings, and the model must select the most plausible continuation. HellaSwag is specifically designed to be challenging for language models by minimizing the effectiveness of superficial statistical cues, thereby requiring genuine understanding of everyday scenarios and commonsense reasoning. This benchmark is widely used to assess whether models can move beyond surface-level pattern matching to deeper contextual comprehension.

\paragraph{Intrinsic Evaluation: Perplexity and Linguistic Probes.} Perplexity \citep{jelinek1977perplexity} remains a standard metric for evaluating the predictive quality of language models, though it does not capture all aspects of language understanding. Recent work \citep{magnusson2024paloma} argues for broader evaluation frameworks, including statistical properties of generated text. Linguistic benchmarks such as BLiMP \citep{warstadt2020blimp} and MSGS \citep{warstadt2020msgs} probe syntactic, semantic, and morphological competence, while Paloma \citep{magnusson2024paloma} measures how well models fit real-world human text distributions.
These benchmarks collectively provide a comprehensive picture of LLM capabilities, from basic language understanding and reasoning to factuality, safety, and linguistic competence. As models continue to improve, the development of new and more challenging benchmarks remains essential for tracking progress and diagnosing limitations.

\section{Properties of Large Language Models}

\subsection{Emergent Abilities}

Emergent abilities in large language models refer to capabilities that arise unpredictably as model size increases. \citet{wei2022emergent} introduce this concept, demonstrating that certain tasks, such as arithmetic reasoning and multi-step problem-solving, are only performed successfully by models beyond a specific scale. \citet{wu2024u} investigate the scaling patterns of LLMs, revealing that performance on certain tasks follows U-shaped or inverted-U scaling curves, proposing a method to predict the emergence threshold and model performance beyond that point. However, \citet{schaeffer2023mirage} challenge the notion that emergent abilities are unpredictable, arguing that the appearance of these abilities is a result of how performance metrics are measured and that, with appropriate evaluation methods, the emergence can be predicted. Additionally, \citet{wei2022chain} explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning, demonstrating that such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting.

\subsection{Generalization}

Generalization in large language models is a critical property that allows them to perform well on unseen data. \citet{belkin2019reconciling} show that increasing model capacity beyond the point of interpolation can actually improve test performance, depending on the given task. Many few-shot prompting abilities only emerge around 100-500B parameters, and over 1E+23 flops roughly. This is demonstrated empirically on linear models, neural networks, and kernel methods. \citet{yilmaz2022regularization} provide a theoretical exploration of why deep double descent occurs, relating to interpolation threshold, noise, and regularization, and providing insights for how and when overparameterized models avoid overfitting.

\subsection{Scaling Laws}

Scaling laws describe the relationship between model performance and factors such as model size, dataset size, and computational resources. \citet{kaplan2020scaling} establish a power-law relationship between these factors, recommending making models bigger while keeping the training dataset size relatively fixed. \citet{henighan2020scaling} extend these laws to a multimodal setting, showing that larger models continue improving even on out-of-distribution data. \citet{hoffman2022chinchilla} introduce the Chinchilla model, finding that models trained with more data for longer, but with fewer parameters, perform better for the same amount of compute, suggesting that data size should scale more aggressively than model size to be compute-optimal. \citet{hernandez2021scaling} explore how scaling affects transfer performance to downstream tasks, finding that larger models require less fine-tuning data, and downstream task performance also follows predictable scaling curves.

\section{Small Language Models}

This is something else I want to write.






