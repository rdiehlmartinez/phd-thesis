% === GENERAL ===

% ---- Software packages ---- 

@misc{hydra,
  author = {Omry Yadan},
  title = {Hydra -- A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year = {2019},
  url =          {https://github.com/facebookresearch/hydra}
}

% HuggingFace
@article{huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

% Pytorch
@inproceedings{paszke2017pytorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@Misc{accelerate,
  title = {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author = {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/accelerate}},
  year = {2022}
}

% Lightning Fabric
@misc{lightning-fabric,
  title = {Lightning Fabric},
  author = {{Lightning AI}},
  note = {Version 2.5.1},
  url = {https://lightning.ai/docs/fabric/stable/},
  year = {2025}
}

% Wandb
@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

% Eustace, Sébastien 2018 - Poetry: Python packaging and dependency management made easy
@misc{poetry,
  author       = {Eustace, Sébastien and others},
  title        = {Poetry: Python packaging and dependency management made easy},
  howpublished = {\url{https://python-poetry.org/}},
  year         = {2018},
  note         = {Version 1.0.0},
}

% Myle Ott 2019 - fairseq: A Fast, Extensible Toolkit for Sequence Modeling
@inproceedings{fairseq,
  title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author={Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:91184134}
}

@inproceedings{transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = "oct",
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6/",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
}


% NLTK
@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}


% ---- Architectures ---- 

% ROPE Embeddings 
@article{su2024rope,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}


% Zhang, Biao 2019 - Root mean square layer normalization
@article{zhang2019rms,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% Unknown 2022 - Flashattention: Fast and memory-efficient exact attention with io-awareness
@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

% Shazeer, Noam 2020 - Glu variants improve transformer
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

% Ainslie, Joshua 2023 - GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
@inproceedings{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023}
}


% Attention Is All You Need
@inproceedings{vaswani2017attention,
  title = {Attention Is All You Need},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}


% ---- Benchmarks ----

% GLUE 
@article{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={EMNLP 2018},
  pages={353},
  year={2018}
}

% Super GLUE
@article{wang2019superglue,
  title={{SuperGLUE}: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% BLIMP
@article{warstadt2020blimp,
  title={{BLiMP}: The benchmark of linguistic minimal pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={377--392},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


% MMLU
@inproceedings{
    hendrycks2021mmlu,
    title={Measuring Massive Multitask Language Understanding},
    author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

% BigBenchHard
@inproceedings{suzgun2023bigbenchhard,
    title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
    author = {Suzgun, Mirac  and
      Scales, Nathan  and
      Sch{\"a}rli, Nathanael  and
      Gehrmann, Sebastian  and
      Tay, Yi  and
      Chung, Hyung Won  and
      Chowdhery, Aakanksha  and
      Le, Quoc  and
      Chi, Ed  and
      Zhou, Denny  and
      Wei, Jason},
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.824",
    doi = "10.18653/v1/2023.findings-acl.824",
    pages = "13003--13051",
}

% Aarohi Srivastava 2023 - Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
@article{
    srivastava2023bigbench,
    title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
    author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and others},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=uyTL5Bvosj},
}

% Zellers, Rowan 2019 - HellaSwag: Can a Machine Really Finish Your Sentence?
@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

% Cobbe, Karl 2021 - GSM8K
@article{cobbe2021gsm8k,
    title={Training Verifiers to Solve Math Word Problems},
    author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
      journal={arXiv preprint arXiv:2110.14168},
      year={2021}
}

% Magnusson, Ian 2024 - Paloma: A benchmark for evaluating language model fit
@article{magnusson2024paloma,
  title={Paloma: A benchmark for evaluating language model fit},
  author={Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and Schwenk, Dustin and Walsh, Evan and Elazar, Yanai and Lo, Kyle and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={64338--64376},
  year={2024}
}

@article{warstadt2019cola,
    title = "Neural Network Acceptability Judgments",
    author = "Warstadt, Alex  and
      Singh, Amanpreet  and
      Bowman, Samuel R.",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1040/",
    doi = "10.1162/tacl_a_00290",
    pages = "625--641",
}

@inproceedings{socher2013sst,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170/",
    pages = "1631--1642"
}

@inproceedings{williams2018mnli,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101/",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
}

@inproceedings{rajpurkar2016squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264/",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392"
}

% Warstadt 2020 - MSGS
@inproceedings{warstadt2020msgs,
    title = "Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually)",
    author = "Warstadt, Alex  and
      Zhang, Yian  and
      Li, Xiaocheng  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.16/",
    doi = "10.18653/v1/2020.emnlp-main.16",
    pages = "217--235",
}

% ---- Tokenization  ----

% Gage, Philip 1994 - A new algorithm for data compression
@article{gage1994bpe,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={McPherson, KS: R \& D Publications, c1987-1994.}
}

% Rico Sennrich - BPE 
@inproceedings{sennrich2016bpe,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162/",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725"
}

% Kudo, Taku 2018 - Sentencepiece 
@inproceedings{kudo2018sentencepiece,
    title = "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
    author = "Kudo, Taku",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1007",
    doi = "10.18653/v1/P18-1007",
    pages = "66--75",
}

% --- Optimization Algorithms --- 

% Ilya Loshchilov 2019 - Decoupled Weight Decay Regularization (AdamW)
@article{
    loshchilov2019decoupled,
    title={Decoupled Weight Decay Regularization}, 
    author={Ilya Loshchilov and Frank Hutter},
    year={2019},
    journal = {arXiv preprint arXiv:1711.05101}
}

% --- Language/Semantics --- 

% Zipf's law 
@book{manning2009introduction,
  title={An introduction to information retrieval},
  author={Manning, Christopher D},
  year={2009},
  publisher={Cambridge university press}
}

@book{zipf1935zipflaw,
    author = "George K. Zipf",
    title = "The Psychobiology of Language",
    publisher = "Boston: Houghton-Mifflin",
    year = {1935}
}

@inproceedings{petrov2012universalpos,
    title = "A Universal Part-of-Speech Tagset",
    author = "Petrov, Slav  and
      Das, Dipanjan  and
      McDonald, Ryan",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}`12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L12-1115/",
    pages = "2089--2096",
}

% Salazar, Julian 2020 - Masked Language Model Scoring (Pseudo-Perplexity )
@inproceedings{salazar2020masked,
    title = "Masked Language Model Scoring",
    author = "Salazar, Julian  and
      Liang, Davis  and
      Nguyen, Toan Q.  and
      Kirchhoff, Katrin",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.240",
    doi = "10.18653/v1/2020.acl-main.240",
    pages = "2699--2712",
}

% Alfred V. Aho 1972 - The Theory of Parsing, Translation and Compiling
@book{aho1972parsing,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

% Dan Gusfield 1997 - Algorithms on Strings, Trees and Sequences
@book{gusfield1997algorithms,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

% --- Datasets ---

% Soldaini, Luca 2024 - Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research
@inproceedings{soldaini2024dolma,
  title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15725--15788},
  year={2024}
}

% Leo Gao 2020 - The Pile: An 800GB Dataset of Diverse Text for Language Modeling
@misc{gao2020pile,
    title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
    author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
    year={2020},
    journal={arXiv preprint arXiv:2101.00027}
}

% --- Models/ Model Frameworks ----

% Zhao, Wayne Xin 2023 - A survey of large language models
@article{zhao2023llmsurvey,
    title={A survey of large language models},
    author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
    url = {https://arxiv.org/abs/2303.18223},
    year = {2023},
    journal = {arXiv preprint arXiv:2303.18223}
}

@article{liu2019roberta,
    title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach}, 
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year = {2019},
    volume = {arXiv:1907.11692},
    journal = {arXiv preprint arXiv:1907.11692},
    url = {https://arxiv.org/abs/1907.11692},
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

% Touvron, Hugo 2023 - Llama 2: Open foundation and fine-tuned chat models
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{hoffman2022chinchilla,
    title={Training Compute-Optimal Large Language Models}, 
    author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
    year={2022},
    booktitle = {Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS)}
}

@article{black2021gptneo,
  title={Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow},
  author={Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  journal={If you use this software, please cite it using these metadata},
  volume={58},
  number={2},
  year={2021}
}

% Groeneveld, Dirk 2024 - OLMo: Accelerating the Science of Language Models
@inproceedings{groeneveld2024olmo,
  title={OLMo: Accelerating the Science of Language Models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Evan and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15789--15809},
  year={2024}
}

% Biderman, Stella 2023 - Pythia: A suite for analyzing large language models across training and scaling
@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

% Unknown 2023 - Bloom: A 176b-parameter open-access multilingual language model
@article{le2023bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal = {arXiv preprint arXiv:2211.05100},
  year={2023},
}

% Chowdhery, Aakanksha 2023 - Palm: Scaling language modeling with pathways
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

% Brown, Tom 2020 - GPT3
@article{brown2020gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% Zhang, Susan 2022 - Opt: Open pre-trained transformer language models
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

% Black, Sidney 2022 - GPT-NeoX-20B: An Open-Source Autoregressive Language Model
@inproceedings{black2022gpt,
	title        = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
	author       = {Black, Sidney and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
	year         = 2022,
	booktitle    = {Proceedings of BigScience Episode\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models},
	pages        = {95--136}
}

% Jason Wei 2021 - FLAN
@article{wei2021flan,
  author = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  title        = {Finetuned Language Models Are Zero-Shot Learners},
  year         = {2021},
  volume    = {arXiv:2109.01652},
  url          = {https://arxiv.org/abs/2109.01652},
  journal={ arXiv preprint arXiv:2109.01652},
}


% Raffel, Colin 2020 - Exploring the limits of transfer learning with a unified text-to-text transformer (T5 and C4 dataset)
@article{raffel2020t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

% Deepseek r1
@article{guo2025deepseekr1,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

% Deepseek v3 base model 
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}


% === Analysis  ===

% --- Scaling Law ---

% Nikhil Sardana 2024 - Forty-first International Conference on Machine Learning
@inproceedings{sardana2024beyond,
    title={Beyond Chinchilla-Optimal: {A}ccounting for Inference in Language Model Scaling Laws},
    author={Nikhil Sardana and Jacob Portes and Sasha Doubov and Jonathan Frankle},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=0bmXrtTDUu}
}

% Utkarsh Sharma 2022 - Scaling Laws from the Data Manifold Dimension
@article{sharma2022scalinglaws,
    title        = {Scaling Laws from the Data Manifold Dimension},
    author       = {Utkarsh Sharma and Jared Kaplan},
    year         = 2022,
    journal      = {Journal of Machine Learning Research},
    volume       = 23,
    number       = 9,
    pages        = {1--34},
    url          = {http://jmlr.org/papers/v23/20-1111.html}
}

% Hoffmann, Jordan 2022 - Training compute-optimal large language models
@article{hoffmann2022training,
    title        = {Training compute-optimal large language models},
    author       = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
    year         = 2022,
    journal      = {arXiv preprint 2203.15556}
}

% Kaplan, Jared 2020 - Scaling laws for neural language models
@article{kaplan2020scaling,
    title        = {Scaling laws for neural language models},
    author       = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
    year         = 2020,
    journal      = {arXiv preprint 2001.08361}
}

% --- Generalization ---

% Belkin, Mikhail 2019 - Reconciling modern machine-learning practice and the classical bias--variance trade-off
@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

% Hinton, Geoffrey 2015 - Distilling the knowledge in a neural network
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

% Phang, Jason 2021 - Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers
@inproceedings{phang2021finetuned,
    title = "Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers",
    author = "Phang, Jason  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    editor = "Bastings, Jasmijn  and
      Belinkov, Yonatan  and
      Dupoux, Emmanuel  and
      Giulianelli, Mario  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.blackboxnlp-1.42",
    doi = "10.18653/v1/2021.blackboxnlp-1.42",
    pages = "529--538",
}

% Du, Li 2023 - A Measure-Theoretic Characterization of Tight Language Models
@inproceedings{du2023measure,
    title = "A Measure-Theoretic Characterization of Tight Language Models",
    author = "Du, Li  and
      Torroba Hennigen, Lucas  and
      Pimentel, Tiago  and
      Meister, Clara  and
      Eisner, Jason  and
      Cotterell, Ryan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.543",
    doi = "10.18653/v1/2023.acl-long.543",
    pages = "9744--9770",
}

% Ryan Cotterell 2024 - Formal Aspects of Language Modeling
@article{cotterell-etal-2024-formal,
    title={Formal Aspects of Language Modeling}, 
    author={Ryan Cotterell and Anej Svete and Clara Meister and Tianyu Liu and Li Du},
    year={2024},
    url          = {https://arxiv.org/abs/2311.04329},
    journal      = {arXiv preprint 2311.04329}
}


% --- Influence Functions/Data Importance --- 

% Koh, Pang Wei 2017 - Understanding black-box predictions via influence functions
@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

% Cook, R Dennis 1980 - Characterizations of an empirical influence function for detecting influential cases in regression
@article{cook1980characterizations,
  title={Characterizations of an empirical influence function for detecting influential cases in regression},
  author={Cook, R Dennis and Weisberg, Sanford},
  journal={Technometrics},
  volume={22},
  number={4},
  pages={495--508},
  year={1980},
  publisher={Taylor \& Francis}
}

% Filighera, Anna 2019 - Automatic text difficulty estimation using embeddings and neural networks
@inproceedings{filighera2019automatic,
  title={Automatic text difficulty estimation using embeddings and neural networks},
  author={Filighera, Anna and Steuer, Tim and Rensing, Christoph},
  booktitle={Transforming Learning with Meaningful Technologies: 14th European Conference on Technology Enhanced Learning, EC-TEL 2019, Delft, The Netherlands, September 16--19, 2019, Proceedings 14},
  pages={335--348},
  year={2019},
  organization={Springer}
}


% --- Memorization ---

% Haviv, Adi 2023 - Understanding Transformer Memorization Recall Through Idioms
@inproceedings{haviv2023understanding,
    title = "Understanding Transformer Memorization Recall Through Idioms",
    author = "Haviv, Adi  and
      Cohen, Ido  and
      Gidron, Jacob  and
      Schuster, Roei  and
      Goldberg, Yoav  and
      Geva, Mor",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.19",
    doi = "10.18653/v1/2023.eacl-main.19",
    pages = "248--264",
}

% Feldman, Vitaly 2020 - What neural networks memorize and why: Discovering the long tail via influence estimation
@article{feldman2020neural,
  title={What neural networks memorize and why: Discovering the long tail via influence estimation},
  author={Feldman, Vitaly and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2881--2891},
  year={2020}
}

% Feldman, Vitaly 2020 - Does learning require memorization? a short tale about a long tail
@inproceedings{feldman2020does,
  title={Does learning require memorization? a short tale about a long tail},
  author={Feldman, Vitaly},
  booktitle={Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={954--959},
  year={2020}
}

@inproceedings{zheng2022memorization,
  title={An Empirical Study of Memorization in NLP},
  author={Xiaosen Zheng and Jing Jiang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247618876}
}


% Lesci, Pietro 2024 - Causal Estimation of Memorisation Profiles
@inproceedings{lesci2024causal,
    title = "Causal Estimation of Memorisation Profiles",
    author = "Lesci, Pietro  and
      Meister, Clara  and
      Hofmann, Thomas  and
      Vlachos, Andreas  and
      Pimentel, Tiago",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.834",
    doi = "10.18653/v1/2024.acl-long.834",
    pages = "15616--15635",
}

% Carlini, Nicholas 2021 - Extracting training data from large language models
@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

% Carlini, Nicholas 2022 - Quantifying Memorization Across Neural Language Models
@inproceedings{carlini2022quantifying,
  title={Quantifying Memorization Across Neural Language Models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

% Tirumala, Kushal 2022 - Memorization without overfitting: Analyzing the training dynamics of large language models
@article{tirumala2022memorization,
	title        = {Memorization without overfitting: Analyzing the training dynamics of large language models},
	author       = {Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
	year         = 2022,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {38274--38290}
}

% --- Behavior Probing ---

% Wes Gurnee 2023 - Finding Neurons in a Haystack: Case Studies with Sparse Probing
@article{
    gurnee2023finding,
    title={Finding Neurons in a Haystack: Case Studies with Sparse Probing},
    author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=JYs1R9IMJr},
    note={}
}

% Clark, Kevin 2019 - Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
@inproceedings{clark2019does,
    title = "What Does {BERT} Look at? An Analysis of {BERT}`s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Belinkov, Yonatan  and
      Hupkes, Dieuwke",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828/",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
}


% syntax trees are encoded in the geometry of BERT 
@inproceedings{hewitt2019structural,
    title = "{A} Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1419",
    doi = "10.18653/v1/N19-1419",
    pages = "4129--4138",
}


% Singh, Jasdeep 2019 - Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)
@inproceedings{singh2019bert,
    title = "{BERT} is Not an Interlingua and the Bias of Tokenization",
    author = "Singh, Jasdeep  and
      McCann, Bryan  and
      Socher, Richard  and
      Xiong, Caiming",
    editor = "Cherry, Colin  and
      Durrett, Greg  and
      Foster, George  and
      Haffari, Reza  and
      Khadivi, Shahram  and
      Peng, Nanyun  and
      Ren, Xiang  and
      Swayamdipta, Swabha",
    booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6106",
    doi = "10.18653/v1/D19-6106",
    pages = "47--55",
}

% --- Interpretability  --- 

% Michel, Paul 2019 - Advances in Neural Information Processing Systems
@inproceedings{michel2019sixteen,
     author = {Michel, Paul and Levy, Omer and Neubig, Graham},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
     pages = {},
     publisher = {Curran Associates, Inc.},
     title = {Are Sixteen Heads Really Better than One?},
     url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
     volume = {32},
     year = {2019}
}

% Nora Belrose 2023 - Eliciting Latent Predictions from Transformers with the Tuned Lens
@misc{belrose2023eliciting,
      title={Eliciting Latent Predictions from Transformers with the Tuned Lens}, 
      author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
      year={2023},
      eprint={2303.08112},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Voita, Elena 2019 - Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned
@inproceedings{voita2019analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580/",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
}


% Olah, Chris 2020 - Zoom in: An introduction to circuits
@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

% Elhage, Nelson 2021 - A Mathematical Framework for Transformer Circuits
@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

% Neel N 2022 - TransformerLens
@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

% Meng, Kevin 2022 - Locating and editing factual associations in GPT
@inproceedings{meng2022locating,
    author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
    title = {Locating and editing factual associations in GPT},
    year = {2022},
    isbn = {9781713871088},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
    articleno = {1262},
    numpages = {14},
    location = {New Orleans, LA, USA},
    series = {NIPS '22}
}

% Anthropic - Towards automated circuit discovery for mechanistic interpretability
@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}

% van Wingerden, Stan 2024 - DevInterp
@misc{devinterpcode,
  title = {DevInterp},
  author = {van Wingerden, Stan and Hoogland, Jesse and Wang, George and Zhou, William},
  year = {2024},
  howpublished = {\url{https://github.com/timaeus-research/devinterp}},
}

@misc{hoogland2023towards,
	title = {Towards {Developmental} {Interpretability}},
	url = {https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability},
	abstract = {Developmental interpretability is a research agenda that has grown out of a meeting of the Singular Learning Theory (SLT) and AI alignment communitie…},
	language = {en},
	urldate = {2025-03-18},
	author = {Hoogland, Jesse and Oldenziel, Alexander Gietelink and Murfet, Daniel and Wingerden, Stan van},
	month = jul,
	year = {2023},
}

% Javier Ferr 2024 - A Primer on the Inner Workings of Transformer-based Language Models
@article{ferrando2024primer,
    title={A Primer on the Inner Workings of Transformer-based Language Models}, 
    author={Javier Ferrando and Gabriele Sarti and Arianna Bisazza and Marta R. Costa-jussà},
    year={2024},
    journal      = {arXiv preprint 2405.00208},
    url = {https://arxiv.org/abs/2405.00208}
}

% Fabien Roger 2023 - Large Language Models Sometimes Generate Purely Negatively-Reinforced Text
@misc{roger2023negativelyreinforced,
      title={Large Language Models Sometimes Generate Purely Negatively-Reinforced Text}, 
      author={Fabien Roger},
      year={2023},
      eprint={2306.07567},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.07567}, 
}

% --- Learning Dynamics ---

% Diehl Martinez, Richard 2024 - Tending Towards Stability: Convergence Challenges in Small Language Models
@inproceedings{diehlmartinez2024tending,
    title = "Tending Towards Stability: Convergence Challenges in Small Language Models",
    author = "Diehl Martinez, Richard  and
      Lesci, Pietro  and
      Buttery, Paula",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.187/",
    doi = "10.18653/v1/2024.findings-emnlp.187",
    pages = "3275--3286",
}

@inproceedings{chai2024training,
    title = "On Training Data Influence of {GPT} Models",
    author = "Chai, Yekun  and
      Liu, Qingyi  and
      Wang, Shuohuan  and
      Sun, Yu  and
      Peng, Qiwei  and
      Wu, Hua",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.183/",
    doi = "10.18653/v1/2024.emnlp-main.183",
    pages = "3126--3150",
}

@article{godey2024small,
    title        = {Why do small language models underperform? {S}tudying Language Model Saturation via the Softmax Bottleneck},
    author       = {Godey, Nathan and de la Clergerie, {\'E}ric and Sagot, Beno{\^\i}t},
    year         = 2024,
    journal      = {arXiv preprint 2404.07647},
    url = {https://arxiv.org/abs/2404.07647}
}

@inproceedings{nguyen2020wide,
    title={Do Wide and Deep Networks Learn the Same Things? {U}ncovering How Neural Network Representations Vary with Width and Depth},
    author={Thao Nguyen and Maithra Raghu and Simon Kornblith},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=KJNcAkY8tY4}
}

@inproceedings{phang2021fine,
    title = "Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers",
    author = "Phang, Jason  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    editor = "Bastings, Jasmijn  and
      Belinkov, Yonatan  and
      Dupoux, Emmanuel  and
      Giulianelli, Mario  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.blackboxnlp-1.42",
    doi = "10.18653/v1/2021.blackboxnlp-1.42",
    pages = "529--538",
}

% SVCCA ELMO learns similar representation to that of POS tagger
@inproceedings{saphra2019understanding,
    title = "Understanding Learning Dynamics Of Language Models with {SVCCA}",
    author = "Saphra, Naomi  and
      Lopez, Adam",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1329",
    doi = "10.18653/v1/N19-1329",
    pages = "3257--3267",
}

% Belrose, Nora 2024 - Neural Networks Learn Statistics of Increasing Complexity
@article{belrose2024neural,
	title        = {Neural Networks Learn Statistics of Increasing Complexity},
	author       = {Belrose, Nora and Pope, Quintin and Quirke, Lucia and Mallen, Alex and Fern, Xiaoli},
	year         = 2024,
	journal      = {arXiv preprint 2402.04362},
url = {https://arxiv.org/abs/2402.04362}
}

% Enric Boix-Adsera 2023 - Transformers Learn Through Gradual Rank Increase
@inproceedings{boix-adsera2023rank,
	title        = {Transformers Learn Through Gradual Rank Increase},
	author       = {Enric Boix-Adsera and Etai Littwin and Emmanuel Abbe and Samy Bengio and Joshua Susskind},
	year         = 2023,
	booktitle    = {NeurIPS},
	url          = {https://arxiv.org/abs/2306.07042}
}

% Michaelov, James 2023 - Emergent Inabilities? Inverse Scaling Over the Course of Pretraining
@inproceedings{michaelov2023emergent,
    title = "Emergent Inabilities? Inverse Scaling Over the Course of Pretraining",
    author = "Michaelov, James  and
      Bergen, Ben",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.973/",
    doi = "10.18653/v1/2023.findings-emnlp.973",
    pages = "14607--14615",
}

% Jesse Hoogl 2025 - Loss Landscape Degeneracy Drives Stagewise Development in Transformers
@misc{hoogland2025losslandscape,
      title={Loss Landscape Degeneracy Drives Stagewise Development in Transformers}, 
      author={Jesse Hoogland and George Wang and Matthew Farrugia-Roberts and Liam Carroll and Susan Wei and Daniel Murfet},
      year={2025},
      eprint={2402.02364},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02364}, 
}

% --- Representation Similarity/ Metrics ---

% Kornblith, Simon 2019 - Similarity of neural network representations revisited
@inproceedings{kornblith2019cka,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

% Morcos, Ari 2018 - Insights on representational similarity in neural networks with canonical correlation
@article{morcos2018pwcca,
  title={Insights on representational similarity in neural networks with canonical correlation},
  author={Morcos, Ari and Raghu, Maithra and Bengio, Samy},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

% Raghu, Maithra 2017 - Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability
@article{raghu2017svcca,
  title={Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
  author={Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% Bansal, Yamini 2021 - Revisiting model stitching to compare neural representations
@article{bansal2021revisiting,
	title        = {Revisiting model stitching to compare neural representations},
	author       = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
	year         = 2021,
	journal      = {Advances in neural information processing systems},
	volume       = 34,
	pages        = {225--236}
}

% Lenc, Karel 2015 - Understanding image representations by measuring their equivariance and equivalence
@inproceedings{lenc2015understanding,
	title        = {Understanding image representations by measuring their equivariance and equivalence},
	author       = {Lenc, Karel and Vedaldi, Andrea},
	year         = 2015,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {991--999}
}

% Wu, John 2020 - Similarity Analysis of Contextual Word Representation Models
@inproceedings{wu2020similarity,
    title = "Similarity Analysis of Contextual Word Representation Models",
    author = "Wu, John  and
      Belinkov, Yonatan  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Glass, James",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.422",
    doi = "10.18653/v1/2020.acl-main.422",
    pages = "4638--4655",
}

% Brown, Davis 2023 - Understanding the Inner-workings of Language Models Through Representation Dissimilarity
@inproceedings{brown2023understanding,
    title = "Understanding the Inner-workings of Language Models Through Representation Dissimilarity",
    author = "Brown, Davis  and
      Godfrey, Charles  and
      Konz, Nicholas  and
      Tu, Jonathan  and
      Kvinge, Henry",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.403",
    doi = "10.18653/v1/2023.emnlp-main.403",
    pages = "6543--6558",
}

% --- Analysis Metrics ---- %

% Hoyer, Patrik O 2004 - Non-negative matrix factorization with sparseness constraints
@article{hoyer2004sparsity,
  title={Non-negative matrix factorization with sparseness constraints},
  author={Hoyer, Patrik O},
  journal={Journal of machine learning research},
  volume={5},
  number={Nov},
  pages={1457--1469},
  year={2004}
}

% Hurley, Niall 2009 - Comparing measures of sparsity
@article{hurley2009gini,
  title={Comparing measures of sparsity},
  author={Hurley, Niall and Rickard, Scott},
  journal={IEEE Transactions on Information Theory},
  volume={55},
  number={10},
  pages={4723--4741},
  year={2009},
  publisher={IEEE}
}


% Roy, Olivier 2007 - 15th European Signal Processing Conference
@inproceedings{roy2007effectiverank,
  author={Roy, Olivier and Vetterli, Martin},
  booktitle={15th European Signal Processing Conference}, 
  title={The effective rank: {A} measure of effective dimensionality}, 
  year={2007},
  pages={606-610},
  url={https://www.eurasip.org/Proceedings/Eusipco/Eusipco2007/Papers/a5p-h05.pdf}
}


% === Pretraining  ===

% Label Smoothing 
@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}


% --- Transfer/Multi-task Learning  ---

% Caruana, Rich 1997 - Multitask learning
@article{caruana1997multitask,
    title={Multitask learning},
    author={Caruana, Rich},
    journal={Machine learning},
    volume={28},
    pages={41--75},
    year={1997},
    publisher={Springer}
}

% Ando, Rie Kubota 2005 - A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data
@article{ando2005predictivetasks,
    Acmid = {1194905},
    Author = {Ando, Rie Kubota and Zhang, Tong},
    Issn = {1532-4435},
    Issue_Date = {12/1/2005},
    Journal = {Journal of Machine Learning Research},
    Month = dec,
    Numpages = {37},
    Pages = {1817--1853},
    Publisher = {JMLR.org},
    Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
    Volume = {6},
    Year = {2005},
    url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

% --- Curriculum Learning ---

@inproceedings{diehlmartinez2023climb,
    title = "{CLIMB} {--} Curriculum Learning for Infant-inspired Model Building",
    author = "Diehl Martinez, Richard  and
      Goriely, Z{\'e}bulon  and
      McGovern, Hope  and
      Davis, Christopher  and
      Caines, Andrew  and
      Buttery, Paula  and
      Beinborn, Lisa",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.10/",
    doi = "10.18653/v1/2023.conll-babylm.10",
    pages = "112--127"
}

% Jeffrey L. Elman 1993 - Learning and development in neural networks: the importance of starting small
@article{elman1993learning,
    author = {Jeffrey L. Elman},
    title = {Learning and development in neural networks: the importance of starting small},
    journal = {Cognition},
    volume = {48},
    number = {1},
    pages = {71-99},
    year = {1993},
    issn = {0010-0277},
    doi = {https://doi.org/10.1016/0010-0277(93)90058-4},
}

% Soviany, Petru 2022 - Curriculum learning: A survey
@article{soviany2022curriculum,
  title={Curriculum learning: A survey},
  author={Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  journal={International Journal of Computer Vision},
  pages={1--40},
  year={2022},
  publisher={Springer}
}

% Tom Kocmi 2017 - Curriculum Learning and Minibatch Bucketing in Neural Machine Translation
@inproceedings{kocmi2017curriculum,
    title={Curriculum Learning and Minibatch Bucketing in Neural Machine Translation},
  author={Kocmi, Tom and Bojar, Ond{\v{r}}ej},
    booktitle={Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP 2017},
    pages={379--386},
    year={2017}
}

% Yoshua Bengio 2009 - Curriculum learning
@inproceedings{bengio2009curriculum,
    title={Curriculum learning},
    author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
    booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
    pages={41--48},
    year={2009}
}

% Xiaoxia Wu 2021 - When Do Curricula Work?
@inproceedings{wu2021when,
    title={When Do Curricula Work?},
    author={Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=tW4QEInpni}
}

% Unknown 2019 - Competence-based Curriculum Learning for Neural Machine Translation
@inproceedings{platanios2019competence,
  title={Competence-based Curriculum Learning for Neural Machine Translation},
  author={Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Pocz{\'o}s, Barnab{\'a}s and Mitchell, Tom},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1162--1172},
  year={2019}
}

% Yile Wang 2023 - Language Model Pre-training with Linguistically Motivated Curriculum Learning
@misc{wang2023language,
    title={Language Model Pre-training with Linguistically Motivated Curriculum Learning},
    author={Yile Wang and Yue Zhang and Peng Li and Yang Liu},
    year={2023},
    url={https://openreview.net/forum?id=y7CNId2RnV}
}

% Liu, Cao 2018 - Curriculum Learning for Natural Answer Generation
@inproceedings{liu2018curriculum,
    title={Curriculum Learning for Natural Answer Generation},
    author={Liu, Cao and He, Shizhu and Liu, Kang and Zhao, Jun and others},
    booktitle={IJCAI},
    pages={4223--4229},
    year={2018}
}

% Daniel Campos 2021 - Curriculum learning for language modeling
@article{campos2021curriculum,
    title={Curriculum learning for language modeling}, 
    author={Daniel Campos},
    year={2021},
    journal={arXiv preprint arXiv:2108.02170}
}

% Natalie Schluter 2018 - When data permutations are pathological: the case of neural natural language inference
@inproceedings{schluter2018data,
    title = "When data permutations are pathological: the case of neural natural language inference",
    author = "Schluter, Natalie  and
      Varab, Daniel",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1534/",
    doi = "10.18653/v1/D18-1534",
    pages = "4935--4939",
}


@inproceedings{sachan2016easy,
    title = "Easy Questions First? A Case Study on Curriculum Learning for Question Answering",
    author = "Sachan, Mrinmaya  and
      Xing, Eric",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1043/",
    doi = "10.18653/v1/P16-1043",
    pages = "453--463"
}

@inproceedings{lalor2020dynamic,
    title = "Dynamic Data Selection for Curriculum Learning via Ability Estimation",
    author = "Lalor, John P.  and
      Yu, Hong",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.48/",
    doi = "10.18653/v1/2020.findings-emnlp.48",
    pages = "545--555",
}


@article{li2021curriculum,
    author       = {Conglong Li and
                  Minjia Zhang and
                  Yuxiong He},
    title        = {Curriculum Learning: {A} Regularization Method for Efficient and Stable
                  Billion-Scale {GPT} Model Pre-Training},
    journal      = {CoRR},
    volume       = {abs/2108.06084},
    year         = {2021},
    url          = {https://arxiv.org/abs/2108.06084},
    eprinttype    = {arXiv},
    eprint       = {2108.06084},
    timestamp    = {Wed, 18 Aug 2021 19:45:42 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2108-06084.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% --- Small Language Model/ Frameworks  --- 

% Karpathy, Andrej 2023 - nanoGPT
@misc{karpathy2023nanogpt,
    title = {nanoGPT},
    author = {Karpathy, Andrej},
    year = {2023},
    url = {https://github.com/karpathy/nanoGPT},
}


% Ronen Eldan 2023 - TinyStories: How Small Can Language Models Be and Still Speak Coherent English?
@misc{eldan2023tinystoriessmalllanguagemodels,
    title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?}, 
    author={Ronen Eldan and Yuanzhi Li},
    year={2023},
    eprint={2305.07759},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2305.07759}, 
}

% Wu 2024 - LaMini Model 
@inproceedings{wu2024lamini,
  title={LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},
  author={Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and Abdul-Mageed, Muhammad and Aji, Alham},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={944--964},
  year={2024}
}

% Zhang, Peiyuan 2024 - Tinyllama: An open-source small language model
@article{zhang2024tinyllama,
    title={Tinyllama: An open-source small language model},
    author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
    journal={arXiv preprint arXiv:2401.02385},
    year={2024}
}

% Liu et al 2024 - mobilellm 
@inproceedings{liu2024mobilellm,
    title={Mobilellm: Optimizing sub-billion parameter language models for on-device use cases},
    author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
}

% Abdin - Phi-3 model 
@article{abdin2024phi,
    title={Phi-3 technical report: A highly capable language model locally on your phone},
    author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
    journal={arXiv preprint arXiv:2404.14219},
    year={2024}
}

% Mehta el al. Apple OpenELM 
@article{mehta2024openelm,
  title={Openelm: An efficient language model family with open training and inference framework},
  author={Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and others},
  journal={arXiv preprint arXiv:2404.14619},
  year={2024}
}


% Loubna Ben Allal 2025 - SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model
@article{allal2025smollm2,
  title={SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model},
  author={Allal, Loubna Ben and Lozhkov, Anton and Bakouch, Elie and Bl{\'a}zquez, Gabriel Mart{\'\i}n and Penedo, Guilherme and Tunstall, Lewis and Marafioti, Andr{\'e}s and Kydl{\'\i}{\v{c}}ek, Hynek and Lajar{\'\i}n, Agust{\'\i}n Piqueres and Srivastav, Vaibhav and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}


% --- Efficient learning --- 

% Conover, Mike 2023 - Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM
@misc{databricksdolly2023,
    author = {Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
    title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    shorttitle = {Free Dolly},
    journal = {Databricks Blog},
    year = {2023},
    month = {April},
    url = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate = {2025-03-12},
    language = {en},
}

@misc{mosaic2023mpt,
    title = {Introducing {MPT}-{7B}: {A} {New} {Standard} for {Open}-{Source}, {Commercially} {Usable} {LLMs}},
    shorttitle = {Introducing {MPT}-{7B}},
    url = {https://www.databricks.com/blog/mpt-7b},
    abstract = {Introducing MPT-7B, the first entry in our MosaicML Foundation Series. MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. MPT-7B was trained on the MosaicML platform in 9.5 days with zero human intervention at a cost of {\textasciitilde}\$200k.},
    language = {en-US},
    urldate = {2025-03-12},
    journal = {Databricks},
    author = {{MosaicML NLP Team}},
    month = may,
    year = {2023},
}

% Narayanan, Deepak 2021 - Efficient large-scale language model training on gpu clusters using megatron-lm
@inproceedings{narayanan2021megatron,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages={1--15},
  year={2021}
}

% Rasley, Jeff 2020 - Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters
@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={3505--3506},
  year={2020}
}

% Rajbh 2020 - Zero: Memory optimizations toward training trillion parameter models
@inproceedings{rajbhandari2020zero,
    title = {Zero: Memory optimizations toward training trillion parameter models},
    author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
    year = 2020,
    booktitle = {SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
    pages = {1--16},
    organization = {IEEE}
}

% Geiping, Jonas 2023 - International Conference on Machine Learning
@inproceedings{geiping2023cramming,
    title={Cramming: Training a Language Model on a single {GPU} in one day.},
    author={Geiping, Jonas and Goldstein, Tom},
    booktitle={International Conference on Machine Learning},
    pages={11117--11143},
    year={2023},
    organization={PMLR}
}

@inproceedings{zhang2021need,
    title = "When Do You Need Billions of Words of Pretraining Data?",
    author = "Zhang, Yian  and
      Warstadt, Alex  and
      Li, Xiaocheng  and
      Bowman, Samuel R.",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.90/",
    doi = "10.18653/v1/2021.acl-long.90",
    pages = "1112--1125",
}

@inproceedings{izsak2021train,
    title = "How to Train {BERT} with an Academic Budget",
    author = "Izsak, Peter  and
      Berchansky, Moshe  and
      Levy, Omer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.831/",
    doi = "10.18653/v1/2021.emnlp-main.831",
    pages = "10644--10652",
}

@inproceedings{huebner2021babyberta,
  title={BabyBERTa: Learning more grammar with small-scale child-directed language},
  author={Huebner, Philip A and Sulem, Elior and Cynthia, Fisher and Roth, Dan},
  booktitle={Proceedings of the 25th conference on computational natural language learning},
  pages={624--646},
  year={2021}
}

% --- Architectural Modification ---

@inproceedings{samuel2023ltgbert,
    title = "Trained on 100 million words and still in shape: {BERT} meets {B}ritish {N}ational {C}orpus",
    author = "Samuel, David  and
      Kutuzov, Andrey  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.146/",
    doi = "10.18653/v1/2023.findings-eacl.146",
    pages = "1954--1974",
}


% --- Knowledge Distillation --- 

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{yam2024tinyminds,
    title = "Teaching Tiny Minds: Exploring Methods to Enhance Knowledge Distillation for Small Language Models",
    author = "Yam, Hong Meng  and
      Paek, Nathan",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.27/",
    pages = "302--307",
}

% BabyLlama 2023 - tiny models can be distilled (babylm 2023 submission)
@inproceedings{timiryasov-tastet-2023-baby,
    title = "Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty",
    author = "Timiryasov, Inar  and
      Tastet, Jean-Loup",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.24/",
    doi = "10.18653/v1/2023.conll-babylm.24",
    pages = "279--289"
}

% BabyLlama2 2024 - follow up to BabyLlama (babylm 2024 submission)
@inproceedings{tastet2024babyllama2,
    title = "{B}aby{L}lama-2: Ensemble-Distilled Models Consistently Outperform Teachers With Limited Data",
    author = "Tastet, Jean-Loup  and
      Timiryasov, Inar",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.26/",
    pages = "292--301",
}

% === Word Embeddings  ===

% Character-level embeddings 
@inproceedings{kim2016character,
  title={Character-aware neural language models},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}


% -- Morphological/Syntatic  Information --

@inproceedings{diehlmartinez2024syntacticsmoothing,
    title = "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing",
    author = "Diehl Martinez, Richard  and
      Goriely, Z{\'e}bulon  and
      Caines, Andrew  and
      Buttery, Paula  and
      Beinborn, Lisa",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.344/",
    doi = "10.18653/v1/2024.emnlp-main.344",
    pages = "5999--6011",
}

@inproceedings{bai2022better,
    title = "Better Language Model with Hypernym Class Prediction",
    author = "Bai, He  and
      Wang, Tong  and
      Sordoni, Alessandro  and
      Shi, Peng",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.96/",
    doi = "10.18653/v1/2022.acl-long.96",
    pages = "1352--1362",
}


@article{stratos2016unsupervisedpos,
    title = "Unsupervised Part-Of-Speech Tagging with Anchor Hidden {M}arkov Models",
    author = "Stratos, Karl  and
      Collins, Michael  and
      Hsu, Daniel",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1018/",
    doi = "10.1162/tacl_a_00096",
    pages = "245--257",
}

% Bojanowski, Piotr 2017 - Enriching Word Vectors with Subword Information
@article{bojanowski2017enrichingsubword,
    title = "Enriching Word Vectors with Subword Information",
    author = "Bojanowski, Piotr  and
      Grave, Edouard  and
      Joulin, Armand  and
      Mikolov, Tomas",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1010",
    doi = "10.1162/tacl_a_00051",
    pages = "135--146",
}

@inproceedings{botha2014compositional,
    title={Compositional morphology for word representations and language modelling},
    author={Botha, Jan and Blunsom, Phil},
    booktitle={International Conference on Machine Learning},
    pages={1899--1907},
    year={2014},
    organization={PMLR}
}

@inproceedings{vulic2017morphfitting,
    title = "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules",
    author = "Vuli{\'c}, Ivan  and
      Mrk{\v{s}}i{\'c}, Nikola  and
      Reichart, Roi  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Young, Steve  and
      Korhonen, Anna",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1006/",
    doi = "10.18653/v1/P17-1006",
    pages = "56--68",
}

@inproceedings{salle2018incorporating,
    title = "Incorporating Subword Information into Matrix Factorization Word Embeddings",
    author = "Salle, Alexandre  and
      Villavicencio, Aline",
    editor = {Faruqui, Manaal  and
      Sch{\"u}tze, Hinrich  and
      Trancoso, Isabel  and
      Tsvetkov, Yulia  and
      Yaghoobzadeh, Yadollah},
    booktitle = "Proceedings of the Second Workshop on Subword/Character {LE}vel Models",
    month = jun,
    year = "2018",
    address = "New Orleans",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-1209/",
    doi = "10.18653/v1/W18-1209",
    pages = "66--71",
}

@inproceedings{cotterel2015morphological,
    title = "Morphological Word-Embeddings",
    author = {Cotterell, Ryan  and
      Sch{\"u}tze, Hinrich},
    editor = "Mihalcea, Rada  and
      Chai, Joyce  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "–" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N15-1140/",
    doi = "10.3115/v1/N15-1140",
    pages = "1287--1292"
}

@inproceedings{bhatia2016morphological,
    title = "Morphological Priors for Probabilistic Neural Word Embeddings",
    author = "Bhatia, Parminder  and
      Guthrie, Robert  and
      Eisenstein, Jacob",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1047/",
    doi = "10.18653/v1/D16-1047",
    pages = "490--500"
}

% Cui, Yiming 2022 - Lert: A linguistically-motivated pre-trained language model
@article{cui2022lert,
  title={Lert: A linguistically-motivated pre-trained language model},
  author={Cui, Yiming and Che, Wanxiang and Wang, Shijin and Liu, Ting},
  journal={arXiv preprint arXiv:2211.05344},
  year={2022}
}

% Mueller, Aaron 2023 - How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases
@article{mueller2023plant,
    title={How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases},
    author={Mueller, Aaron and Linzen, Tal},
    url= {https://arxiv.org/abs/2305.19905},
    year={2023},
    journal={arXiv preprint arXiv:2305.19905}}
}

% --- Rare Words --- 

% Unknown 2019 - Attentive mimicking: Better word embeddings by attending to informative contexts
@article{schick2019attentive,
  title={Attentive mimicking: Better word embeddings by attending to informative contexts},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1904.01617},
  year={2019}
}

% Shick, Timo 2020 - Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking
@inproceedings{schick2020rare,
  title={Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  issue={05},
  pages={8766--8774},
  year={2020}
}

% Gong, Chengyue 2018 - Frage: Frequency-agnostic word representation
@article{gong2018frage,
  title={Frage: Frequency-agnostic word representation},
  author={Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

% Raunak, Vikas 2019 - Effective Dimensionality Reduction for Word Embeddings
@inproceedings{raunak2019effective,
    title = "Effective Dimensionality Reduction for Word Embeddings",
    author = "Raunak, Vikas  and
      Gupta, Vivek  and
      Metze, Florian",
    editor = "Augenstein, Isabelle  and
      Gella, Spandana  and
      Ruder, Sebastian  and
      Kann, Katharina  and
      Can, Burcu  and
      Welbl, Johannes  and
      Conneau, Alexis  and
      Ren, Xiang  and
      Rei, Marek",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4328",
    doi = "10.18653/v1/W19-4328",
    pages = "235--243",
}

% Mu, Jiaqi 2018 - All-but-the-Top: Simple and Effective Postprocessing for Word Representations
@inproceedings{mu2018all,
  title={All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
  author={Mu, Jiaqi and Viswanath, Pramod},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

% Arora, Sanjeev 2016 - A Simple but Tough-to-Beat Baseline for Sentence Embeddings
@inproceedings{arora2016simple,
  title={A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
  author={Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

% Press, Ofir 2017 - Using the Output Embedding to Improve Language Models
@inproceedings{press2017using,
  title={Using the Output Embedding to Improve Language Models},
  author={Press, Ofir and Wolf, Lior},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={157--163},
  year={2017}
}

% Hakan Inan 2017 - Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling
@inproceedings{
    inan2017tying,
    title={Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling},
    author={Hakan Inan and Khashayar Khosravi and Richard Socher},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=r1aPbsFle}
}

% Yu, Sangwon 2022 - Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings
@inproceedings{yu2022rare,
    title = "Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings",
    author = "Yu, Sangwon  and
      Song, Jongyoon  and
      Kim, Heeseung  and
      Lee, Seongmin  and
      Ryu, Woo-Jong  and
      Yoon, Sungroh",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.3",
    doi = "10.18653/v1/2022.acl-long.3",
    pages = "29--45",
}

% Kobayashi, Goro 2023 - Transformer Language Models Handle Word Frequency in Prediction Head
@inproceedings{kobayashi2023transformer,
    title = "Transformer Language Models Handle Word Frequency in Prediction Head",
    author = "Kobayashi, Goro  and
      Kuribayashi, Tatsuki  and
      Yokoi, Sho  and
      Inui, Kentaro",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.276",
    doi = "10.18653/v1/2023.findings-acl.276",
    pages = "4523--4535"
}


% -- Anisotropy ---

@article{arora2016latent,
    title = "A Latent Variable Model Approach to {PMI}-based Word Embeddings",
    author = "Arora, Sanjeev  and
      Li, Yuanzhi  and
      Liang, Yingyu  and
      Ma, Tengyu  and
      Risteski, Andrej",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1028",
    doi = "10.1162/tacl_a_00106",
    pages = "385--399",
}


% Ait-Saada, Mira 2023 - Is Anisotropy Truly Harmful? A Case Study on Text Clustering
@inproceedings{ait2023anisotropy,
    title = "Is Anisotropy Truly Harmful? A Case Study on Text Clustering",
    author = "Ait-Saada, Mira  and
      Nadif, Mohamed",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.103",
    doi = "10.18653/v1/2023.acl-short.103",
    pages = "1194--1203",
}

% Godey 2024 - Anisotropy Is Inherent to Self-Attention in Transformers
@article{godey2024anisotropy,
  title={Anisotropy Is Inherent to Self-Attention in Transformers},
  author={Godey, Nathan and de la Clergerie, {\'E}ric and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:2401.12143},
  year={2024}
}

% Ethayarajh, Kawin 2019 - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
@inproceedings{ethayarajh2019contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
}

% Cai, Xingyu 2020 - Isotropy in the contextual embedding space: Clusters and manifolds
@inproceedings{cai2020isotropy,
  title={Isotropy in the contextual embedding space: Clusters and manifolds},
  author={Cai, Xingyu and Huang, Jiaji and Bian, Yuchen and Church, Kenneth},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

% Puccetti, Giovanni 2022 - Outlier Dimensions that Disrupt Transformers are Driven by Frequency
@inproceedings{puccetti2022outlier,
  title={Outlier Dimensions that Disrupt Transformers are Driven by Frequency},
  author={Puccetti, Giovanni and Rogers, Anna and Drozd, Aleksandr and Dell’Orletta, Felice},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={1286--1304},
  year={2022}
}

% Kovaleva, Olga 2021 - BERT Busters: Outlier Dimensions that Disrupt Transformers
@inproceedings{kovaleva2021bert,
  title={BERT Busters: Outlier Dimensions that Disrupt Transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={3392--3405},
  year={2021}
}

% Rajaee, Sara 2022 - Findings of the Association for Computational Linguistics: ACL 2022
@inproceedings{rajaee2022isotropy,
    title = "An Isotropy Analysis in the Multilingual {BERT} Embedding Space",
    author = "Rajaee, Sara  and
      Pilehvar, Mohammad Taher",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.103",
    doi = "10.18653/v1/2022.findings-acl.103",
    pages = "1309--1316",
}

% Su, Jianlin 2021 - Whitening sentence representations for better semantics and faster retrieval
@article{su2021whitening,
  title={Whitening sentence representations for better semantics and faster retrieval},
  author={Su, Jianlin and Cao, Jiarun and Liu, Weijie and Ou, Yangyiwen},
  journal={arXiv preprint arXiv:2103.15316},
  year={2021}
}

% Ding, Yue 2022 - On Isotropy Calibration of Transformer Models
@inproceedings{ding2022isotropy,
  title={On Isotropy Calibration of Transformer Models},
  author={Ding, Yue and Martinkus, Karolis and Pascual, Damian and Clematide, Simon and Wattenhofer, Roger},
  booktitle={Proceedings of the Third Workshop on Insights from Negative Results in NLP},
  pages={1--9},
  year={2022}
}

% Zhilin Yang 2018 - International Conference on Learning Representations
@inproceedings{yang2018breaking,
    title={Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
    author={Zhilin Yang and Zihang Dai and Ruslan Salakhutdinov and William W. Cohen},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=HkwZSG-CZ},
}

% Jun Gao 2019 - Representation Degeneration Problem in Training Natural Language Generation Models
@inproceedings{
    gao2018representation,
    title={Representation Degeneration Problem in Training Natural Language Generation Models},
    author={Jun Gao and Di He and Xu Tan and Tao Qin and Liwei Wang and Tieyan Liu},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=SkEYojRqtm},
}

% Daniel Bis 2021 - Too much in common: Shifting of embeddings in transformer language models and its implications
@inproceedings{bis2021too,
  title={Too much in common: Shifting of embeddings in transformer language models and its implications},
  author={Bi{\'s}, Daniel and Podkorytov, Maksim and Liu, Xiuwen},
  booktitle={Proceedings of the 2021 conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5117--5130},
  year={2021}
}

% Wang, Lingxiao 2019 - Improving neural language generation with spectrum control
@inproceedings{wang2019improving,
  title={Improving neural language generation with spectrum control},
  author={Wang, Lingxiao and Huang, Jing and Huang, Kevin and Hu, Ziniu and Wang, Guangtao and Gu, Quanquan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

% Li, Bohan 2020 - On the Sentence Embeddings from Pre-trained Language Models
@inproceedings{li2020sentence,
  title={On the Sentence Embeddings from Pre-trained Language Models},
  author={Li, Bohan and Zhou, Hao and He, Junxian and Wang, Mingxuan and Yang, Yiming and Li, Lei},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9119--9130},
  year={2020}
}

% === Human Language learning  ===

% BabyLM 1 Proceedings
@inproceedings{warstadt2023findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

% BabyLM 2 Proceedings
@proceedings{conll-2024-babylm,
    title = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.0/"
}

% Linzen 2021 - PAID frameowrk 
@inproceedings{linzen-2020-accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.465/",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
}

% Alex Warstadt 2022 - What artificial neural networks can tell us about human language acquisition
@incollection{warstadt2022what,
    author = {Alex Warstadt and Samuel Bowman},
    title = {What artificial neural networks can tell us about human language acquisition},
    year = {2022},
    editor = {Shalom Lappin and Jean-Philippe Bernardy},
    booktitle = {Algebraic Structures in Natural Language},
    publisher = {CRC Press}
}

% Saxe, Andrew 2021 - If deep learning is the answer, what is the question?
@article{saxe2021if,
    title={If deep learning is the answer, what is the question?},
    author={Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher},
    journal={Nature Reviews Neuroscience},
    volume={22},
    number={1},
    pages={55--67},
    year={2021},
    publisher={Nature Publishing Group UK London}
}


% --- Biology ---

% 86 billion neurons 
@article{azevedo2009neurons,
  title={Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain},
  author={Azevedo, Frederico AC and Carvalho, Ludmila RB and Grinberg, Lea T and Farfel, Jos{\'e} Marcelo and Ferretti, Renata EL and Leite, Renata EP and Filho, Wilson Jacob and Lent, Roberto and Herculano-Houzel, Suzana},
  journal={Journal of Comparative Neurology},
  volume={513},
  number={5},
  pages={532--541},
  year={2009},
  publisher={Wiley Online Library}
}


% --- Psycholinguistics ---

% Goldilocks principle of task attention
@article{kidd2012goldilocks,
    doi = {10.1371/journal.pone.0036399},
    author = {Kidd, Celeste AND Piantadosi, Steven T. AND Aslin, Richard N.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The {Goldilocks Effect}: Human Infants Allocate Attention to Visual Sequences That Are Neither Too Simple Nor Too Complex},
    year = {2012},
    month = {05},
    volume = {7},
    url = {https://doi.org/10.1371/journal.pone.0036399},
    pages = {1-8},
    number = {5},
}

@article{bergelson2015early,
  title={Early word comprehension in infants: Replication and extension},
  author={Bergelson, Elika and Swingley, Daniel},
  journal={Language Learning and Development},
  volume={11},
  number={4},
  pages={369--380},
  year={2015},
  publisher={Taylor \& Francis}
}

% Clark, Eve V 2015 - First language acquisition
@inbook{clark2015first,
    title={First language acquisition},
    author={Clark, Eve V and Casillas, Marisa},
    booktitle={The Routledge handbook of linguistics},
    pages={167--167},
    year={2015},
    publisher={Routledge}
}

% Alishahi, Afra 2010 - Computational modeling of human language acquisition
@book{alishahi2010computational,
    title={Computational modeling of human language acquisition},
    author={Alishahi, Afra},
    year={2010},
    pages={35},
    publisher={Morgan \& Claypool Publishers}
}

% Gleitman, Lila 1990 - The structural sources of verb meanings
@article{gleitman1990structural,
  title={The structural sources of verb meanings},
  author={Gleitman, Lila},
  journal={Language acquisition},
  volume={1},
  number={1},
  pages={3--55},
  year={1990},
  publisher={Taylor \& Francis}
}

% Jill Gilkerson 2017 - Mapping the early language environment using all-day recordings and automated analysis
@article{gilkerson2017mapping,
    author = {Jill Gilkerson and Jeffrey A Richards and Steven F Warren and Judith K Montgomery and Charles R Greenwood and D Kimbrough Oller and John H L Hansen and Terrance D Paul},
    year = {2017},
    title = {Mapping the early language environment using all-day recordings and automated analysis},
    journal = {American Journal of Speech-Language Pathology},
    volume = {26},
    issue = {2},
    pages = {248–265},
    doi = {https://doi.org/10.1044/2016_AJSLP-15-0169}
}

% Weizman, Zehava Oz 2001 - Lexical output as related to children's vocabulary acquisition: Effects of sophisticated exposure and support for meaning.
@article{weizman2001lexical,
  title={Lexical output as related to children's vocabulary acquisition: Effects of sophisticated exposure and support for meaning.},
  author={Weizman, Zehava Oz and Snow, Catherine E},
  journal={Developmental psychology},
  volume={37},
  number={2},
  pages={265},
  year={2001},
  publisher={American Psychological Association}
}

% Unknown 2023 - Evidence of a predictive coding hierarchy in the human brain listening to speech
@article{caucheteux2023evidence,
  title={Evidence of a predictive coding hierarchy in the human brain listening to speech},
  author={Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  journal={Nature human behaviour},
  volume={7},
  number={3},
  pages={430--441},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


% === Ethics/Privacy/Environment ===

% Bender, Emily M. 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency
@inproceedings{bender2021dangers,
    author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
    title = {On the Dangers of Stochastic Parrots: {C}an Language Models Be Too Big?},
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3442188.3445922},
    doi = {10.1145/3442188.3445922},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {610–623},
    numpages = {14},
    location = {Virtual Event, Canada},
    series = {FAccT '21}
}

% Yifan Yao 2024 - A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly
@article{yao2024privacysurvey,
    title = {A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly},
    journal = {High-Confidence Computing},
    volume = {4},
    number = {2},
    pages = {100211},
    year = {2024},
    issn = {2667-2952},
    doi = {https://doi.org/10.1016/j.hcc.2024.100211},
    url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
    author = {Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang},
    keywords = {Large Language Model (LLM), LLM security, LLM privacy, ChatGPT, LLM attacks, LLM vulnerabilities},
}

% Huang, Jie 2022 - Are Large Pre-Trained Language Models Leaking Your Personal Information?
@inproceedings{huang2022large,
    title = "Are Large Pre-Trained Language Models Leaking Your Personal Information?",
    author = "Huang, Jie  and
      Shao, Hanyin  and
      Chang, Kevin Chen-Chuan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.148",
    doi = "10.18653/v1/2022.findings-emnlp.148",
    pages = "2038--2047",
}

@article{schwartz2020greenai,
    author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
    title = {Green {AI}},
    year = {2020},
    issue_date = {December 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {63},
    number = {12},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3381831},
    doi = {10.1145/3381831},
    journal = {Commun. ACM},
    month = {nov},
    pages = {54–63},
    numpages = {10}
}
