% === GENERAL ===

% ---- Software packages ---- 

@misc{hydra,
  author = {Omry Yadan},
  title = {Hydra -- A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year = {2019},
  url = {https://github.com/facebookresearch/hydra}
}

% HuggingFace
@article{huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

% Pytorch
@inproceedings{paszke2017pytorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@Misc{accelerate,
  title = {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author = {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/accelerate}},
  year = {2022}
}

% Lightning Fabric
@misc{lightning-fabric,
  title = {Lightning Fabric},
  author = {{Lightning AI}},
  note = {Version 2.5.1},
  url = {https://lightning.ai/docs/fabric/stable/},
  year = {2025}
}

% Wandb
@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

% Eustace, Sébastien 2018 - Poetry: Python packaging and dependency management made easy
@misc{poetry,
  author       = {Eustace, Sébastien and others},
  title        = {Poetry: Python packaging and dependency management made easy},
  howpublished = {\url{https://python-poetry.org/}},
  year         = {2018},
  note         = {Version 1.0.0},
}

% Myle Ott 2019 - fairseq: A Fast, Extensible Toolkit for Sequence Modeling
@inproceedings{fairseq,
  title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author={Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:91184134}
}

% Transformers 
@inproceedings{transformers,
  title = "Transformers: State-of-the-Art Natural Language Processing",
  author = "Wolf, Thomas  and
    Debut, Lysandre  and
    Sanh, Victor  and
    Chaumond, Julien  and
    Delangue, Clement  and
    Moi, Anthony  and
    Cistac, Pierric  and
    Rault, Tim  and
    Louf, Remi  and
    Funtowicz, Morgan  and
    Davison, Joe  and
    Shleifer, Sam  and
    von Platen, Patrick  and
    Ma, Clara  and
    Jernite, Yacine  and
    Plu, Julien  and
    Xu, Canwen  and
    Le Scao, Teven  and
    Gugger, Sylvain  and
    Drame, Mariama  and
    Lhoest, Quentin  and
    Rush, Alexander",
  editor = "Liu, Qun  and
    Schlangen, David",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
  month = "oct",
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.emnlp-demos.6/",
  doi = "10.18653/v1/2020.emnlp-demos.6",
  pages = "38--45",
}


% NLTK
@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

% ---- Benchmarks ----

% GLUE 
@article{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={EMNLP 2018},
  pages={353},
  year={2018}
}


% - GLUE Tasks 

% MNLI 
@inproceedings{williams2018mnli,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1112--1122},
  year={2018}
}

% SST 
@inproceedings{socher2013sst,
  title={Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1631--1642},
  year={2013}
}

% CoLA 
@article{warstadt2019cola,
  title={Neural Network Acceptability Judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019}
}

% sts-b 
@inproceedings{cer2017stsb,
  title={SemEval-2017 Task 1: Semantic Textual Similarity—Multilingual and Cross-lingual Focused Evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  booktitle={Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
  pages={1--14},
  year={2017}
}

% MRPC 
@inproceedings{dolan2005mrpc,
  title={Automatically Constructing a Corpus of Sentential Paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP@IJCNLP)},
  year={2005}
}

% RTE 
@inproceedings{dagan2006rte,
  title={The PASCAL Recognising Textual Entailment Challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  pages={177--190},
  year={2006},
  organization={Springer}
}

% Winograd Scheme 
@inproceedings{levesque2011winograd,
  title={The Winograd Schema Challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
  pages={552--557},
  year={2011}
}


% - GLUE Tasks


% Super GLUE
@article{wang2019superglue,
  title={{SuperGLUE}: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


% - Super GLUE Tasks 
% ReCoRD
@inproceedings{zhang2018record,
  title={ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},
  author={Zhang, Shuailong and Sheng, Haoyu and Dong, Xingxing and Xu, Xu and Feng, Yansong and Lapata, Mirella and Zhao, Dongyan},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2174--2184},
  year={2018}
}

% WiC
@inproceedings{pilehvar2019wic,
  title={WiC: The Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1267--1273},
  year={2019}
}

% MultiRC
@inproceedings{khashabi2018multirc,
  title={Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Dan and Upadhyay, Shyam},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={252--257},
  year={2018}
}

% COPA
@inproceedings{roemmele2011copa,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={Proceedings of the 2011 AAAI Spring Symposium Series: Logical Formalizations of Commonsense Reasoning},
  year={2011}
}

% - Super GLUE Tasks 

% SQuAD 1.0
@inproceedings{rajpurkar2016squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264/",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392"
}

% SQuAD 2.0
@inproceedings{rajpurkar2018squad2,
  title={Know What You Don't Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={784--789},
  year={2018}
}



% Zellers, Rowan 2019 - HellaSwag: Can a Machine Really Finish Your Sentence?
@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

% MMLU
@inproceedings{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

% Aarohi Srivastava 2023 - Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
@article{srivastava2023bigbench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and others},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
}

% BigBenchHard
@inproceedings{suzgun2023bigbenchhard,
  title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
  author = {Suzgun, Mirac  and
    Scales, Nathan  and
    Sch{\"a}rli, Nathanael  and
    Gehrmann, Sebastian  and
    Tay, Yi  and
    Chung, Hyung Won  and
    Chowdhery, Aakanksha  and
    Le, Quoc  and
    Chi, Ed  and
    Zhou, Denny  and
    Wei, Jason},
  editor = "Rogers, Anna  and
    Boyd-Graber, Jordan  and
    Okazaki, Naoaki",
  booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.findings-acl.824",
  doi = "10.18653/v1/2023.findings-acl.824",
  pages = "13003--13051",
}

% ARC
@inproceedings{clark2018arc,
  title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4144--4153},
  year={2018}
}

% Cobbe, Karl 2021 - GSM8K
@article{cobbe2021gsm8k,
    title={Training Verifiers to Solve Math Word Problems},
    author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
      journal={arXiv preprint arXiv:2110.14168},
      year={2021}
}

% TruthfulQA
@inproceedings{lin2022truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Askell, Amanda},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={3214--3235},
  year={2022}
}

% GAIA
@inproceedings{efrat2023gaia,
  title={GAIA: A Benchmark for General AI Assistants},
  author={Efrat, Avia and Jiang, Sheng and Wallace, Eric and Lin, Stephanie and Kiela, Douwe and Le, Phu Mon and Khashabi, Daniel},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023}
}

% - Intrinsic Evaluation Tasks - 

% BLIMP
@article{warstadt2020blimp,
  title={{BLiMP}: The benchmark of linguistic minimal pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={377--392},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


% Magnusson, Ian 2024 - Paloma: A benchmark for evaluating language model fit
@article{magnusson2024paloma,
  title={Paloma: A benchmark for evaluating language model fit},
  author={Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and Schwenk, Dustin and Walsh, Evan and Elazar, Yanai and Lo, Kyle and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={64338--64376},
  year={2024}
}


% Warstadt 2020 - MSGS
@inproceedings{warstadt2020msgs,
    title = "Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually)",
    author = "Warstadt, Alex  and
      Zhang, Yian  and
      Li, Xiaocheng  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.16/",
    doi = "10.18653/v1/2020.emnlp-main.16",
    pages = "217--235",
}

% ---- NGram Models ----

% Jurafsky, Daniel and Martin, James H. 2025 - Speech and Language Processing: An Introduction to
% Introduces NGram models
@Book{jurafsky2025speech,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2025",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released January 12, 2025",
  edition =         "3rd",
  }

% Katz, Slava 2003 - Estimation of probabilities from sparse data for the language model component of a speech recognizer
@article{katz2003estimation,
  title={Estimation of probabilities from sparse data for the language model component of a speech recognizer},
  author={Katz, Slava},
  journal={IEEE transactions on acoustics, speech, and signal processing},
  volume={35},
  number={3},
  pages={400--401},
  year={2003},
  publisher={IEEE}
}

% Kneser, Reinhard and Ney, Hermann 1995 - Improved backing-off for m-gram language modeling

@inproceedings{kneser1995improved,
  title={Improved backing-off for m-gram language modeling},
  author={Kneser, Reinhard and Ney, Hermann},
  booktitle={1995 international conference on acoustics, speech, and signal processing},
  volume={1},
  pages={181--184},
  year={1995},
  organization={IEEE}
}

An Empirical Study of Smoothing Techniques for Language Modeling
@article{chen1999empirical,
  title={An empirical study of smoothing techniques for language modeling},
  author={Chen, Stanley F and Goodman, Joshua},
  journal={Computer Speech \& Language},
  volume={13},
  number={4},
  pages={359--394},
  year={1999},
  publisher={Elsevier}
}

% ---- Tokenization  ----

% Gage, Philip 1994 - A new algorithm for data compression
@article{gage1994bpe,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={McPherson, KS: R \& D Publications, c1987-1994.}
}

% Rico Sennrich - BPE 
@inproceedings{sennrich2016bpe,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162/",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725"
}

% Wu, Yonghui 2016 - Google's neural machine translation system: Bridging the gap between human and machine translation
% Introduces the WordPiece tokenization algorithm -- adopted in BERT (crurical for handling rare words + transformers)
@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

% Kudo, Taku 2018 - Sentencepiece 
@inproceedings{kudo2018unigram,
    title = "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
    author = "Kudo, Taku",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1007",
    doi = "10.18653/v1/P18-1007",
    pages = "66--75",
}

% Kudo, Taku 2018 - Sentencepiece
% Introduces the SentencePiece tokenization algorithm 
@inproceedings{kudo2018sentencepiece,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={66--71},
  year={2018}
}

% Tokenization Free Approaches 

% Clark, Jonathan H 2022 - Canine: Pre-training an efficient tokenization-free encoder for language representation
@article{clark2022canine,
  title={Canine: Pre-training an efficient tokenization-free encoder for language representation},
  author={Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={73--91},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

% Xue, Linting 2022 - Byt5: Towards a token-free future with pre-trained byte-to-byte models
@article{xue2022byt5,
  title={Byt5: Towards a token-free future with pre-trained byte-to-byte models},
  author={Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={291--306},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


% --- Optimization Algorithms --- 

% Rumelhart, David E 1986 - Learning representations by back-propagating errors (OG Backprop paper)
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  doi={10.1038/323533a0}
}

% Robbins, Herbert 1951 - A Stochastic Approximation Method
@article{robbins1951stochastic,
  title={A Stochastic Approximation Method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  volume={22},
  number={3},
  pages={400--407},
  year={1951},
  publisher={Institute of Mathematical Statistics}
}

% Duchi, John 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
@inproceedings{duchi2011adaptive,
  title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  booktitle={Proceedings of the 24th International Conference on Neural Information Processing Systems},
  pages={257--265},
  year={2011}
}

% Tieleman, Tijmen 2012 - RMSProp: Divide the Gradient by a Running Average of Its Recent Magnitude
@misc{tieleman2012lecture,
  title={Lecture 6.5 - RMSProp: Divide the Gradient by a Running Average of Its Recent Magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  note={COURSERA: Neural Networks for Machine Learning},
  year={2012},
  howpublished={\url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}
}

% Sutskever, Ilya 2013 - On the Importance of Initialization and Momentum in Deep Learning
@inproceedings{sutskever2013importance,
  title={On the Importance of Initialization and Momentum in Deep Learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={Proceedings of the 30th International Conference on Machine Learning},
  volume={28},
  pages={1139--1147},
  year={2013}
}

% Kingma, Diederik P 2015 - Adam: A Method for Stochastic Optimization
@article{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

% Ilya Loshchilov 2019 - Decoupled Weight Decay Regularization (AdamW)
@article{loshchilov2019decoupled,
  title={Decoupled Weight Decay Regularization}, 
  author={Ilya Loshchilov and Frank Hutter},
  year={2019},
  journal = {arXiv preprint arXiv:1711.05101}
}

% --- Language/Semantics --- 

% Bengio, Yoshua 2003 - A neural probabilistic language model
% Defines the language modeling task
@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}

% Jozefowicz, Rafal 2016 - Exploring the limits of language modeling
% Formarlizes the next-token prediction task
# Shows that architecture and training data affect PPL
@article{jozefowicz2016exploring,
  title={Exploring the limits of language modeling},
  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal={arXiv preprint arXiv:1602.02410},
  year={2016}
}

% Zipf's law 
@book{manning2009introduction,
  title={An introduction to information retrieval},
  author={Manning, Christopher D},
  year={2009},
  publisher={Cambridge university press}
}

@book{zipf1935zipflaw,
  author = "George K. Zipf",
  title = "The Psychobiology of Language",
  publisher = "Boston: Houghton-Mifflin",
  year = {1935}
}

@inproceedings{petrov2012universalpos,
  title = "A Universal Part-of-Speech Tagset",
  author = "Petrov, Slav  and
    Das, Dipanjan  and
    McDonald, Ryan",
  editor = "Calzolari, Nicoletta  and
    Choukri, Khalid  and
    Declerck, Thierry  and
    Do{\u{g}}an, Mehmet U{\u{g}}ur  and
    Maegaard, Bente  and
    Mariani, Joseph  and
    Moreno, Asuncion  and
    Odijk, Jan  and
    Piperidis, Stelios",
  booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}`12)",
  month = may,
  year = "2012",
  address = "Istanbul, Turkey",
  publisher = "European Language Resources Association (ELRA)",
  url = "https://aclanthology.org/L12-1115/",
  pages = "2089--2096",
}

% Jelinek, Frederick 1977 - Perplexity -- a measure of the difficulty of speech recognition tasks
@inproceedings{jelinek1977perplexity,
  author    = {Frederick Jelinek and Robert L. Mercer and Lalit R. Bahl and James K. Baker},
  title     = {Perplexity -- a measure of the difficulty of speech recognition tasks},
  booktitle = {Proceedings of the Workshop on Pattern Recognition in Practice},
  year      = {1977},
  address   = {Amsterdam, The Netherlands},
  publisher = {North-Holland},
  note      = {Reprinted in various later NLP collections},
}

% Salazar, Julian 2020 - Masked Language Model Scoring (Pseudo-Perplexity )
@inproceedings{salazar2020masked,
  title = "Masked Language Model Scoring",
  author = "Salazar, Julian  and
    Liang, Davis  and
    Nguyen, Toan Q.  and
    Kirchhoff, Katrin",
  editor = "Jurafsky, Dan  and
    Chai, Joyce  and
    Schluter, Natalie  and
    Tetreault, Joel",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.240",
  doi = "10.18653/v1/2020.acl-main.240",
  pages = "2699--2712",
}

% Meister, Clara 2021 - Language Model Evaluation Beyond Perplexity
@inproceedings{meister2021perplexity,
  title     = {Language Model Evaluation Beyond Perplexity},
  author    = {Meister, Clara and Cotterell, Ryan},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages     = {5328--5339},
  year      = {2021},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-long.414/},
  doi       = {10.18653/v1/2021.acl-long.414}
}


% Alfred V. Aho 1972 - The Theory of Parsing, Translation and Compiling
@book{aho1972parsing,
  author  = {Alfred V. Aho and Jeffrey D. Ullman},
  title   = {The Theory of Parsing, Translation and Compiling},
  year    = "1972",
  volume  = "1",
  publisher = {Prentice-Hall},
  address = {Englewood Cliffs, NJ}
}

% Dan Gusfield 1997 - Algorithms on Strings, Trees and Sequences
@book{gusfield1997algorithms,
  author  = {Dan Gusfield},
  title   = {Algorithms on Strings, Trees and Sequences},
  year    = "1997",
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

% --- Datasets ---

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{zhu2015aligning,
  title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages={19--27},
  year={2015},
  url={https://arxiv.org/abs/1506.06724}
}


% Leo Gao 2020 - The Pile: An 800GB Dataset of Diverse Text for Language Modeling
@misc{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
  author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
  year={2020},
  journal={arXiv preprint arXiv:2101.00027}
}

@misc{together2023redpajama,
  title={RedPajama: An Open Dataset for Training Large Language Models},
  author={Together AI},
  year={2023},
  howpublished={\url{https://arxiv.org/abs/2411.12372}}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023},
  url={https://arxiv.org/abs/2306.01116}
}

% Soldaini, Luca 2024 - Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research
@inproceedings{soldaini2024dolma,
  title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15725--15788},
  year={2024}
}

@article{penedo2024fineweb,
  title={FineWeb: Decanting the Web for the Finest Text Data at Scale},
  author={Penedo, Guilherme and Kydlíček, Hynek and Ben Allal, Loubna and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas},
  journal={arXiv preprint arXiv:2406.17557},
  year={2024},
  url={https://arxiv.org/abs/2406.17557}
}

% ---- Historical Models ----

% Bengio, Yoshua 2003 - A neural probabilistic language model
@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}

% Mikolov, Tomas 2010 - Recurrent neural network based language model
@inproceedings{mikolov2010recurrent,
  title     = {Recurrent neural network based language model},
  author    = {Tomáš Mikolov and Martin Karafiát and Lukáš Burget and Jan Černocký and Sanjeev Khudanpur},
  year      = {2010},
  booktitle = {Interspeech 2010},
  pages     = {1045--1048},
  doi       = {10.21437/Interspeech.2010-343},
  issn      = {2958-1796},
}

% Bengio, Yoshua 1994 - Learning long-term dependencies with gradient descent is difficult
@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

% Hochreiter, Sepp 1997 - Long short-term memory
@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

% Cho, Kyunghyun 2014 - Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation
@inproceedings{cho-etal-2014-learning,
  title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
  author = {Cho, Kyunghyun  and
    van Merri{\"e}nboer, Bart  and
    Gulcehre, Caglar  and
    Bahdanau, Dzmitry  and
    Bougares, Fethi  and
    Schwenk, Holger  and
    Bengio, Yoshua},
  editor = "Moschitti, Alessandro  and
    Pang, Bo  and
    Daelemans, Walter",
  booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
  month = oct,
  year = "2014",
  address = "Doha, Qatar",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D14-1179/",
  doi = "10.3115/v1/D14-1179",
    pages = "1724--1734"
}

% Sutskever, Ilya 2014 - Sequence to sequence learning with neural networks
@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

% Peters, Matthew E. 2018 - Deep Contextualized Word Representations (ELMo)
@inproceedings{peters-etal-2018-deep,
  title = "Deep Contextualized Word Representations",
  author = "Peters, Matthew E.  and
    Neumann, Mark  and
    Iyyer, Mohit  and
    Gardner, Matt  and
    Clark, Christopher  and
    Lee, Kenton  and
    Zettlemoyer, Luke",
  editor = "Walker, Marilyn  and
    Ji, Heng  and
    Stent, Amanda",
  booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
  month = jun,
  year = "2018",
  address = "New Orleans, Louisiana",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N18-1202/",
  doi = "10.18653/v1/N18-1202",
  pages = "2227--2237",
}

% Howard, Jeremy 2018 - Universal Language Model Fine-tuning for Text Classification
@inproceedings{howard-ruder-2018-universal,
  title = "Universal Language Model Fine-tuning for Text Classification",
  author = "Howard, Jeremy  and
    Ruder, Sebastian",
  editor = "Gurevych, Iryna  and
    Miyao, Yusuke",
  booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2018",
  address = "Melbourne, Australia",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P18-1031/",
  doi = "10.18653/v1/P18-1031",
  pages = "328--339",
}

% --- Transformer Models ---

% Bahdanau, Dzmitry 2015 - Neural machine translation by jointly learning to align and translate
@inproceedings{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}

% Vaswani, Ashish 2017 - Attention is all you need
@inproceedings{vaswani2017attention,
  title = {Attention Is All You Need},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

% Devlin, Jacob 2019 - Bert: Pre-training of deep bidirectional transformers for language understanding
@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

% ---- BERT Variants ----

% Liu, Yinhan 2019 - RoBERTa: A Robustly Optimized BERT Pretraining Approach
@article{liu2019roberta,
    title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach}, 
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year = {2019},
    volume = {arXiv:1907.11692},
    journal = {arXiv preprint arXiv:1907.11692},
    url = {https://arxiv.org/abs/1907.11692},
}

% Lan, Zhenzhong 2019 - ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
@inproceedings{lan2019albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

% Joshi, Mandar 2020 - SpanBERT: Improving Pre-training by Representing and Predicting Spans
@article{joshi2020spanbert,
  title={SpanBERT: Improving Pre-training by Representing and Predicting Spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press}
}

% Sanh, Victor 2019 - DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={Advances in Neural Information Processing Systems Workshop},
  year={2019}
}

@inproceedings{he2021deberta,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}


% ---- GPT Variants ----

% Radford, Alec 2018 - Improving Language Understanding by Generative Pre-Training
@misc{radford2018gpt1,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  note={Technical report, OpenAI},
  howpublished={\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}}
}

% Radford, Alec 2019 - Language Models are Unsupervised Multitask Learners
@misc{radford2019gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  note={Technical report, OpenAI},
  howpublished={\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}}
}

% Brown, Tom 2020 - Language Models are Few-Shot Learners
@inproceedings{brown2020gpt3,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% ---- Transformer Combination Models ----

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  pages={2978--2988},
  url={https://aclanthology.org/P19-1285},
}

@misc{keskar2019ctrl,
  title={CTRL: A Conditional Transformer Language Model for Controllable Generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  year={2019},
  eprint={1909.05858},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1909.05858}
}

@inproceedings{clark2020electra,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
  url={https://arxiv.org/abs/2003.10555}
}

@article{raffel2020t5,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  url={http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{lewis2020bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020},
  url={https://aclanthology.org/2020.acl-main.703}
}

@inproceedings{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019},
  url={https://arxiv.org/abs/1906.08237}
}

% ---- Model Architectures/Design ---- 

% - Layer Normalization - 

% Ioffe, Sergey 2015 - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
@inproceedings{ioffe2015batchnorm,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML)},
  pages={448--456},
  year={2015},
  url={https://proceedings.mlr.press/v37/ioffe15.html}
}

% Ba, Jimmy Lei 2016 - Layer Normalization
@misc{ba2016layernorm,
  title={Layer Normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  year={2016},
  eprint={1607.06450},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/1607.06450}
}

% RMSNorm
@misc{zhang2019rmsnorm,
  title={Root Mean Square Layer Normalization},
  author={Zhang, Yuxuan and Sennrich, Rico},
  year={2019},
  eprint={1910.07467},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1910.07467}
}

% Xiong, Ruibin 2020 - On Layer Normalization in the Transformer Architecture
@misc{xiong2020layer,
  title={On Layer Normalization in the Transformer Architecture},
  author={Xiong, Ruibin and Yang, Yi and He, Di and Zheng, Kai and Zheng, Shuo and Xing, Chang and Zhang, Yanyan and Lan, Yingce and Wang, Liwei and Liu, Tie-Yan},
  year={2020},
  eprint={2002.04745},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2002.04745}
}


% - Positional Encodings - 

@inproceedings{shaw2018self,
  title={Self-Attention with Relative Position Representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={464--468},
  year={2018},
  url={https://aclanthology.org/N18-2074}
}


@misc{press2021train,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Press, Ofir and Smith, Noah A. and Levy, Omer},
  year={2021},
  eprint={2108.12409},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2108.12409}
}


% ROPE Embeddings 
@article{su2024rope,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

% - Activation Functions - 

% LeCun, Yann 1998 - Efficient backprop (tanh and sigmoid)
@incollection{lecun1998efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural Networks: Tricks of the Trade},
  pages={9--50},
  publisher={Springer},
  year={1998},
  doi={10.1007/3-540-49430-8_2}
}


% Glorot, Xavier 2010 - Understanding the difficulty of training deep feedforward neural networks
@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={249--256},
  year={2010},
  url={http://proceedings.mlr.press/v9/glorot10a.html}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML)},
  pages={807--814},
  year={2010},
  url={http://www.cs.toronto.edu/~hinton/absps/relus.pdf}
}

@inproceedings{maas2013rectifier,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
  booktitle={Proc. ICML},
  year={2013},
  note={Workshop on Deep Learning for Audio, Speech and Language Processing},
  url={https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf}
}
@misc{hendrycks2016gaussian,
  title={Gaussian Error Linear Units (GELUs)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  year={2016},
  eprint={1606.08415},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1606.08415}
}

@misc{ramachandran2017searching,
  title={Searching for Activation Functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  year={2017},
  eprint={1710.05941},
  archivePrefix={arXiv},
  primaryClass={cs.NE},
  url={https://arxiv.org/abs/1710.05941}
}

% Shazeer, Noam 2020 - Glu variants improve transformer
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

% - Regularization - 

% Dropout
@article{srivastava2014dropout,
  title={Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={Journal of Machine Learning Research},
  volume={15},
  number={56},
  pages={1929--1958},
  year={2014},
  url={http://jmlr.org/papers/v15/srivastava14a.html}
}

% Pascanu, Razvan 2013 - On the Difficulty of Training Recurrent Neural Networks (introduce vanishing/exploding gradients)
@inproceedings{pascanu2013difficulty,
  title={On the Difficulty of Training Recurrent Neural Networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={Proceedings of the 30th International Conference on Machine Learning (ICML)},
  pages={1310--1318},
  year={2013},
  url={https://proceedings.mlr.press/v28/pascanu13.html}
}

% - Modern Transformer Architectures - 

% He, Kaiming 2016 - Deep Residual Learning for Image Recognition - Residual (Skip) Connections
@inproceedings{he2016deep,
  title={Deep Residual Learning for Image Recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={770--778},
  year={2016}
}

% Ainslie, Joshua 2023 - GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
@inproceedings{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023}
}

% --- Architecture Search ---

@inproceedings{zoph2017nejural,
  title={Neural Architecture Search with Reinforcement Learning},
  author={Zoph, Barret and Le, Quoc V},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@inproceedings{chen2021autoformer,
  title={AutoFormer: Searching Transformers for Visual Recognition},
  author={Chen, Minghao and Peng, Houwen and Fu, Jianlong and Ling, Haibin},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={12270--12280},
  year={2021}
}

@article{xu2021nasbert,
  title={NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search},
  author={Xu, Jin and Tan, Xu and Luo, Renqian and Song, Kaitao and Li, Jian and Qin, Tao and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2105.14444},
  year={2021}

}

% --- Large Language Models ---

% Zhao, Wayne Xin 2023 - A survey of large language models
@article{zhao2023llmsurvey,
    title={A survey of large language models},
    author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
    url = {https://arxiv.org/abs/2303.18223},
    year = {2023},
    journal = {arXiv preprint arXiv:2303.18223}
}

@misc{openai2023gpt4,
  title = {GPT-4 Technical Report},
  author = {OpenAI and Brockman, Greg and Bubnov, Sandhini Agarwal and Chowdhery, Aditi and Dohan, David and Mann, Jacob and Murati, Mira and Ndousse, Jan and Power, Rewon Child and Radford, Alec and Sutskever, Ilya and Zoph, Barret},
  year = {2023},
  howpublished = {\url{https://arxiv.org/abs/2303.08774}},
  note = {arXiv:2303.08774 [cs.CL]}
}


% Touvron, Hugo 2023 - LLaMA: Open and Efficient Foundation Language Models
@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aur{\'e}lien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  howpublished={arXiv preprint arXiv:2302.13971},
  year={2023}
}

% Touvron, Hugo 2023 - Llama 2: Open foundation and fine-tuned chat models
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{jiang2023mistral,
  title = {Mistral 7B},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Le Scao, Teven and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and El Sayed, William},
  year = {2023},
  howpublished = {\url{https://arxiv.org/abs/2310.06825}},
  note = {arXiv:2310.06825 [cs.CL]}
}

% Anthropic - Claude 3 Model Family
@misc{anthropic2024claude3,
  title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author = {{Anthropic}},
  year = {2024},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-family}},
  note = {Accessed: 2025-05-22}
}

@misc{anthropic2024claude35,
  title = {Claude 3.5 Sonnet},
  author = {{Anthropic}},
  year = {2024},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-5-sonnet}},
  note = {Accessed: 2025-05-22}
}

@misc{anthropic2025claude37,
  title = {Claude 3.7 Sonnet},
  author = {{Anthropic}},
  year = {2025},
  howpublished = {\url{https://www.anthropic.com/claude}},
  note = {Accessed: 2025-05-22}
}

% Gopher - Rae, Jack W. 2021 - Scaling Language Models: Methods, Analysis & Insights from Training (Gopher)
@article{rae2021scaling,
  title={Scaling Language Models: Methods, Analysis & Insights from Training Gopher},
  author={Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and de Masson d'Autume, Cyprien and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@inproceedings{hoffman2022chinchilla,
    title={Training Compute-Optimal Large Language Models}, 
    author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
    year={2022},
    booktitle = {Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS)}
}

@article{deepmind2023gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={DeepMind},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023},
  url={https://arxiv.org/abs/2312.11805}
}

@misc{baidu2023ernie4,
  title={ERNIE 4.0: Baidu's Next-Generation Foundation Model},
  author={Baidu},
  year={2023},
  howpublished={\url{https://research.baidu.com/Blog/index-view?id=183}},
  note={Accessed: 2025-05-22}
}

@misc{alibaba2023qwen,
  title={Qwen: Open-Source Chinese-Centric Language Models},
  author={Alibaba Cloud},
  year={2023},
  howpublished={\url{https://github.com/QwenLM/Qwen}},
  note={Accessed: 2025-05-22}
}

% Le Scao, Teven (Big Science) 2023 - Bloom: A 176b-parameter open-access multilingual language model
@article{le2023bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal = {arXiv preprint arXiv:2211.05100},
  year={2023},
}

% Chowdhery, Aakanksha 2023 - Palm: Scaling language modeling with pathways
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

% Zhang, Susan 2022 - Opt: Open pre-trained transformer language models
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

% Deepseek - Scaling Open-Source Language Models with Longtermism
@misc{deepseek2024llm,
  title = {DeepSeek LLM: Scaling Open-Source Language Models with Longtermism},
  author = {DeepSeek-AI and Xiao Bi and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du and Zhe Fu and Huazuo Gao and Kaige Gao and Wenjun Gao and Ruiqi Ge and Kang Guan and Daya Guo and Jianzhong Guo and Guangbo Hao and Zhewen Hao and Ying He and Wenjie Hu and Panpan Huang and Erhang Li and Guowei Li and Jiashi Li and Yao Li and Y. K. Li and Wenfeng Liang and Fangyun Lin and A. X. Liu and Bo Liu and Wen Liu and Xiaodong Liu and Xin Liu and Yiyuan Liu and Haoyu Lu and Shanghao Lu and Fuli Luo and Shirong Ma and Xiaotao Nie and Tian Pei and Yishi Piao and Junjie Qiu and Hui Qu and Tongzheng Ren and Zehui Ren and Chong Ruan and Zhangli Sha and Zhihong Shao and Junxiao Song and Xuecheng Su and Jingxiang Sun and Yaofeng Sun and Minghui Tang and Bingxuan Wang and Peiyi Wang and Shiyu Wang and Yaohui Wang and Yongji Wang and Tong Wu and Y. Wu and Xin Xie and Zhenda Xie and Ziwei Xie and Yiliang Xiong and Hanwei Xu and R. X. Xu and Yanhong Xu and Dejian Yang and Yuxiang You and Shuiping Yu and Xingkai Yu and B. Zhang and Haowei Zhang and Lecong Zhang and Liyue Zhang and Mingchuan Zhang and Minghua Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Qihao Zhu and Yuheng Zou},
  year = {2024},
  howpublished = {\url{https://arxiv.org/abs/2401.02954}},
  note = {arXiv:2401.02954 [cs.CL]}
}

% --- Post-training/ Models --- 

% RAG / Retrieval models 
@article{lewis2020retrieval,
  title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  journal={arXiv preprint arXiv:2005.11401},
  year={2020}
}

% Cohere 2024 - command r+ model
@misc{cohere2024commandrplus,
  title        = {c4ai-command-r-plus-08-2024},
  author       = {{Cohere Labs}},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/CohereLabs/c4ai-command-r-plus-08-2024}},
  note         = {Accessed: 2025-05-22}
}

% Instruction Tuned / Preference Tuning 

% RLHF 
@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  url={https://arxiv.org/abs/1706.03741}
}

% PPO 
@article{schulman2017proximal,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017},
  url={https://arxiv.org/abs/1707.06347}
}


% DPO 
@inproceedings{rafailov2023direct,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
  url={https://arxiv.org/abs/2305.18290}
}


% Jason Wei 2021 - FLAN
@article{wei2021flan,
  author = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  title        = {Finetuned Language Models Are Zero-Shot Learners},
  year         = {2021},
  volume    = {arXiv:2109.01652},
  url          = {https://arxiv.org/abs/2109.01652},
  journal={ arXiv preprint arXiv:2109.01652},
}


% Ouyang, Long 2022 - Training language models to follow instructions with human feedback
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022},
  url={https://arxiv.org/abs/2203.02155}
}


@misc{huggingface2023zephyr,
  title        = {Zephyr-7B-β},
  author       = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Clémentine and Habib, Nathan and Sarrazin, Nathan and Sanseviero, Omar and Rush, Alexander M. and Wolf, Thomas},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/HuggingFaceH4/zephyr-7b-beta}},
  note         = {Accessed: 2025-05-22}
}


% Deepseek r1
@article{guo2025deepseekr1,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

% Tuning Approaches 


% --- Non-Transformer Models ---

% Gu, Albert 2020 - HiPPO: Recurrent Memory with Optimal Polynomial Projections
@inproceedings{gu2020hippo,
  title={HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14744--14756},
  year={2020}
}

% Gu, Albert 2021 - Efficiently Modeling Long Sequences with Structured State Spaces
@inproceedings{gu2021efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

% Gu, Albert 2023 - Mamba: Linear-Time Sequence Modeling with Selective State Spaces
@article{gu2023mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{peng-etal-2023-rwkv,
  title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
  author = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Derczynski, Leon and Du, Xingjian and Grella, Matteo and GV, Kranthi Kiran and He, Xuzheng and Hou, Haowen and Kazienko, Przemyslaw and Kocon, Jan and Kong, Jiaming and Koptyra, Bart{\l}omiej and Lau, Hayden and Lin, Jiaju and Mantri, Krishna Sri Ipsit and Mom, Ferdinand and Saito, Atsushi and Song, Guangyu and Tang, Xiangru and Wind, Johan and Wo{\'z}niak, Stanis{\l}aw and Zhang, Zhenyuan and Zhou, Qinghua and Zhu, Jian and Zhu, Rui-Jie},
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.findings-emnlp.936/",
  doi = "10.18653/v1/2023.findings-emnlp.936",
  pages = "14048--14077"
}


% === Analysis  ===

% --- Analysis Frameworks --- 

% Biderman, Stella 2023 - Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling
@inproceedings{biderman2023pythia,
  title     = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author    = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, Usvsn Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {2397--2430},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--29 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v202/biderman23a/biderman23a.pdf},
  url       = {https://proceedings.mlr.press/v202/biderman23a.html}
}

% Groeneveld, Dirk 2024 - OLMo: Accelerating the Science of Language Models
@inproceedings{groeneveld2024olmo,
  title     = {OLMo: Accelerating the Science of Language Models},
  author    = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {15052--15077},
  year      = {2024},
  url       = {https://aclanthology.org/2024.acl-long.841/},
  pdf       = {https://aclanthology.org/2024.acl-long.841.pdf}
}


@software{pico2025,
    author = {Diehl Martinez, Richard},
    title = {Pico: A Lightweight Framework for Studying Language Model Learning Dynamics},
    year = {2025},
    url = {https://github.com/pico-lm}
}


% --- Emergent Abilities --- 

% Wei, Jason 2022 - Emergent abilities of large language models
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{wu2024u,
  title={U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models},
  author={Wu, Tony Z. and Yao, Shinn and Duerig, Tom and Roberts, Adam and Pham, Hieu and Raffel, Colin and Wei, Jason and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.09605},
  year={2024}
}

@article{schaeffer2023mirage,
  title={Are Emergent Abilities of Large Language Models a Mirage?},
  author={Schaeffer, Rowan and Mirchandani, Akul and Tworkowski, Max and Ganguli, Deep and Irizarry, Julian and Ju, Wenlong and Krueger, David and Hestness, Joel and Tamkin, Aaron and Arora, Simran and others},
  journal={arXiv preprint arXiv:2304.15004},
  year={2023}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

% --- Scaling Law ---

% Henighan, Tom 2020 - Scaling Laws for Autoregressive Generative Modeling
@article{henighan2020scaling,
  title={Scaling Laws for Autoregressive Generative Modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B. and Dhariwal, Prafulla and Gray, Scott and Hallacy, Chris and Mann, Benjamin and Radford, Alec and Ramesh, Aditya and Ryder, Nick and Ziegler, Daniel M. and Schulman, John and Amodei, Dario and McCandlish, Sam},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020},
  url={https://arxiv.org/abs/2010.14701}
}

% Hernandez, Danny 2021 - Scaling Laws for Transfer
@article{hernandez2021scaling,
  title={Scaling Laws for Transfer},
  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021},
  url={https://arxiv.org/abs/2102.01293}
}

% Nikhil Sardana 2024 - Forty-first International Conference on Machine Learning
@inproceedings{sardana2024beyond,
    title={Beyond Chinchilla-Optimal: {A}ccounting for Inference in Language Model Scaling Laws},
    author={Nikhil Sardana and Jacob Portes and Sasha Doubov and Jonathan Frankle},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=0bmXrtTDUu}
}

% Utkarsh Sharma 2022 - Scaling Laws from the Data Manifold Dimension
@article{sharma2022scalinglaws,
    title={Scaling Laws from the Data Manifold Dimension},
    author={Utkarsh Sharma and Jared Kaplan},
    year={2022},
    journal={Journal of Machine Learning Research},
    volume={23},
    number={9},
    pages={1--34},
    url={http://jmlr.org/papers/v23/20-1111.html}
}

% Hoffmann, Jordan 2022 - Training compute-optimal large language models
@article{hoffmann2022training,
    title        = {Training compute-optimal large language models},
    author       = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
    year         = 2022,
    journal      = {arXiv preprint 2203.15556}
}

% Kaplan, Jared 2020 - Scaling laws for neural language models
@article{kaplan2020scaling,
    title        = {Scaling laws for neural language models},
    author       = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
    year         = 2020,
    journal      = {arXiv preprint 2001.08361}
}

% --- Generalization ---

% Belkin, Mikhail 2019 - Reconciling modern machine-learning practice and the classical bias--variance trade-off
@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

% Yilmaz, Fatih Furkan 2022 - Regularization-wise double descent: Why it occurs and how to eliminate it
@article{yilmaz2022regularization,
  title={Regularization-wise double descent: Why it occurs and how to eliminate it},
  author={Yilmaz, Fatih Furkan and Heckel, Reinhard},
  journal={arXiv preprint arXiv:2206.01378},
  year={2022}
}

% Hinton, Geoffrey 2015 - Distilling the knowledge in a neural network
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

% Phang, Jason 2021 - Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers
@inproceedings{phang2021finetuned,
  title = "Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers",
  author = "Phang, Jason  and
    Liu, Haokun  and
    Bowman, Samuel R.",
  editor = "Bastings, Jasmijn  and
    Belinkov, Yonatan  and
    Dupoux, Emmanuel  and
    Giulianelli, Mario  and
    Hupkes, Dieuwke  and
    Pinter, Yuval  and
    Sajjad, Hassan",
  booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
  month = nov,
  year = "2021",
  address = "Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.blackboxnlp-1.42",
  doi = "10.18653/v1/2021.blackboxnlp-1.42",
  pages = "529--538",
}

% Du, Li 2023 - A Measure-Theoretic Characterization of Tight Language Models
@inproceedings{du2023measure,
  title = "A Measure-Theoretic Characterization of Tight Language Models",
  author = "Du, Li  and
    Torroba Hennigen, Lucas  and
    Pimentel, Tiago  and
    Meister, Clara  and
    Eisner, Jason  and
    Cotterell, Ryan",
  editor = "Rogers, Anna  and
    Boyd-Graber, Jordan  and
    Okazaki, Naoaki",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.543",
  doi = "10.18653/v1/2023.acl-long.543",
  pages = "9744--9770",
}

% Ryan Cotterell 2024 - Formal Aspects of Language Modeling
@article{cotterell-etal-2024-formal,
  title={Formal Aspects of Language Modeling}, 
  author={Ryan Cotterell and Anej Svete and Clara Meister and Tianyu Liu and Li Du},
  year={2024},
  url          = {https://arxiv.org/abs/2311.04329},
  journal      = {arXiv preprint 2311.04329}
}


% --- Influence Functions/Data Importance --- 

% Koh, Pang Wei 2017 - Understanding black-box predictions via influence functions
@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

% Cook, R Dennis 1980 - Characterizations of an empirical influence function for detecting influential cases in regression
@article{cook1980characterizations,
  title={Characterizations of an empirical influence function for detecting influential cases in regression},
  author={Cook, R Dennis and Weisberg, Sanford},
  journal={Technometrics},
  volume={22},
  number={4},
  pages={495--508},
  year={1980},
  publisher={Taylor \& Francis}
}

% Filighera, Anna 2019 - Automatic text difficulty estimation using embeddings and neural networks
@inproceedings{filighera2019automatic,
  title={Automatic text difficulty estimation using embeddings and neural networks},
  author={Filighera, Anna and Steuer, Tim and Rensing, Christoph},
  booktitle={Transforming Learning with Meaningful Technologies: 14th European Conference on Technology Enhanced Learning, EC-TEL 2019, Delft, The Netherlands, September 16--19, 2019, Proceedings 14},
  pages={335--348},
  year={2019},
  organization={Springer}
}

% Grosse, Roger 2023 - Studying Large Language Model Generalization with Influence Functions
@article{grosse2023influence,
  title = {Studying Large Language Model Generalization with Influence Functions},
  author = {Grosse, Roger and Krueger, David and Oberman, Adam and Wang, Longshaokan and Pan, Xuezhou and Romero, Julio and Schuurmans, Dale and Swersky, Kevin},
  journal = {arXiv preprint arXiv:2305.12035},
  year = {2023},
  url = {https://arxiv.org/abs/2305.12035}
}

% --- Memorization ---

% Lu, Xingyu 2024 - Scaling Laws for Fact Memorization of Large Language Models
@inproceedings{lu2024scaling,
  title     = {Scaling Laws for Fact Memorization of Large Language Models},
  author    = {Lu, Xingyu and Li, Xiaonan and Cheng, Qinyuan and Ding, Kai and Huang, Xuanjing and Qiu, Xipeng},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages     = {845--859},
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-emnlp.658/}
}

% Biderman, Stella 2023 - Emergent and Predictable Memorization in Large Language Models
@article{biderman2023emergent,
  title   = {Emergent and Predictable Memorization in Large Language Models},
  author  = {Biderman, Stella and Prashanth, USVSN Sai and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
  journal = {arXiv preprint arXiv:2304.11158},
  year    = {2023},
  url     = {https://arxiv.org/abs/2304.11158}
}

% Kiyomaru, Hirokazu 2024 - A Comprehensive Analysis of Memorization in Large Language Models
@inproceedings{kiyomaru2024comprehensive,
  title     = {A Comprehensive Analysis of Memorization in Large Language Models},
  author    = {Kiyomaru, Hirokazu and Sugiura, Issa and Kawahara, Daisuke and Kurohashi, Sadao},
  booktitle = {Proceedings of the 17th International Natural Language Generation Conference},
  pages     = {584--596},
  year      = {2024},
  address   = {Tokyo, Japan},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.inlg-main.45/}
}

% Haviv, Adi 2023 - Understanding Transformer Memorization Recall Through Idioms
@inproceedings{haviv2023understanding,
  title = "Understanding Transformer Memorization Recall Through Idioms",
  author = "Haviv, Adi  and
    Cohen, Ido  and
    Gidron, Jacob  and
    Schuster, Roei  and
    Goldberg, Yoav  and
    Geva, Mor",
  editor = "Vlachos, Andreas  and
    Augenstein, Isabelle",
  booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
  month = may,
  year = "2023",
  address = "Dubrovnik, Croatia",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.eacl-main.19",
  doi = "10.18653/v1/2023.eacl-main.19",
  pages = "248--264",
}

% Feldman, Vitaly 2020 - What neural networks memorize and why: Discovering the long tail via influence estimation
@article{feldman2020neural,
  title={What neural networks memorize and why: Discovering the long tail via influence estimation},
  author={Feldman, Vitaly and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2881--2891},
  year={2020}
}

% Feldman, Vitaly 2020 - Does learning require memorization? a short tale about a long tail
@inproceedings{feldman2020does,
  title={Does learning require memorization? a short tale about a long tail},
  author={Feldman, Vitaly},
  booktitle={Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={954--959},
  year={2020}
}

@inproceedings{zheng2022memorization,
  title={An Empirical Study of Memorization in NLP},
  author={Xiaosen Zheng and Jing Jiang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247618876}
}

% Lesci, Pietro 2024 - Causal Estimation of Memorisation Profiles
@inproceedings{lesci2024causal,
  title = "Causal Estimation of Memorisation Profiles",
  author = "Lesci, Pietro  and
    Meister, Clara  and
    Hofmann, Thomas  and
    Vlachos, Andreas  and
    Pimentel, Tiago",
  editor = "Ku, Lun-Wei  and
    Martins, Andre  and
    Srikumar, Vivek",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = aug,
  year = "2024",
  address = "Bangkok, Thailand",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.acl-long.834",
  doi = "10.18653/v1/2024.acl-long.834",
  pages = "15616--15635",
}

% Carlini, Nicholas 2019 - The Secret Sharer: Measuring Unintended Memorization in Neural Networks
@inproceedings{carlini2019secret,
  title={The Secret Sharer: Measuring Unintended Memorization in Neural Networks},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Raffel, Colin and others},
  booktitle={28th USENIX Security Symposium (USENIX Security 19)},
  pages={267--284},
  year={2019},
  organization={USENIX Association}
}

% Carlini, Nicholas 2021 - Extracting training data from large language models
@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

% Carlini, Nicholas 2022 - Quantifying Memorization Across Neural Language Models
@inproceedings{carlini2022quantifying,
  title={Quantifying Memorization Across Neural Language Models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

% Tirumala, Kushal 2022 - Memorization without overfitting: Analyzing the training dynamics of large language models
@article{tirumala2022memorization,
	title        = {Memorization without overfitting: Analyzing the training dynamics of large language models},
	author       = {Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
	year         = 2022,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {38274--38290}
}

% --- Differential Privacy 

% Dwork, Cynthia 2006 - Calibrating Noise to Sensitivity in Private Data Analysis
@inproceedings{dwork2006calibrating,
  author    = {Cynthia Dwork and Frank McSherry and Kobbi Nissim and Adam Smith},
  title     = {Calibrating Noise to Sensitivity in Private Data Analysis},
  booktitle = {Theory of Cryptography Conference (TCC)},
  series    = {Lecture Notes in Computer Science},
  volume    = {3876},
  pages     = {265--284},
  publisher = {Springer},
  year      = {2006},
  doi       = {10.1007/11681878_14},
  url       = {https://iacr.org/archive/tcc2006/38760266/38760266.pdf}
}

% McMahan, H. Brendan 2017 - Communication-Efficient Learning of Deep Networks from Decentralized Data
@inproceedings{mcmahan2017communication,
  title     = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author    = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Agüera y Arcas, Blaise},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  series    = {Proceedings of Machine Learning Research},
  volume    = {54},
  pages     = {1273--1282},
  publisher = {PMLR},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v54/mcmahan17a.html}
}

% --- Behavior Probing ---

% Wes Gurnee 2023 - Finding Neurons in a Haystack: Case Studies with Sparse Probing
@article{
    gurnee2023finding,
    title={Finding Neurons in a Haystack: Case Studies with Sparse Probing},
    author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=JYs1R9IMJr},
    note={}
}

% Clark, Kevin 2019 - Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
@inproceedings{clark2019does,
    title = "What Does {BERT} Look at? An Analysis of {BERT}`s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Belinkov, Yonatan  and
      Hupkes, Dieuwke",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828/",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
}


% Syntax trees are encoded in the geometry of BERT 
@inproceedings{hewitt2019structural,
    title = "{A} Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1419",
    doi = "10.18653/v1/N19-1419",
    pages = "4129--4138",
}


% Singh, Jasdeep 2019 - Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)
@inproceedings{singh2019bert,
    title = "{BERT} is Not an Interlingua and the Bias of Tokenization",
    author = "Singh, Jasdeep  and
      McCann, Bryan  and
      Socher, Richard  and
      Xiong, Caiming",
    editor = "Cherry, Colin  and
      Durrett, Greg  and
      Foster, George  and
      Haffari, Reza  and
      Khadivi, Shahram  and
      Peng, Nanyun  and
      Ren, Xiang  and
      Swayamdipta, Swabha",
    booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6106",
    doi = "10.18653/v1/D19-6106",
    pages = "47--55",
}

% --- Interpretability  --- 

% Olah, Chris 2014 - Neural Networks, Manifolds, and Topology
@misc{olah2014manifolds,
  author = {Chris Olah},
  title = {Neural Networks, Manifolds, and Topology},
  year = {2014},
  howpublished = {\url{https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/}},
  note = {Accessed: 2025-05-26}
}

% Elhage, Nelson 2021 - A Mathematical Framework for Transformer Circuits
@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

% Elhage, Nelson 2022 - Toy Models of Superposition
@article{elhage2022toy,
  title = {Toy Models of Superposition},
  author = {Elhage, Nelson and Nanda, Neel and Olah, Chris and Amodei, Dario},
  year = {2022},
  journal = {Transformer Circuits Thread, Anthropic},
  howpublished = {\url{https://transformer-circuits.pub/2022/toy_model/index.html}},
  note = {Accessed: 2025-05-26}
}

% Bircken, Ulrich 2023 - Towards Monosemanticity: Decomposing Language Models with Dictionary Learning
@article{bircken2023monosemanticity,
  title = {Towards Monosemanticity: Decomposing Language Models with Dictionary Learning},
  author = {Bircken, Ulrich and Bergal, Alex and Gurnee, Kyle and Elhage, Nelson and Nanda, Neel and Olah, Chris and Amodei, Dario},
  year = {2023},
  journal = {Anthropic},
  howpublished = {\url{https://transformer-circuits.pub/2023/monosemanticity/index.html}},
  note = {Accessed: 2025-05-26}
}

% Anthropic 2023 - Decomposing Language Models Into Understandable Components
@misc{anthropic2023components,
  title = {Decomposing Language Models Into Understandable Components},
  author = {{Anthropic}},
  year = {2023},
  howpublished = {\url{https://www.anthropic.com/index/decomposing-language-models}},
  note = {Accessed: 2025-05-26}
}

% Nora Belrose 2023 - Eliciting Latent Predictions from Transformers with the Tuned Lens
@misc{belrose2023eliciting,
  title={Eliciting Latent Predictions from Transformers with the Tuned Lens}, 
  author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
  year={2023},
  eprint={2303.08112},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

% Voita, Elena 2019 - Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned
@inproceedings{voita2019analyzing,
  title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
  author = "Voita, Elena  and
    Talbot, David  and
    Moiseev, Fedor  and
    Sennrich, Rico  and
    Titov, Ivan",
  editor = "Korhonen, Anna  and
    Traum, David  and
    M{\`a}rquez, Llu{\'i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1580/",
  doi = "10.18653/v1/P19-1580",
  pages = "5797--5808",
}

% Michel, Paul 2019 - Advances in Neural Information Processing Systems
@inproceedings{michel2019sixteen,
     author = {Michel, Paul and Levy, Omer and Neubig, Graham},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
     pages = {},
     publisher = {Curran Associates, Inc.},
     title = {Are Sixteen Heads Really Better than One?},
     url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
     volume = {32},
     year = {2019}
}

% Olah, Chris 2020 - Zoom in: An introduction to circuits
@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}


% Neel N 2022 - TransformerLens
@misc{nanda2022transformerlens,
  title = {TransformerLens},
  author = {Neel Nanda and Joseph Bloom},
  year = {2022},
  howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

% Meng, Kevin 2022 - Locating and editing factual associations in GPT
@inproceedings{meng2022locating,
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  title = {Locating and editing factual associations in GPT},
  year = {2022},
  isbn = {9781713871088},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
  articleno = {1262},
  numpages = {14},
  location = {New Orleans, LA, USA},
  series = {NIPS '22}
}

% Anthropic - Towards automated circuit discovery for mechanistic interpretability
@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}

% van Wingerden, Stan 2024 - DevInterp
@misc{devinterpcode,
  title = {DevInterp},
  author = {van Wingerden, Stan and Hoogland, Jesse and Wang, George and Zhou, William},
  year = {2024},
  howpublished = {\url{https://github.com/timaeus-research/devinterp}},
}

% Hoogland, Jesse 2023 - Towards Developmental Interpretability
@misc{hoogland2023towards,
	title = {Towards {Developmental} {Interpretability}},
	url = {https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability},
	abstract = {Developmental interpretability is a research agenda that has grown out of a meeting of the Singular Learning Theory (SLT) and AI alignment communitie…},
	language = {en},
	urldate = {2025-03-18},
	author = {Hoogland, Jesse and Oldenziel, Alexander Gietelink and Murfet, Daniel and Wingerden, Stan van},
	month = jul,
	year = {2023},
}

% Javier Ferr 2024 - A Primer on the Inner Workings of Transformer-based Language Models
@article{ferrando2024primer,
  title={A Primer on the Inner Workings of Transformer-based Language Models}, 
  author={Javier Ferrando and Gabriele Sarti and Arianna Bisazza and Marta R. Costa-jussà},
  year={2024},
  journal      = {arXiv preprint 2405.00208},
  url = {https://arxiv.org/abs/2405.00208}
}

% Fabien Roger 2023 - Large Language Models Sometimes Generate Purely Negatively-Reinforced Text
@misc{roger2023negativelyreinforced,
  title={Large Language Models Sometimes Generate Purely Negatively-Reinforced Text}, 
  author={Fabien Roger},
  year={2023},
  eprint={2306.07567},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2306.07567}, 
}

% --- Learning Dynamics ---

% Diehl Martinez, Richard 2024 - Tending Towards Stability: Convergence Challenges in Small Language Models
@inproceedings{diehlmartinez2024tending,
  title = "Tending Towards Stability: Convergence Challenges in Small Language Models",
  author = "Diehl Martinez, Richard  and
    Lesci, Pietro  and
    Buttery, Paula",
  editor = "Al-Onaizan, Yaser  and
    Bansal, Mohit  and
    Chen, Yun-Nung",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
  month = nov,
  year = "2024",
  address = "Miami, Florida, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.findings-emnlp.187/",
  doi = "10.18653/v1/2024.findings-emnlp.187",
  pages = "3275--3286",
}

@inproceedings{chai2024training,
  title = "On Training Data Influence of {GPT} Models",
  author = "Chai, Yekun  and
    Liu, Qingyi  and
    Wang, Shuohuan  and
    Sun, Yu  and
    Peng, Qiwei  and
    Wu, Hua",
  editor = "Al-Onaizan, Yaser  and
    Bansal, Mohit  and
    Chen, Yun-Nung",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2024",
  address = "Miami, Florida, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.emnlp-main.183/",
  doi = "10.18653/v1/2024.emnlp-main.183",
  pages = "3126--3150",
}

@article{godey2024small,
  title        = {Why do small language models underperform? {S}tudying Language Model Saturation via the Softmax Bottleneck},
  author       = {Godey, Nathan and de la Clergerie, {\'E}ric and Sagot, Beno{\^\i}t},
  year         = 2024,
  journal      = {arXiv preprint 2404.07647},
  url = {https://arxiv.org/abs/2404.07647}
}

@inproceedings{nguyen2020wide,
  title={Do Wide and Deep Networks Learn the Same Things? {U}ncovering How Neural Network Representations Vary with Width and Depth},
  author={Thao Nguyen and Maithra Raghu and Simon Kornblith},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=KJNcAkY8tY4}
}

@inproceedings{phang2021fine,
  title = "Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers",
  author = "Phang, Jason  and
    Liu, Haokun  and
    Bowman, Samuel R.",
  editor = "Bastings, Jasmijn  and
    Belinkov, Yonatan  and
    Dupoux, Emmanuel  and
    Giulianelli, Mario  and
    Hupkes, Dieuwke  and
    Pinter, Yuval  and
    Sajjad, Hassan",
  booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
  month = nov,
  year = "2021",
  address = "Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.blackboxnlp-1.42",
  doi = "10.18653/v1/2021.blackboxnlp-1.42",
  pages = "529--538",
}

% SVCCA ELMO learns similar representation to that of POS tagger
@inproceedings{saphra2019understanding,
  title = "Understanding Learning Dynamics Of Language Models with {SVCCA}",
  author = "Saphra, Naomi  and
    Lopez, Adam",
  editor = "Burstein, Jill  and
    Doran, Christy  and
    Solorio, Thamar",
  booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
  month = jun,
  year = "2019",
  address = "Minneapolis, Minnesota",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N19-1329",
  doi = "10.18653/v1/N19-1329",
  pages = "3257--3267",
}

% Belrose, Nora 2024 - Neural Networks Learn Statistics of Increasing Complexity
@article{belrose2024neural,
	title        = {Neural Networks Learn Statistics of Increasing Complexity},
	author       = {Belrose, Nora and Pope, Quintin and Quirke, Lucia and Mallen, Alex and Fern, Xiaoli},
	year         = 2024,
	journal      = {arXiv preprint 2402.04362},
url = {https://arxiv.org/abs/2402.04362}
}

% Enric Boix-Adsera 2023 - Transformers Learn Through Gradual Rank Increase
@inproceedings{boix-adsera2023rank,
	title        = {Transformers Learn Through Gradual Rank Increase},
	author       = {Enric Boix-Adsera and Etai Littwin and Emmanuel Abbe and Samy Bengio and Joshua Susskind},
	year         = 2023,
	booktitle    = {NeurIPS},
	url          = {https://arxiv.org/abs/2306.07042}
}

% Michaelov, James 2023 - Emergent Inabilities? Inverse Scaling Over the Course of Pretraining
@inproceedings{michaelov2023emergent,
  title = "Emergent Inabilities? Inverse Scaling Over the Course of Pretraining",
  author = "Michaelov, James  and
    Bergen, Ben",
  editor = "Bouamor, Houda  and
    Pino, Juan  and
    Bali, Kalika",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.findings-emnlp.973/",
  doi = "10.18653/v1/2023.findings-emnlp.973",
  pages = "14607--14615",
}

% Jesse Hoogl 2025 - Loss Landscape Degeneracy Drives Stagewise Development in Transformers
@misc{hoogland2025losslandscape,
  title={Loss Landscape Degeneracy Drives Stagewise Development in Transformers}, 
  author={Jesse Hoogland and George Wang and Matthew Farrugia-Roberts and Liam Carroll and Susan Wei and Daniel Murfet},
  year={2025},
  eprint={2402.02364},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2402.02364}, 
}

% --- Representation Similarity/ Metrics ---

% Kornblith, Simon 2019 - Similarity of neural network representations revisited
@inproceedings{kornblith2019cka,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

% Morcos, Ari 2018 - Insights on representational similarity in neural networks with canonical correlation
@article{morcos2018pwcca,
  title={Insights on representational similarity in neural networks with canonical correlation},
  author={Morcos, Ari and Raghu, Maithra and Bengio, Samy},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

% Raghu, Maithra 2017 - Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability
@article{raghu2017svcca,
  title={Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
  author={Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% Bansal, Yamini 2021 - Revisiting model stitching to compare neural representations
@article{bansal2021revisiting,
	title        = {Revisiting model stitching to compare neural representations},
	author       = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
	year         = 2021,
	journal      = {Advances in neural information processing systems},
	volume       = 34,
	pages        = {225--236}
}

% Lenc, Karel 2015 - Understanding image representations by measuring their equivariance and equivalence
@inproceedings{lenc2015understanding,
	title        = {Understanding image representations by measuring their equivariance and equivalence},
	author       = {Lenc, Karel and Vedaldi, Andrea},
	year         = 2015,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {991--999}
}

% Wu, John 2020 - Similarity Analysis of Contextual Word Representation Models
@inproceedings{wu2020similarity,
  title = "Similarity Analysis of Contextual Word Representation Models",
  author = "Wu, John  and
    Belinkov, Yonatan  and
    Sajjad, Hassan  and
    Durrani, Nadir  and
    Dalvi, Fahim  and
    Glass, James",
  editor = "Jurafsky, Dan  and
    Chai, Joyce  and
    Schluter, Natalie  and
    Tetreault, Joel",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.422",
  doi = "10.18653/v1/2020.acl-main.422",
  pages = "4638--4655",
}

% Brown, Davis 2023 - Understanding the Inner-workings of Language Models Through Representation Dissimilarity
@inproceedings{brown2023understanding,
  title = "Understanding the Inner-workings of Language Models Through Representation Dissimilarity",
  author = "Brown, Davis  and
    Godfrey, Charles  and
    Konz, Nicholas  and
    Tu, Jonathan  and
    Kvinge, Henry",
  editor = "Bouamor, Houda  and
    Pino, Juan  and
    Bali, Kalika",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.emnlp-main.403",
  doi = "10.18653/v1/2023.emnlp-main.403",
  pages = "6543--6558",
}

% --- Analysis Metrics ---- %

% Hoyer, Patrik O 2004 - Non-negative matrix factorization with sparseness constraints
@article{hoyer2004sparsity,
  title={Non-negative matrix factorization with sparseness constraints},
  author={Hoyer, Patrik O},
  journal={Journal of machine learning research},
  volume={5},
  number={Nov},
  pages={1457--1469},
  year={2004}
}

% Hurley, Niall 2009 - Comparing measures of sparsity
@article{hurley2009gini,
  title={Comparing measures of sparsity},
  author={Hurley, Niall and Rickard, Scott},
  journal={IEEE Transactions on Information Theory},
  volume={55},
  number={10},
  pages={4723--4741},
  year={2009},
  publisher={IEEE}
}

% Roy, Olivier 2007 - 15th European Signal Processing Conference
@inproceedings{roy2007effectiverank,
  author={Roy, Olivier and Vetterli, Martin},
  booktitle={15th European Signal Processing Conference}, 
  title={The effective rank: {A} measure of effective dimensionality}, 
  year={2007},
  pages={606-610},
  url={https://www.eurasip.org/Proceedings/Eusipco/Eusipco2007/Papers/a5p-h05.pdf}
}

% === Small Language Models === 

% --- Small Language Model Frameworks  --- 

% Wu 2024 - LaMini Model 
@inproceedings{wu2024lamini,
  title={LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},
  author={Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and Abdul-Mageed, Muhammad and Aji, Alham},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={944--964},
  year={2024}
}

% Zhang, Peiyuan 2024 - Tinyllama: An open-source small language model
@article{zhang2024tinyllama,
    title={Tinyllama: An open-source small language model},
    author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
    journal={arXiv preprint arXiv:2401.02385},
    year={2024}
}

% Liu et al 2024 - mobilellm 
@inproceedings{liu2024mobilellm,
    title={Mobilellm: Optimizing sub-billion parameter language models for on-device use cases},
    author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
}

% Thawakar, Omkar 2024 - MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT
@article{thawakar2024mobillama,
  title={MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT},
  author={Thawakar, Omkar and Vayani, Ashmal and Khan, Salman and Cholakkal, Hisham and Anwer, Rao Muhammad and Felsberg, Michael and Baldwin, Timothy and Xing, Eric P. and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2402.16840},
  year={2024},
  url={https://arxiv.org/abs/2402.16840}
}

% Abdin - Phi-3 model 
@article{abdin2024phi,
    title={Phi-3 technical report: A highly capable language model locally on your phone},
    author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
    journal={arXiv preprint arXiv:2404.14219},
    year={2024}
}

% Mehta el al. Apple OpenELM 
@article{mehta2024openelm,
  title={Openelm: An efficient language model family with open training and inference framework},
  author={Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and others},
  journal={arXiv preprint arXiv:2404.14619},
  year={2024}
}


% Loubna Ben Allal 2025 - SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model
@article{allal2025smollm2,
  title={SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model},
  author={Allal, Loubna Ben and Lozhkov, Anton and Bakouch, Elie and Bl{\'a}zquez, Gabriel Mart{\'\i}n and Penedo, Guilherme and Tunstall, Lewis and Marafioti, Andr{\'e}s and Kydl{\'\i}{\v{c}}ek, Hynek and Lajar{\'\i}n, Agust{\'\i}n Piqueres and Srivastav, Vaibhav and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}


% small open-source version of large frameworks/democratization perspective 

% Conover, Mike 2023 - Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM
@misc{databricksdolly2023,
    author = {Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
    title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    shorttitle = {Free Dolly},
    journal = {Databricks Blog},
    year = {2023},
    month = {April},
    url = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate = {2025-03-12},
    language = {en},
}

@misc{mosaic2023mpt,
    title = {Introducing {MPT}-{7B}: {A} {New} {Standard} for {Open}-{Source}, {Commercially} {Usable} {LLMs}},
    shorttitle = {Introducing {MPT}-{7B}},
    url = {https://www.databricks.com/blog/mpt-7b},
    abstract = {Introducing MPT-7B, the first entry in our MosaicML Foundation Series. MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. MPT-7B was trained on the MosaicML platform in 9.5 days with zero human intervention at a cost of {\textasciitilde}\$200k.},
    language = {en-US},
    urldate = {2025-03-12},
    journal = {Databricks},
    author = {{MosaicML NLP Team}},
    month = may,
    year = {2023},
}

% --- Benefits of Small Language Models --- 

@article{bommasani2021foundation,
  title={On the Opportunities and Risks of Foundation Models},
  author={Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dorottya and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021},
  url={https://arxiv.org/abs/2108.07258}
}

% --- Democractization 

% Cottier, Ben 2024 - The Rising Costs of Training Frontier AI Models
@article{cottier2024rising,
  title   = {The Rising Costs of Training Frontier AI Models},
  author  = {Cottier, Ben and Rahman, Robi and Fattorini, Loredana and Maslej, Nestor and Besiroglu, Tamay and Owen, David},
  journal = {arXiv preprint arXiv:2405.21015},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.21015}
}

% Sharir, Or 2020 - The Cost of Training NLP Models: A Concise Overview
@article{sharir2020cost,
  title   = {The Cost of Training NLP Models: A Concise Overview},
  author  = {Sharir, Or and Peleg, Barak and Shoham, Yoav},
  journal = {arXiv preprint arXiv:2004.08900},
  year    = {2020},
  url     = {https://arxiv.org/abs/2004.08900}
}

% Karpathy, Andrej 2023 - nanoGPT
@misc{karpathy2023nanogpt,
    title = {nanoGPT},
    author = {Karpathy, Andrej},
    year = {2023},
    url = {https://github.com/karpathy/nanoGPT},
}


% -- Environmental 

% Bender, Emily M. 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency
@inproceedings{bender2021dangers,
    author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
    title = {On the Dangers of Stochastic Parrots: {C}an Language Models Be Too Big?},
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3442188.3445922},
    doi = {10.1145/3442188.3445922},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {610–623},
    numpages = {14},
    location = {Virtual Event, Canada},
    series = {FAccT '21}
}

% Schwartz, Roy 2020 - Green AI
@article{schwartz2020greenai,
    author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
    title = {Green {AI}},
    year = {2020},
    issue_date = {December 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {63},
    number = {12},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3381831},
    doi = {10.1145/3381831},
    journal = {Commun. ACM},
    month = {nov},
    pages = {54–63},
    numpages = {10}
}

% Strubell, Emma 2019 - Energy and Policy Considerations for Deep Learning in NLP
@article{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019},
  url={https://arxiv.org/abs/1906.02243}
}

% Lacoste, Alexandre 2019 - Quantifying the Carbon Emissions of Machine Learning
@article{lacoste2019quantifying,
  title={Quantifying the Carbon Emissions of Machine Learning},
  author={Alexandre Lacoste and Alexandra Luccioni and Victor Schmidt and Thomas Dandres},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019},
  url={https://arxiv.org/abs/1910.09700}
}

% Luccioni, Alexandra 2023 - Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning
@article{luccioni2023counting,
  title={Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning},
  author={Alexandra Luccioni and Alexander Hernandez-Garcia},
  journal={arXiv preprint arXiv:2302.08476},
  year={2023},
  url={https://arxiv.org/abs/2302.08476}
}

% Zhou, Xiyou 2021 - HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing
@inproceedings{zhou2021hulk,
  title={HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing},
  author={Xiyou Zhou and Zhiyu Chen and Xiaoyong Jin and William Yang Wang},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations},
  pages={329--336},
  year={2021},
  url={https://aclanthology.org/2021.eacl-demos.39/}
}

% OpenAI 2018 - AI and Compute
@misc{openai2018compute,
  title={AI and Compute},
  author={OpenAI},
  year={2018},
  url={https://openai.com/index/ai-and-compute/}
}


% Patterson, David 2021 - Carbon Emissions and Large Neural Network Training
@article{patterson2021carbon,
  title={Carbon Emissions and Large Neural Network Training},
  author={David Patterson and Joseph Gonzalez and Quoc V. Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David R. So and Maud Texier and Jeff Dean},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021},
  url={https://arxiv.org/abs/2104.10350}
}

% --- Data Privacy/ Memorization --- 

% Yifan Yao 2024 - A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly
@article{yao2024privacysurvey,
    title = {A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly},
    journal = {High-Confidence Computing},
    volume = {4},
    number = {2},
    pages = {100211},
    year = {2024},
    issn = {2667-2952},
    doi = {https://doi.org/10.1016/j.hcc.2024.100211},
    url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
    author = {Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang},
    keywords = {Large Language Model (LLM), LLM security, LLM privacy, ChatGPT, LLM attacks, LLM vulnerabilities},
}

% Huang, Jie 2022 - Are Large Pre-Trained Language Models Leaking Your Personal Information?
@inproceedings{huang2022large,
    title = "Are Large Pre-Trained Language Models Leaking Your Personal Information?",
    author = "Huang, Jie  and
      Shao, Hanyin  and
      Chang, Kevin Chen-Chuan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.148",
    doi = "10.18653/v1/2022.findings-emnlp.148",
    pages = "2038--2047",
}

% Neel, Seth 2023 - Privacy Issues in Large Language Models: A Survey
@article{neel2023privacy,
  author       = {Seth Neel and
                  Peter W. Chang},
  title        = {Privacy Issues in Large Language Models: A Survey},
  journal      = {CoRR},
  volume       = {abs/2312.06717},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.06717},
  doi          = {10.48550/ARXIV.2312.06717},
  eprinttype   = {arXiv},
  eprint       = {2312.06717},
  timestamp    = {Thu, 04 Jan 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2312-06717.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


% --- On-Device/Local LLMs --- 

% Alizadeh, Keivan 2024 - LLM in a Flash: Efficient Large Language Model Inference with Limited Memory
@inproceedings{alizadeh2024llm,
  title     = {LLM in a Flash: Efficient Large Language Model Inference with Limited Memory},
  author    = {Alizadeh, Keivan and Mirzadeh, Seyed Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C.},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {12345--12360},
  year      = {2024},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.acl-long.678/}
}


% Howard, Andrew G. 2017 - MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
@article{howard2017mobilenets,
  title   = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author  = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal = {arXiv preprint arXiv:1704.04861},
  year    = {2017},
  url     = {https://arxiv.org/abs/1704.04861}
}


% Li, Yanyu 2022 - EfficientFormer: Vision Transformers at MobileNet Speed
@inproceedings{li2022efficientformer,
  title     = {EfficientFormer: Vision Transformers at MobileNet Speed},
  author    = {Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Ju and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2022},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5452ad8ee6ea6e7dc41db1cbd31ba0b8-Abstract-Conference.html}
}

% Han, Song 2016 - EIE: Efficient Inference Engine on Compressed Deep Neural Network
@inproceedings{han2016eie,
  title     = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  author    = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
  booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
  pages     = {243--254},
  year      = {2016},
  publisher = {IEEE Press},
  doi       = {10.1145/3007787.3001163}
}

% Malladi, Krishna T. 2012 - Towards Energy-Proportional Datacenter Memory with Mobile DRAM
@inproceedings{malladi2012towards,
  title     = {Towards Energy-Proportional Datacenter Memory with Mobile DRAM},
  author    = {Malladi, Krishna T. and Nothaft, Frank A. and Periyathambi, Karthika and Lee, Benjamin C. and Kozyrakis, Christos and Horowitz, Mark},
  booktitle = {Proceedings of the 39th Annual International Symposium on Computer Architecture},
  pages     = {37--48},
  year      = {2012},
  publisher = {IEEE Computer Society},
  doi       = {10.1145/2366231.2337164}
}

% --- Data Hungry --- 

@misc{usdoj2005act,
  title = {Justice Manual: 9-48.000 - Computer Fraud and Abuse Act},
  author = {{U.S. Department of Justice}},
  year = {2005},
  url = {https://www.justice.gov/jm/jm-9-48000-computer-fraud},
  note = {Accessed: 2025-05-26}
}

@misc{hiqvslinkedin,
  title = {hiQ Labs, Inc. v. LinkedIn Corp., No. 17-16783 (9th Cir. 2019)},
  author = {{United States Court of Appeals for the Ninth Circuit}},
  year = {2019},
  url = {https://cdn.ca9.uscourts.gov/datastore/opinions/2019/09/09/17-16783.pdf},
  note = {Accessed: 2025-05-26}
}

@misc{nytvsopenai,
  title        = {The New York Times Company v. Microsoft Corporation and OpenAI, Inc.},
  author       = {{The New York Times Company}},
  year         = {2023},
  url          = {https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf},
  note         = {Accessed: 2025-05-26}
}

@misc{letalvsalphabet,
  title        = {Letal v. Alphabet Inc. et al., No. 3:23-cv-03440 (N.D. Cal. 2023)},
  author       = {{United States District Court for the Northern District of California}},
  year         = {2023},
  url          = {https://www.bloomberglaw.com/public/desktop/document/LetalvAlphabetIncetalDocketNo323cv03440NDCalJul112023CourtDocket/2},
  note         = {Accessed: 2025-05-26}
}

% Villalobos, Pablo 2022 - Will We Run Out of Data? Limits of LLM Scaling Based on Human-Generated Data
@article{villalobos2022will,
  title   = {Will We Run Out of Data? Limits of LLM Scaling Based on Human-Generated Data},
  author  = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
  journal = {arXiv preprint arXiv:2211.04325},
  year    = {2022},
  url     = {https://arxiv.org/abs/2211.04325}
}

% Reinsel, David 2018 - The Digitization of the World: From Edge to Core
@techreport{reinsel2018digitization,
  title       = {The Digitization of the World: From Edge to Core},
  author      = {Reinsel, David and Gantz, John and Rydning, John},
  institution = {International Data Corporation (IDC)},
  year        = {2018},
  url         = {https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf}
}

% Van den Bosch, Antal 2016 - Estimating Search Engine Index Size Variability: A 9-Year Longitudinal Study
@article{van2016estimating,
  title   = {Estimating Search Engine Index Size Variability: A 9-Year Longitudinal Study},
  author  = {Van den Bosch, Antal and Bogers, Toine and de Kunder, Maurice},
  journal = {Scientometrics},
  volume  = {107},
  number  = {2},
  pages   = {839--856},
  year    = {2016},
  doi     = {10.1007/s11192-016-1863-z},
  url     = {https://link.springer.com/article/10.1007/s11192-016-1863-z}
}


% === Training LMs Efficiently  ===

% Label Smoothing 
@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}


% --- Transfer/Multi-task Learning  ---

% Caruana, Rich 1997 - Multitask learning
@article{caruana1997multitask,
    title={Multitask learning},
    author={Caruana, Rich},
    journal={Machine learning},
    volume={28},
    pages={41--75},
    year={1997},
    publisher={Springer}
}

% Ando, Rie Kubota 2005 - A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data
@article{ando2005predictivetasks,
    Acmid = {1194905},
    Author = {Ando, Rie Kubota and Zhang, Tong},
    Issn = {1532-4435},
    Issue_Date = {12/1/2005},
    Journal = {Journal of Machine Learning Research},
    Month = dec,
    Numpages = {37},
    Pages = {1817--1853},
    Publisher = {JMLR.org},
    Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
    Volume = {6},
    Year = {2005},
    url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

% --- Hyper-parameter tuning --- 

@inproceedings{izsak2021train,
  title = "How to Train {BERT} with an Academic Budget",
  author = "Izsak, Peter  and
    Berchansky, Moshe  and
    Levy, Omer",
  editor = "Moens, Marie-Francine  and
    Huang, Xuanjing  and
    Specia, Lucia  and
    Yih, Scott Wen-tau",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Online and Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.831/",
  doi = "10.18653/v1/2021.emnlp-main.831",
  pages = "10644--10652",
}

% Geiping, Jonas 2023 - International Conference on Machine Learning
@inproceedings{geiping2023cramming,
  title={Cramming: Training a Language Model on a single {GPU} in one day.},
  author={Geiping, Jonas and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={11117--11143},
  year={2023},
  organization={PMLR}
}

% Hillier, Dylan 2024 - Super Tiny Language Models
@article{hillier2024super,
  title   = {Super Tiny Language Models},
  author  = {Dylan Hillier and Leon Guertler and Cheston Tan and Palaash Agrawal and Chen Ruirui and Bobby Cheng},
  journal = {arXiv preprint arXiv:2405.14159},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.14159}
}

% --- Curriculum Learning ---

@inproceedings{diehlmartinez2023climb,
  title = "{CLIMB} {--} Curriculum Learning for Infant-inspired Model Building",
  author = "Diehl Martinez, Richard  and
    Goriely, Z{\'e}bulon  and
    McGovern, Hope  and
    Davis, Christopher  and
    Caines, Andrew  and
    Buttery, Paula  and
    Beinborn, Lisa",
  editor = "Warstadt, Alex  and
    Mueller, Aaron  and
    Choshen, Leshem  and
    Wilcox, Ethan  and
    Zhuang, Chengxu  and
    Ciro, Juan  and
    Mosquera, Rafael  and
    Paranjabe, Bhargavi  and
    Williams, Adina  and
    Linzen, Tal  and
    Cotterell, Ryan",
  booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.conll-babylm.10/",
  doi = "10.18653/v1/2023.conll-babylm.10",
  pages = "112--127"
}

% Jeffrey L. Elman 1993 - Learning and development in neural networks: the importance of starting small
@article{elman1993learning,
  author = {Jeffrey L. Elman},
  title = {Learning and development in neural networks: the importance of starting small},
  journal = {Cognition},
  volume = {48},
  number = {1},
  pages = {71-99},
  year = {1993},
  issn = {0010-0277},
  doi = {https://doi.org/10.1016/0010-0277(93)90058-4},
}

% Soviany, Petru 2022 - Curriculum learning: A survey
@article{soviany2022curriculum,
  title={Curriculum learning: A survey},
  author={Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  journal={International Journal of Computer Vision},
  pages={1--40},
  year={2022},
  publisher={Springer}
}

% Tom Kocmi 2017 - Curriculum Learning and Minibatch Bucketing in Neural Machine Translation
@inproceedings{kocmi2017curriculum,
    title={Curriculum Learning and Minibatch Bucketing in Neural Machine Translation},
  author={Kocmi, Tom and Bojar, Ond{\v{r}}ej},
    booktitle={Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP 2017},
    pages={379--386},
    year={2017}
}

% Yoshua Bengio 2009 - Curriculum learning
@inproceedings{bengio2009curriculum,
    title={Curriculum learning},
    author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
    booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
    pages={41--48},
    year={2009}
}

% Xiaoxia Wu 2021 - When Do Curricula Work?
@inproceedings{wu2021when,
  title={When Do Curricula Work?},
  author={Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=tW4QEInpni}
}

% Unknown 2019 - Competence-based Curriculum Learning for Neural Machine Translation
@inproceedings{platanios2019competence,
  title={Competence-based Curriculum Learning for Neural Machine Translation},
  author={Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Pocz{\'o}s, Barnab{\'a}s and Mitchell, Tom},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1162--1172},
  year={2019}
}

% Yile Wang 2023 - Language Model Pre-training with Linguistically Motivated Curriculum Learning
@misc{wang2023language,
  title={Language Model Pre-training with Linguistically Motivated Curriculum Learning},
  author={Yile Wang and Yue Zhang and Peng Li and Yang Liu},
  year={2023},
  url={https://openreview.net/forum?id=y7CNId2RnV}
}

% Liu, Cao 2018 - Curriculum Learning for Natural Answer Generation
@inproceedings{liu2018curriculum,
  title={Curriculum Learning for Natural Answer Generation},
  author={Liu, Cao and He, Shizhu and Liu, Kang and Zhao, Jun and others},
  booktitle={IJCAI},
  pages={4223--4229},
  year={2018}
}

% Daniel Campos 2021 - Curriculum learning for language modeling
@article{campos2021curriculum,
  title={Curriculum learning for language modeling}, 
  author={Daniel Campos},
  year={2021},
  journal={arXiv preprint arXiv:2108.02170}
}

% Natalie Schluter 2018 - When data permutations are pathological: the case of neural natural language inference
@inproceedings{schluter2018data,
  title = "When data permutations are pathological: the case of neural natural language inference",
  author = "Schluter, Natalie  and
    Varab, Daniel",
  editor = "Riloff, Ellen  and
    Chiang, David  and
    Hockenmaier, Julia  and
    Tsujii, Jun{'}ichi",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  month = oct # "-" # nov,
  year = "2018",
  address = "Brussels, Belgium",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D18-1534/",
  doi = "10.18653/v1/D18-1534",
  pages = "4935--4939",
}

% Sachan, Mrinmaya 2016 - Easy Questions First? A Case Study on Curriculum Learning for Question Answering
@inproceedings{sachan2016easy,
  title = "Easy Questions First? A Case Study on Curriculum Learning for Question Answering",
  author = "Sachan, Mrinmaya  and
    Xing, Eric",
  editor = "Erk, Katrin  and
    Smith, Noah A.",
  booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = aug,
  year = "2016",
  address = "Berlin, Germany",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P16-1043/",
  doi = "10.18653/v1/P16-1043",
  pages = "453--463"
}

% Lalor, John P. 2020 - Dynamic Data Selection for Curriculum Learning via Ability Estimation
@inproceedings{lalor2020dynamic,
  title = "Dynamic Data Selection for Curriculum Learning via Ability Estimation",
  author = "Lalor, John P.  and
    Yu, Hong",
  editor = "Cohn, Trevor  and
    He, Yulan  and
    Liu, Yang",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
  month = nov,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.findings-emnlp.48/",
  doi = "10.18653/v1/2020.findings-emnlp.48",
  pages = "545--555",
}

% Li, Conglong 2021 - Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training
@article{li2021curriculum,
  author       = {Conglong Li and
                Minjia Zhang and
                Yuxiong He},
  title        = {Curriculum Learning: {A} Regularization Method for Efficient and Stable
                Billion-Scale {GPT} Model Pre-Training},
  journal      = {CoRR},
  volume       = {abs/2108.06084},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.06084},
  eprinttype    = {arXiv},
  eprint       = {2108.06084},
  timestamp    = {Wed, 18 Aug 2021 19:45:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-06084.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% --- Software Libraries--- 

% Narayanan, Deepak 2021 - Efficient large-scale language model training on gpu clusters using megatron-lm
@inproceedings{narayanan2021megatron,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages={1--15},
  year={2021}
}

% Rasley, Jeff 2020 - Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters
@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={3505--3506},
  year={2020}
}

% Rajbh 2020 - Zero: Memory optimizations toward training trillion parameter models
@inproceedings{rajbhandari2020zero,
  title = {Zero: Memory optimizations toward training trillion parameter models},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  year = 2020,
  booktitle = {SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages = {1--16},
  organization = {IEEE}
}

% --- Knowledge Distillation --- 

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{yam2024tinyminds,
  title = "Teaching Tiny Minds: Exploring Methods to Enhance Knowledge Distillation for Small Language Models",
  author = "Yam, Hong Meng  and
    Paek, Nathan",
  editor = "Hu, Michael Y.  and
    Mueller, Aaron  and
    Ross, Candace  and
    Williams, Adina  and
    Linzen, Tal  and
    Zhuang, Chengxu  and
    Choshen, Leshem  and
    Cotterell, Ryan  and
    Warstadt, Alex  and
    Wilcox, Ethan Gotlieb",
  booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
  month = nov,
  year = "2024",
  address = "Miami, FL, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.conll-babylm.27/",
  pages = "302--307",
}

% BabyLlama 2023 - tiny models can be distilled (babylm 2023 submission)
@inproceedings{timiryasov2023baby,
  title = "Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty",
  author = "Timiryasov, Inar  and
    Tastet, Jean-Loup",
  editor = "Warstadt, Alex  and
    Mueller, Aaron  and
    Choshen, Leshem  and
    Wilcox, Ethan  and
    Zhuang, Chengxu  and
    Ciro, Juan  and
    Mosquera, Rafael  and
    Paranjabe, Bhargavi  and
    Williams, Adina  and
    Linzen, Tal  and
    Cotterell, Ryan",
  booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.conll-babylm.24/",
  doi = "10.18653/v1/2023.conll-babylm.24",
  pages = "279--289"
}

% BabyLlama2 2024 - follow up to BabyLlama (babylm 2024 submission)
@inproceedings{tastet2024babyllama2,
  title = "{B}aby{L}lama-2: Ensemble-Distilled Models Consistently Outperform Teachers With Limited Data",
  author = "Tastet, Jean-Loup  and
    Timiryasov, Inar",
  editor = "Hu, Michael Y.  and
    Mueller, Aaron  and
    Ross, Candace  and
    Williams, Adina  and
    Linzen, Tal  and
    Zhuang, Chengxu  and
    Choshen, Leshem  and
    Cotterell, Ryan  and
    Warstadt, Alex  and
    Wilcox, Ethan Gotlieb",
  booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
  month = nov,
  year = "2024",
  address = "Miami, FL, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.conll-babylm.26/",
  pages = "292--301",
}

% --- Parameter Efficient ----

% -- Adapters

% Rebuffi, Sylvestre-Alvise 2017 - Learning Multiple Visual Domains with Residual Adapters
@inproceedings{rebuffi2017adapters,
  author    = {Sylvestre-Alvise Rebuffi and Hakan Bilen and Andrea Vedaldi},
  title     = {Learning Multiple Visual Domains with Residual Adapters},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {506--516},
  year      = {2017}
}

% Houlsby, Neil 2019 - Parameter-Efficient Transfer Learning for NLP
@inproceedings{houlsby2019parameter,
  title     = {Parameter-Efficient Transfer Learning for NLP},
  author    = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {2790--2799},
  year      = {2019},
  publisher = {PMLR}
}

% Hu, Edward J 2021 - LoRA: Low-Rank Adaptation of Large Language Models
@article{hu2021lora,
  title   = {LoRA: Low-Rank Adaptation of Large Language Models},
  author  = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  journal = {arXiv preprint arXiv:2106.09685},
  year    = {2021}
}

% Lialin, Vladislav 2023 - ReLoRA: High-Rank Training Through Low-Rank Updates
@article{lialin2023relora,
  title   = {ReLoRA: High-Rank Training Through Low-Rank Updates},
  author  = {Vladislav Lialin and Namrata Shivagunde and Sherin Muckatira and Anna Rumshisky},
  journal = {arXiv preprint arXiv:2307.05695},
  year    = {2023}
}

% -- Other approaches 

% Ben Zaken, Elad 2022 - BitFit: Simple Parameter-Efficient Fine-Tuning for Transformer-Based Masked Language Models
@inproceedings{benzaken2022bitfit,
  title     = {BitFit: Simple Parameter-Efficient Fine-Tuning for Transformer-Based Masked Language Models},
  author    = {Ben Zaken, Elad and Ravfogel, Shauli and Goldberg, Yoav},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages     = {1--9},
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.acl-short.1},
  url       = {https://aclanthology.org/2022.acl-short.1}
}

% Liu, Haokun 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
@inproceedings{liu2022few,
  title     = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author    = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {1--15},
  year      = {2022},
  url       = {https://arxiv.org/abs/2205.05638}
}


% PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods
@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}


% --- Architectural Modification ---

@inproceedings{samuel2023ltgbert,
    title = "Trained on 100 million words and still in shape: {BERT} meets {B}ritish {N}ational {C}orpus",
    author = "Samuel, David  and
      Kutuzov, Andrey  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.146/",
    doi = "10.18653/v1/2023.findings-eacl.146",
    pages = "1954--1974",
}


% -- Mixture of Experts

% Jacobs, Robert A 1991 - Adaptive mixtures of local experts
@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural Computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

% Shazeer, Noam 2017 - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer
@article{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc V and Hinton, Geoffrey E and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

% Fedus, William 2021 - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}


% Jiang, Albert Q 2024 - Mixtral of Experts
@misc{jiang2024mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bou Hanna, Emma and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, Lélio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Le Scao, Teven and Gervet, Théophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and El Sayed, William},
  year={2024},
  eprint={2401.04088},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2401.04088}
}

% Lepikhin, Dmitry 2020 - GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
@article{lepikhin2020gshard,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

% DeepSeek-AI 2024 - DeepSeek-V3 Technical Report (MoE model)
@article{deepseek2024v3,
  title   = {DeepSeek-V3 Technical Report},
  author  = {DeepSeek-AI},
  journal = {arXiv preprint arXiv:2412.19437},
  year    = {2024},
  url     = {https://arxiv.org/abs/2412.19437}
}

% -- Fast Attention

% Luong, Minh-Thang 2015 - Effective Approaches to Attention-based Neural Machine Translation
@inproceedings{luong2015effective,
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  author    = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages     = {1412--1421},
  year      = {2015},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D15-1166/},
  doi       = {10.18653/v1/D15-1166}
}

% Child, Rewon 2019 - Generating Long Sequences with Sparse Transformers
@article{child2019generating,
  title   = {Generating Long Sequences with Sparse Transformers},
  author  = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:1904.10509},
  year    = {2019},
  url     = {https://arxiv.org/abs/1904.10509}
}

% Roy, Aurko 2020 - Efficient Content-Based Sparse Attention with Routing Transformers
@article{roy2020efficient,
  title   = {Efficient Content-Based Sparse Attention with Routing Transformers},
  author  = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal = {arXiv preprint arXiv:2003.05997},
  year    = {2020},
  url     = {https://arxiv.org/abs/2003.05997}
}

% Beltagy, Iz 2020 - Longformer: The Long-Document Transformer
@article{beltagy2020longformer,
  title   = {Longformer: The Long-Document Transformer},
  author  = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  journal = {arXiv preprint arXiv:2004.05150},
  year    = {2020},
  url     = {https://arxiv.org/abs/2004.05150}
}

% Zaheer, Manzil 2020 - Big Bird: Transformers for Longer Sequences
@inproceedings{zaheer2020big,
  title     = {Big Bird: Transformers for Longer Sequences},
  author    = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontañón, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html}
}

% Kitaev, Nikita 2020 - Reformer: The Efficient Transformer
@inproceedings{kitaev2020reformer,
  title     = {Reformer: The Efficient Transformer},
  author    = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.04451}
}

% Wang, Sinong 2020 - Linformer: Self-Attention with Linear Complexity
@article{wang2020linformer,
  title   = {Linformer: Self-Attention with Linear Complexity},
  author  = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal = {arXiv preprint arXiv:2006.04768},
  year    = {2020},
  url     = {https://arxiv.org/abs/2006.04768}
}


% Ding, Jiayu 2023 - LongNet: Scaling Transformers to 1,000,000,000 Tokens
@article{ding2023longnet,
  title   = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  author  = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Wei, Furu},
  journal = {arXiv preprint arXiv:2307.02486},
  year    = {2023},
  url     = {https://arxiv.org/abs/2307.02486}
}

% Xiao, Guangxuan 2023 - Efficient Streaming Language Models with Attention Sinks
@article{xiao2023attentionsink,
  title   = {Efficient Streaming Language Models with Attention Sinks},
  author  = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal = {arXiv preprint arXiv:2309.17453},
  year    = {2023},
  url     = {https://arxiv.org/abs/2309.17453}
}

% Dao, Tri 2022 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
@inproceedings{dao2022flashattention,
  title     = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author    = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2022},
  url       = {https://arxiv.org/abs/2205.14135}
}

% Dao, Tri 2023 - FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
@article{dao2023flashattention2,
  title   = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author  = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R{\'e}},
  journal = {arXiv preprint arXiv:2307.08691},
  year    = {2023},
  url     = {https://arxiv.org/abs/2307.08691}
}

% -- Pruning

% LeCun, Yann 1990 - Optimal Brain Damage
@inproceedings{lecun1990optimal,
  title     = {Optimal Brain Damage},
  author    = {LeCun, Yann and Denker, John S. and Solla, Sara A.},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {2},
  pages     = {598--605},
  year      = {1990},
  publisher = {Morgan Kaufmann}
}

% Hassibi, Babak 1993 - Second Order Derivatives for Network Pruning: Optimal Brain Surgeon
@inproceedings{hassibi1993optimal,
  title     = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
  author    = {Hassibi, Babak and Stork, David G. and Wolff, Gregory J.},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {6},
  pages     = {164--171},
  year      = {1993},
  publisher = {Morgan Kaufmann}
}

% Han, Song 2015 - Learning both Weights and Connections for Efficient Neural Networks
@inproceedings{han2015learning,
  title     = {Learning both Weights and Connections for Efficient Neural Networks},
  author    = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {28},
  pages     = {1135--1143},
  year      = {2015},
  publisher = {Curran Associates, Inc.}
}

% Han, Song 2016 - Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding
@inproceedings{han2016deep,
  title     = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author    = {Han, Song and Mao, Huizi and Dally, William J.},
  booktitle = {International Conference on Learning Representations},
  year      = {2016},
  url       = {https://arxiv.org/abs/1510.00149}
}


% Frankle, Jonathan 2019 - The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
@inproceedings{frankle2019lottery,
  title     = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author    = {Frankle, Jonathan and Carbin, Michael},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=rJl-b3RcF7}
}


% Sun, Mingjie 2024 - A Simple and Effective Pruning Approach for Large Language Models
@inproceedings{sun2024simple,
  title     = {A Simple and Effective Pruning Approach for Large Language Models},
  author    = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  booktitle = {International Conference on Learning Representations},
  year      = {2024},
  url       = {https://arxiv.org/abs/2306.11695}
}

% -- Quantization

% Wang, Hongyu 2023 - BitNet: Scaling 1-bit Transformers for Large Language Models
@article{wang2023bitnet,
  title   = {BitNet: Scaling 1-bit Transformers for Large Language Models},
  author  = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal = {arXiv preprint arXiv:2310.11453},
  year    = {2023},
  url     = {https://arxiv.org/abs/2310.11453}
}

% Lin, Ji 2023 - AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
@article{lin2023awq,
  title   = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author  = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal = {arXiv preprint arXiv:2306.00978},
  year    = {2023},
  url     = {https://arxiv.org/abs/2306.00978}
}

% Dettmers, Tim 2023 - QLoRA: Efficient Finetuning of Quantized LLMs
@article{dettmers2023qlora,
  title   = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author  = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:2305.14314},
  year    = {2023},
  url     = {https://arxiv.org/abs/2305.14314}
}

% Bengio, Yoshua 2013 - Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation
@article{bengio2013estimating,
  title   = {Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
  author  = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
  journal = {arXiv preprint arXiv:1308.3432},
  year    = {2013},
  url     = {https://arxiv.org/abs/1308.3432}
}


% --- Data Presentation/Input ----

% Huebner, Philip A 2021 - BabyBERTa: Learning more grammar with small-scale child-directed language
@inproceedings{huebner2021babyberta,
  title={BabyBERTa: Learning more grammar with small-scale child-directed language},
  author={Huebner, Philip A and Sulem, Elior and Cynthia, Fisher and Roth, Dan},
  booktitle={Proceedings of the 25th conference on computational natural language learning},
  pages={624--646},
  year={2021}
}

% Eldan, Ronen 2023 - TinyStories: How Small Can Language Models Be and Still Speak Coherent English?
@article{eldan2023tinystories,
  title   = {TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  author  = {Eldan, Ronen and Li, Yuanzhi},
  journal = {arXiv preprint arXiv:2305.07759},
  year    = {2023},
  url     = {https://arxiv.org/abs/2305.07759}
}

% Zhang, Yian 2021 - When Do You Need Billions of Words of Pretraining Data?
@inproceedings{zhang2021need,
  title = "When Do You Need Billions of Words of Pretraining Data?",
  author = "Zhang, Yian  and
    Warstadt, Alex  and
    Li, Xiaocheng  and
    Bowman, Samuel R.",
  editor = "Zong, Chengqing  and
    Xia, Fei  and
    Li, Wenjie  and
    Navigli, Roberto",
  booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  month = aug,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.acl-long.90/",
  doi = "10.18653/v1/2021.acl-long.90",
  pages = "1112--1125",
}

% === Word/Token Representations ===

% --- Origin/Historical Approaches ---

% Mikolov, Tomas 2013 - Efficient estimation of word representations in vector space
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

% Mikolov, Tomas 2013 - Distributed representations of words and phrases and their compositionality
@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

% Pennington, Jeffrey 2014 - Glove: Global vectors for word representation
@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

% ---- Historical Applications of Embeddings ----

% Lample, Guillaume 2016 - Neural Architectures for Named Entity Recognition
@inproceedings{lample-etal-2016-neural,
    title = "Neural Architectures for Named Entity Recognition",
    author = "Lample, Guillaume  and
      Ballesteros, Miguel  and
      Subramanian, Sandeep  and
      Kawakami, Kazuya  and
      Dyer, Chris",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1030/",
    doi = "10.18653/v1/N16-1030",
    pages = "260--270"
}

% Kim, Yoon 2014 - Convolutional Neural Networks for Sentence Classification
@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181/",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751"
}

% Qi, Ye 2018 - When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?
@inproceedings{qi-etal-2018-pre,
    title = "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
    author = "Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2084/",
    doi = "10.18653/v1/N18-2084",
    pages = "529--535",
}

% ---- Non-word/Character-level Embeddings ----

% Kim, Yoon 2016 - Character-aware neural language models
@inproceedings{kim2016character,
  title={Character-aware neural language models},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

% Goriely, Zebulon 2024 - From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes
@inproceedings{goriely-etal-2024-babble,
    title = "From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes",
    author = "Goriely, Z{\'e}bulon  and
      Diehl Martinez, Richard  and
      Caines, Andrew  and
      Buttery, Paula  and
      Beinborn, Lisa",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.4/",
    pages = "37--53",
}


% -- Morphological/Syntatic Information --

@inproceedings{diehlmartinez2024syntacticsmoothing,
    title = "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing",
    author = "Diehl Martinez, Richard  and
      Goriely, Z{\'e}bulon  and
      Caines, Andrew  and
      Buttery, Paula  and
      Beinborn, Lisa",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.344/",
    doi = "10.18653/v1/2024.emnlp-main.344",
    pages = "5999--6011",
}

@inproceedings{bai2022better,
    title = "Better Language Model with Hypernym Class Prediction",
    author = "Bai, He  and
      Wang, Tong  and
      Sordoni, Alessandro  and
      Shi, Peng",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.96/",
    doi = "10.18653/v1/2022.acl-long.96",
    pages = "1352--1362",
}


@article{stratos2016unsupervisedpos,
    title = "Unsupervised Part-Of-Speech Tagging with Anchor Hidden {M}arkov Models",
    author = "Stratos, Karl  and
      Collins, Michael  and
      Hsu, Daniel",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1018/",
    doi = "10.1162/tacl_a_00096",
    pages = "245--257",
}

% Bojanowski, Piotr 2017 - Enriching Word Vectors with Subword Information
@article{bojanowski2017enrichingsubword,
    title = "Enriching Word Vectors with Subword Information",
    author = "Bojanowski, Piotr  and
      Grave, Edouard  and
      Joulin, Armand  and
      Mikolov, Tomas",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1010",
    doi = "10.1162/tacl_a_00051",
    pages = "135--146",
}

@inproceedings{botha2014compositional,
    title={Compositional morphology for word representations and language modelling},
    author={Botha, Jan and Blunsom, Phil},
    booktitle={International Conference on Machine Learning},
    pages={1899--1907},
    year={2014},
    organization={PMLR}
}

@inproceedings{vulic2017morphfitting,
    title = "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules",
    author = "Vuli{\'c}, Ivan  and
      Mrk{\v{s}}i{\'c}, Nikola  and
      Reichart, Roi  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Young, Steve  and
      Korhonen, Anna",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1006/",
    doi = "10.18653/v1/P17-1006",
    pages = "56--68",
}

@inproceedings{salle2018incorporating,
    title = "Incorporating Subword Information into Matrix Factorization Word Embeddings",
    author = "Salle, Alexandre  and
      Villavicencio, Aline",
    editor = {Faruqui, Manaal  and
      Sch{\"u}tze, Hinrich  and
      Trancoso, Isabel  and
      Tsvetkov, Yulia  and
      Yaghoobzadeh, Yadollah},
    booktitle = "Proceedings of the Second Workshop on Subword/Character {LE}vel Models",
    month = jun,
    year = "2018",
    address = "New Orleans",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-1209/",
    doi = "10.18653/v1/W18-1209",
    pages = "66--71",
}

@inproceedings{cotterel2015morphological,
    title = "Morphological Word-Embeddings",
    author = {Cotterell, Ryan  and
      Sch{\"u}tze, Hinrich},
    editor = "Mihalcea, Rada  and
      Chai, Joyce  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "–" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N15-1140/",
    doi = "10.3115/v1/N15-1140",
    pages = "1287--1292"
}

@inproceedings{bhatia2016morphological,
    title = "Morphological Priors for Probabilistic Neural Word Embeddings",
    author = "Bhatia, Parminder  and
      Guthrie, Robert  and
      Eisenstein, Jacob",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1047/",
    doi = "10.18653/v1/D16-1047",
    pages = "490--500"
}

% Cui, Yiming 2022 - Lert: A linguistically-motivated pre-trained language model
@article{cui2022lert,
  title={Lert: A linguistically-motivated pre-trained language model},
  author={Cui, Yiming and Che, Wanxiang and Wang, Shijin and Liu, Ting},
  journal={arXiv preprint arXiv:2211.05344},
  year={2022}
}

% Mueller, Aaron 2023 - How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases
@article{mueller2023plant,
    title={How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases},
    author={Mueller, Aaron and Linzen, Tal},
    url= {https://arxiv.org/abs/2305.19905},
    year={2023},
    journal={arXiv preprint arXiv:2305.19905}}
}

% --- Rare Words --- 

% Unknown 2019 - Attentive mimicking: Better word embeddings by attending to informative contexts
@article{schick2019attentive,
  title={Attentive mimicking: Better word embeddings by attending to informative contexts},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1904.01617},
  year={2019}
}

% Shick, Timo 2020 - Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking
@inproceedings{schick2020rare,
  title={Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  issue={05},
  pages={8766--8774},
  year={2020}
}

% Gong, Chengyue 2018 - Frage: Frequency-agnostic word representation
@article{gong2018frage,
  title={Frage: Frequency-agnostic word representation},
  author={Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

% Raunak, Vikas 2019 - Effective Dimensionality Reduction for Word Embeddings
@inproceedings{raunak2019effective,
    title = "Effective Dimensionality Reduction for Word Embeddings",
    author = "Raunak, Vikas  and
      Gupta, Vivek  and
      Metze, Florian",
    editor = "Augenstein, Isabelle  and
      Gella, Spandana  and
      Ruder, Sebastian  and
      Kann, Katharina  and
      Can, Burcu  and
      Welbl, Johannes  and
      Conneau, Alexis  and
      Ren, Xiang  and
      Rei, Marek",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4328",
    doi = "10.18653/v1/W19-4328",
    pages = "235--243",
}

% Mu, Jiaqi 2018 - All-but-the-Top: Simple and Effective Postprocessing for Word Representations
@inproceedings{mu2018all,
  title={All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
  author={Mu, Jiaqi and Viswanath, Pramod},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

% Arora, Sanjeev 2016 - A Simple but Tough-to-Beat Baseline for Sentence Embeddings
@inproceedings{arora2016simple,
  title={A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
  author={Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

% Press, Ofir 2017 - Using the Output Embedding to Improve Language Models
@inproceedings{press2017using,
  title={Using the Output Embedding to Improve Language Models},
  author={Press, Ofir and Wolf, Lior},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={157--163},
  year={2017}
}

% Hakan Inan 2017 - Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling
@inproceedings{
  inan2017tying,
  title={Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling},
  author={Hakan Inan and Khashayar Khosravi and Richard Socher},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=r1aPbsFle}
}

% Yu, Sangwon 2022 - Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings
@inproceedings{yu2022rare,
  title = "Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings",
  author = "Yu, Sangwon  and
    Song, Jongyoon  and
    Kim, Heeseung  and
    Lee, Seongmin  and
    Ryu, Woo-Jong  and
    Yoon, Sungroh",
  editor = "Muresan, Smaranda  and
    Nakov, Preslav  and
    Villavicencio, Aline",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.acl-long.3",
  doi = "10.18653/v1/2022.acl-long.3",
  pages = "29--45",
}

% Kobayashi, Goro 2023 - Transformer Language Models Handle Word Frequency in Prediction Head
@inproceedings{kobayashi2023transformer,
  title = "Transformer Language Models Handle Word Frequency in Prediction Head",
  author = "Kobayashi, Goro  and
    Kuribayashi, Tatsuki  and
    Yokoi, Sho  and
    Inui, Kentaro",
  editor = "Rogers, Anna  and
    Boyd-Graber, Jordan  and
    Okazaki, Naoaki",
  booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.findings-acl.276",
  doi = "10.18653/v1/2023.findings-acl.276",
  pages = "4523--4535"
}


% -- Anisotropy ---

% Arora, Sanjeev 2016 - A Latent Variable Model Approach to PMI-based Word Embeddings
@article{arora2016latent,
  title = "A Latent Variable Model Approach to {PMI}-based Word Embeddings",
  author = "Arora, Sanjeev  and
    Li, Yuanzhi  and
    Liang, Yingyu  and
    Ma, Tengyu  and
    Risteski, Andrej",
  editor = "Lee, Lillian  and
    Johnson, Mark  and
    Toutanova, Kristina",
  journal = "Transactions of the Association for Computational Linguistics",
  volume = "4",
  year = "2016",
  address = "Cambridge, MA",
  publisher = "MIT Press",
  url = "https://aclanthology.org/Q16-1028",
  doi = "10.1162/tacl_a_00106",
  pages = "385--399",
}


% Ait-Saada, Mira 2023 - Is Anisotropy Truly Harmful? A Case Study on Text Clustering
@inproceedings{ait2023anisotropy,
  title = "Is Anisotropy Truly Harmful? A Case Study on Text Clustering",
  author = "Ait-Saada, Mira  and
    Nadif, Mohamed",
  editor = "Rogers, Anna  and
    Boyd-Graber, Jordan  and
    Okazaki, Naoaki",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-short.103",
  doi = "10.18653/v1/2023.acl-short.103",
  pages = "1194--1203",
}

% Godey 2024 - Anisotropy Is Inherent to Self-Attention in Transformers
@article{godey2024anisotropy,
  title={Anisotropy Is Inherent to Self-Attention in Transformers},
  author={Godey, Nathan and de la Clergerie, {\'E}ric and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:2401.12143},
  year={2024}
}

% Ethayarajh, Kawin 2019 - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
@inproceedings{ethayarajh2019contextual,
  title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
  author = "Ethayarajh, Kawin",
  editor = "Inui, Kentaro  and
    Jiang, Jing  and
    Ng, Vincent  and
    Wan, Xiaojun",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month = nov,
  year = "2019",
  address = "Hong Kong, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D19-1006",
  doi = "10.18653/v1/D19-1006",
  pages = "55--65",
}

% Cai, Xingyu 2020 - Isotropy in the contextual embedding space: Clusters and manifolds
@inproceedings{cai2020isotropy,
  title={Isotropy in the contextual embedding space: Clusters and manifolds},
  author={Cai, Xingyu and Huang, Jiaji and Bian, Yuchen and Church, Kenneth},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

% Puccetti, Giovanni 2022 - Outlier Dimensions that Disrupt Transformers are Driven by Frequency
@inproceedings{puccetti2022outlier,
  title={Outlier Dimensions that Disrupt Transformers are Driven by Frequency},
  author={Puccetti, Giovanni and Rogers, Anna and Drozd, Aleksandr and Dell’Orletta, Felice},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={1286--1304},
  year={2022}
}

% Kovaleva, Olga 2021 - BERT Busters: Outlier Dimensions that Disrupt Transformers
@inproceedings{kovaleva2021bert,
  title={BERT Busters: Outlier Dimensions that Disrupt Transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={3392--3405},
  year={2021}
}

% Rajaee, Sara 2022 - Findings of the Association for Computational Linguistics: ACL 2022
@inproceedings{rajaee2022isotropy,
    title = "An Isotropy Analysis in the Multilingual {BERT} Embedding Space",
    author = "Rajaee, Sara  and
      Pilehvar, Mohammad Taher",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.103",
    doi = "10.18653/v1/2022.findings-acl.103",
    pages = "1309--1316",
}

% Su, Jianlin 2021 - Whitening sentence representations for better semantics and faster retrieval
@article{su2021whitening,
  title={Whitening sentence representations for better semantics and faster retrieval},
  author={Su, Jianlin and Cao, Jiarun and Liu, Weijie and Ou, Yangyiwen},
  journal={arXiv preprint arXiv:2103.15316},
  year={2021}
}

% Ding, Yue 2022 - On Isotropy Calibration of Transformer Models
@inproceedings{ding2022isotropy,
  title={On Isotropy Calibration of Transformer Models},
  author={Ding, Yue and Martinkus, Karolis and Pascual, Damian and Clematide, Simon and Wattenhofer, Roger},
  booktitle={Proceedings of the Third Workshop on Insights from Negative Results in NLP},
  pages={1--9},
  year={2022}
}

% Zhilin Yang 2018 - International Conference on Learning Representations
@inproceedings{yang2018breaking,
    title={Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
    author={Zhilin Yang and Zihang Dai and Ruslan Salakhutdinov and William W. Cohen},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=HkwZSG-CZ},
}

% Jun Gao 2019 - Representation Degeneration Problem in Training Natural Language Generation Models
@inproceedings{
    gao2018representation,
    title={Representation Degeneration Problem in Training Natural Language Generation Models},
    author={Jun Gao and Di He and Xu Tan and Tao Qin and Liwei Wang and Tieyan Liu},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=SkEYojRqtm},
}

% Daniel Bis 2021 - Too much in common: Shifting of embeddings in transformer language models and its implications
@inproceedings{bis2021too,
  title={Too much in common: Shifting of embeddings in transformer language models and its implications},
  author={Bi{\'s}, Daniel and Podkorytov, Maksim and Liu, Xiuwen},
  booktitle={Proceedings of the 2021 conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5117--5130},
  year={2021}
}

% Wang, Lingxiao 2019 - Improving neural language generation with spectrum control
@inproceedings{wang2019improving,
  title={Improving neural language generation with spectrum control},
  author={Wang, Lingxiao and Huang, Jing and Huang, Kevin and Hu, Ziniu and Wang, Guangtao and Gu, Quanquan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

% Li, Bohan 2020 - On the Sentence Embeddings from Pre-trained Language Models
@inproceedings{li2020sentence,
  title={On the Sentence Embeddings from Pre-trained Language Models},
  author={Li, Bohan and Zhou, Hao and He, Junxian and Wang, Mingxuan and Yang, Yiming and Li, Lei},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9119--9130},
  year={2020}
}

% === Human Language learning  ===

% BabyLM 1 Proceedings
@inproceedings{warstadt2023babylm1,
  title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
  author = "Warstadt, Alex  and
    Mueller, Aaron  and
    Choshen, Leshem  and
    Wilcox, Ethan  and
    Zhuang, Chengxu  and
    Ciro, Juan  and
    Mosquera, Rafael  and
    Paranjabe, Bhargavi  and
    Williams, Adina  and
    Linzen, Tal  and
    Cotterell, Ryan",
  editor = "Warstadt, Alex  and
    Mueller, Aaron  and
    Choshen, Leshem  and
    Wilcox, Ethan  and
    Zhuang, Chengxu  and
    Ciro, Juan  and
    Mosquera, Rafael  and
    Paranjabe, Bhargavi  and
    Williams, Adina  and
    Linzen, Tal  and
    Cotterell, Ryan",
  booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.conll-babylm.1",
  doi = "10.18653/v1/2023.conll-babylm.1",
  pages = "1--34",
}

% BabyLM 2 Proceedings
@proceedings{conll2024babylm2,
  title = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
  editor = "Hu, Michael Y.  and
    Mueller, Aaron  and
    Ross, Candace  and
    Williams, Adina  and
    Linzen, Tal  and
    Zhuang, Chengxu  and
    Choshen, Leshem  and
    Cotterell, Ryan  and
    Warstadt, Alex  and
    Wilcox, Ethan Gotlieb",
  month = nov,
  year = "2024",
  address = "Miami, FL, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.conll-babylm.0/"
}

% Linzen 2021 - PAID framework 
@inproceedings{linzen2020accelerate,
  title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
  author = "Linzen, Tal",
  editor = "Jurafsky, Dan  and
    Chai, Joyce  and
    Schluter, Natalie  and
    Tetreault, Joel",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.465/",
  doi = "10.18653/v1/2020.acl-main.465",
  pages = "5210--5217",
}

% Alex Warstadt 2022 - What artificial neural networks can tell us about human language acquisition
@incollection{warstadt2022what,
    author = {Alex Warstadt and Samuel Bowman},
    title = {What artificial neural networks can tell us about human language acquisition},
    year = {2022},
    editor = {Shalom Lappin and Jean-Philippe Bernardy},
    booktitle = {Algebraic Structures in Natural Language},
    publisher = {CRC Press}
}

% Saxe, Andrew 2021 - If deep learning is the answer, what is the question?
@article{saxe2021if,
    title={If deep learning is the answer, what is the question?},
    author={Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher},
    journal={Nature Reviews Neuroscience},
    volume={22},
    number={1},
    pages={55--67},
    year={2021},
    publisher={Nature Publishing Group UK London}
}


% --- Biology ---

% 86 billion neurons 
@article{azevedo2009neurons,
  title={Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain},
  author={Azevedo, Frederico AC and Carvalho, Ludmila RB and Grinberg, Lea T and Farfel, Jos{\'e} Marcelo and Ferretti, Renata EL and Leite, Renata EP and Filho, Wilson Jacob and Lent, Roberto and Herculano-Houzel, Suzana},
  journal={Journal of Comparative Neurology},
  volume={513},
  number={5},
  pages={532--541},
  year={2009},
  publisher={Wiley Online Library}
}


% --- Psycholinguistics ---

% Goldilocks principle of task attention
@article{kidd2012goldilocks,
    doi = {10.1371/journal.pone.0036399},
    author = {Kidd, Celeste AND Piantadosi, Steven T. AND Aslin, Richard N.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The {Goldilocks Effect}: Human Infants Allocate Attention to Visual Sequences That Are Neither Too Simple Nor Too Complex},
    year = {2012},
    month = {05},
    volume = {7},
    url = {https://doi.org/10.1371/journal.pone.0036399},
    pages = {1-8},
    number = {5},
}

@article{bergelson2015early,
  title={Early word comprehension in infants: Replication and extension},
  author={Bergelson, Elika and Swingley, Daniel},
  journal={Language Learning and Development},
  volume={11},
  number={4},
  pages={369--380},
  year={2015},
  publisher={Taylor \& Francis}
}

% Clark, Eve V 2015 - First language acquisition
@inbook{clark2015first,
    title={First language acquisition},
    author={Clark, Eve V and Casillas, Marisa},
    booktitle={The Routledge handbook of linguistics},
    pages={167--167},
    year={2015},
    publisher={Routledge}
}

% Alishahi, Afra 2010 - Computational modeling of human language acquisition
@book{alishahi2010computational,
    title={Computational modeling of human language acquisition},
    author={Alishahi, Afra},
    year={2010},
    pages={35},
    publisher={Morgan \& Claypool Publishers}
}

% Gleitman, Lila 1990 - The structural sources of verb meanings
@article{gleitman1990structural,
  title={The structural sources of verb meanings},
  author={Gleitman, Lila},
  journal={Language acquisition},
  volume={1},
  number={1},
  pages={3--55},
  year={1990},
  publisher={Taylor \& Francis}
}

% Jill Gilkerson 2017 - Mapping the early language environment using all-day recordings and automated analysis
@article{gilkerson2017mapping,
  author = {Jill Gilkerson and Jeffrey A Richards and Steven F Warren and Judith K Montgomery and Charles R Greenwood and D Kimbrough Oller and John H L Hansen and Terrance D Paul},
  year = {2017},
  title = {Mapping the early language environment using all-day recordings and automated analysis},
  journal = {American Journal of Speech-Language Pathology},
  volume = {26},
  issue = {2},
  pages = {248–265},
  doi = {https://doi.org/10.1044/2016_AJSLP-15-0169}
}

% Weizman, Zehava Oz 2001 - Lexical output as related to children's vocabulary acquisition: Effects of sophisticated exposure and support for meaning.
@article{weizman2001lexical,
  title={Lexical output as related to children's vocabulary acquisition: Effects of sophisticated exposure and support for meaning.},
  author={Weizman, Zehava Oz and Snow, Catherine E},
  journal={Developmental psychology},
  volume={37},
  number={2},
  pages={265},
  year={2001},
  publisher={American Psychological Association}
}

% Unknown 2023 - Evidence of a predictive coding hierarchy in the human brain listening to speech
@article{caucheteux2023evidence,
  title={Evidence of a predictive coding hierarchy in the human brain listening to speech},
  author={Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  journal={Nature human behaviour},
  volume={7},
  number={3},
  pages={430--441},
  year={2023},
  publisher={Nature Publishing Group UK London}
}